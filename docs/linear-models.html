<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 3 Linear Models | Society of Actuaries Exam PA</title>
  <meta name="description" content="This is a minimal example of using the bookdown package to write a book. The output format for this example is bookdown::gitbook." />
  <meta name="generator" content="bookdown 0.40 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 3 Linear Models | Society of Actuaries Exam PA" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="This is a minimal example of using the bookdown package to write a book. The output format for this example is bookdown::gitbook." />
  <meta name="github-repo" content="jeffamaxey/soa-exam-pa" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 3 Linear Models | Society of Actuaries Exam PA" />
  
  <meta name="twitter:description" content="This is a minimal example of using the bookdown package to write a book. The output format for this example is bookdown::gitbook." />
  

<meta name="author" content="Jeff A. Maxey" />


<meta name="date" content="2024-10-07" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="data-exploration-and-visualization.html"/>

<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>
<script src="libs/kePrint-0.0.1/kePrint.js"></script>
<link href="libs/lightable-0.0.1/lightable.css" rel="stylesheet" />



<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>

<link rel="stylesheet" href="assets/css/style.css" type="text/css" />
<link rel="stylesheet" href="assets/css/toc.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Society of Actuaries Exam PA</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i>Introduction</a>
<ul>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html#getting-started"><i class="fa fa-check"></i>Getting Started</a></li>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html#system-requirements"><i class="fa fa-check"></i>System Requirements</a></li>
</ul></li>
<li class="part"><span><b>I A Crash Course in R</b></span></li>
<li class="chapter" data-level="1" data-path="basics-of-r-programming.html"><a href="basics-of-r-programming.html"><i class="fa fa-check"></i><b>1</b> Basics of R Programming</a>
<ul>
<li class="chapter" data-level="1.1" data-path="basics-of-r-programming.html"><a href="basics-of-r-programming.html#data-types-in-r"><i class="fa fa-check"></i><b>1.1</b> Data Types in R</a>
<ul>
<li class="chapter" data-level="1.1.1" data-path="basics-of-r-programming.html"><a href="basics-of-r-programming.html#exercise-1.2.2---calculating-least-squares-estimates-by-matrix-manipulation"><i class="fa fa-check"></i><b>1.1.1</b> Exercise 1.2.2 - Calculating least squares estimates by matrix manipulation</a></li>
<li class="chapter" data-level="1.1.2" data-path="basics-of-r-programming.html"><a href="basics-of-r-programming.html#data-frames"><i class="fa fa-check"></i><b>1.1.2</b> Data Frames</a></li>
<li class="chapter" data-level="1.1.3" data-path="basics-of-r-programming.html"><a href="basics-of-r-programming.html#exercise-1.2.3---listing-the-components-of-a-list"><i class="fa fa-check"></i><b>1.1.3</b> Exercise 1.2.3 - Listing the components of a list</a></li>
<li class="chapter" data-level="1.1.4" data-path="basics-of-r-programming.html"><a href="basics-of-r-programming.html#functions"><i class="fa fa-check"></i><b>1.1.4</b> Functions</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="basics-of-r-programming.html"><a href="basics-of-r-programming.html#basic-data-management"><i class="fa fa-check"></i><b>1.2</b> Basic Data Management</a>
<ul>
<li class="chapter" data-level="1.2.1" data-path="basics-of-r-programming.html"><a href="basics-of-r-programming.html#task-1-removing-unimportant-variables"><i class="fa fa-check"></i><b>1.2.1</b> Task 1: Removing Unimportant Variables</a></li>
<li class="chapter" data-level="1.2.2" data-path="basics-of-r-programming.html"><a href="basics-of-r-programming.html#task-2-missing-values"><i class="fa fa-check"></i><b>1.2.2</b> Task 2: Missing Values</a></li>
<li class="chapter" data-level="1.2.3" data-path="basics-of-r-programming.html"><a href="basics-of-r-programming.html#task-3-adding-new-variables"><i class="fa fa-check"></i><b>1.2.3</b> Task 3: Adding New Variables</a></li>
<li class="chapter" data-level="1.2.4" data-path="basics-of-r-programming.html"><a href="basics-of-r-programming.html#task-4-using-logical-tests-to-identify-observations-with-certain-characteristics"><i class="fa fa-check"></i><b>1.2.4</b> Task 4: Using Logical Tests to Identify Observations with Certain Characteristics</a></li>
<li class="chapter" data-level="1.2.5" data-path="basics-of-r-programming.html"><a href="basics-of-r-programming.html#end-of-chapter-1-practice-problems"><i class="fa fa-check"></i><b>1.2.5</b> End of Chapter 1 Practice Problems</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="data-exploration-and-visualization.html"><a href="data-exploration-and-visualization.html"><i class="fa fa-check"></i><b>2</b> Data Exploration and Visualization</a>
<ul>
<li class="chapter" data-level="" data-path="data-exploration-and-visualization.html"><a href="data-exploration-and-visualization.html#chapter-02-overview"><i class="fa fa-check"></i>Chapter Overview</a></li>
<li class="chapter" data-level="2.1" data-path="data-exploration-and-visualization.html"><a href="data-exploration-and-visualization.html#chapter-02-making-ggplots"><i class="fa fa-check"></i><b>2.1</b> Making <code>ggplots</code></a>
<ul>
<li class="chapter" data-level="2.1.1" data-path="data-exploration-and-visualization.html"><a href="data-exploration-and-visualization.html#chapter-02-making-ggplots-basic-features"><i class="fa fa-check"></i><b>2.1.1</b> Basic Features</a></li>
<li class="chapter" data-level="2.1.2" data-path="data-exploration-and-visualization.html"><a href="data-exploration-and-visualization.html#chapter-02-making-ggplots-customizing-your-plots"><i class="fa fa-check"></i><b>2.1.2</b> Customizing Your Plots</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="data-exploration-and-visualization.html"><a href="data-exploration-and-visualization.html#chapter-02-data-exploration"><i class="fa fa-check"></i><b>2.2</b> Data Exploration</a>
<ul>
<li class="chapter" data-level="2.2.1" data-path="data-exploration-and-visualization.html"><a href="data-exploration-and-visualization.html#chapter-02-data-exploration-univariate"><i class="fa fa-check"></i><b>2.2.1</b> Univariate Data Exploration</a></li>
<li class="chapter" data-level="2.2.2" data-path="data-exploration-and-visualization.html"><a href="data-exploration-and-visualization.html#chapter-02-data-exploration-bivariate"><i class="fa fa-check"></i><b>2.2.2</b> Bivariate Data Exploration</a></li>
<li class="chapter" data-level="2.2.3" data-path="data-exploration-and-visualization.html"><a href="data-exploration-and-visualization.html#end-of-chapter-practice-problems"><i class="fa fa-check"></i><b>2.2.3</b> End-of-Chapter Practice Problems</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>II Theory of and Case Studies in Predictive Analytics</b></span></li>
<li class="chapter" data-level="3" data-path="linear-models.html"><a href="linear-models.html"><i class="fa fa-check"></i><b>3</b> Linear Models</a>
<ul>
<li class="chapter" data-level="3.1" data-path="linear-models.html"><a href="linear-models.html#a-primer-on-predictive-analytics"><i class="fa fa-check"></i><b>3.1</b> A Primer on Predictive Analytics</a>
<ul>
<li class="chapter" data-level="3.1.1" data-path="linear-models.html"><a href="linear-models.html#basic-terminology"><i class="fa fa-check"></i><b>3.1.1</b> Basic Terminology</a></li>
<li class="chapter" data-level="3.1.2" data-path="linear-models.html"><a href="linear-models.html#the-model-building-process"><i class="fa fa-check"></i><b>3.1.2</b> The Model Building Process</a></li>
<li class="chapter" data-level="3.1.3" data-path="linear-models.html"><a href="linear-models.html#bias-variance-trade-off"><i class="fa fa-check"></i><b>3.1.3</b> Bias-Variance Trade-Off</a></li>
<li class="chapter" data-level="3.1.4" data-path="linear-models.html"><a href="linear-models.html#feature-generation-and-selection"><i class="fa fa-check"></i><b>3.1.4</b> Feature Generation and Selection</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="linear-models.html"><a href="linear-models.html#linear-models-conceptual-foundation"><i class="fa fa-check"></i><b>3.2</b> Linear Models: Conceptual Foundation</a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="linear-models.html"><a href="linear-models.html#model-formulation"><i class="fa fa-check"></i><b>3.2.1</b> Model Formulation</a></li>
<li class="chapter" data-level="3.2.2" data-path="linear-models.html"><a href="linear-models.html#model-evaluation-and-validation"><i class="fa fa-check"></i><b>3.2.2</b> Model Evaluation and Validation</a></li>
<li class="chapter" data-level="3.2.3" data-path="linear-models.html"><a href="linear-models.html#feature-generation"><i class="fa fa-check"></i><b>3.2.3</b> Feature Generation</a></li>
<li class="chapter" data-level="3.2.4" data-path="linear-models.html"><a href="linear-models.html#feature-selection"><i class="fa fa-check"></i><b>3.2.4</b> Feature Selection</a></li>
<li class="chapter" data-level="3.2.5" data-path="linear-models.html"><a href="linear-models.html#regularization"><i class="fa fa-check"></i><b>3.2.5</b> Regularization</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="linear-models.html"><a href="linear-models.html#case-study-1-fitting-linear-models-in-r"><i class="fa fa-check"></i><b>3.3</b> Case Study 1: Fitting Linear Models in R</a>
<ul>
<li class="chapter" data-level="3.3.1" data-path="linear-models.html"><a href="linear-models.html#exploratory-data-analysis"><i class="fa fa-check"></i><b>3.3.1</b> Exploratory Data Analysis</a></li>
<li class="chapter" data-level="3.3.2" data-path="linear-models.html"><a href="linear-models.html#simple-linear-regression"><i class="fa fa-check"></i><b>3.3.2</b> Simple Linear Regression</a></li>
<li class="chapter" data-level="3.3.3" data-path="linear-models.html"><a href="linear-models.html#multiple-linear-regression"><i class="fa fa-check"></i><b>3.3.3</b> Multiple Linear Regression</a></li>
<li class="chapter" data-level="3.3.4" data-path="linear-models.html"><a href="linear-models.html#evaluation-of-linear-models"><i class="fa fa-check"></i><b>3.3.4</b> Evaluation of Linear Models</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="linear-models.html"><a href="linear-models.html#case-study-2-feature-selection-and-regularization"><i class="fa fa-check"></i><b>3.4</b> Case Study 2: Feature Selection and Regularization</a>
<ul>
<li class="chapter" data-level="3.4.1" data-path="linear-models.html"><a href="linear-models.html#preparatory-work"><i class="fa fa-check"></i><b>3.4.1</b> Preparatory Work</a></li>
<li class="chapter" data-level="3.4.2" data-path="linear-models.html"><a href="linear-models.html#model-construction-and-feature-selection"><i class="fa fa-check"></i><b>3.4.2</b> Model Construction and Feature Selection</a></li>
<li class="chapter" data-level="3.4.3" data-path="linear-models.html"><a href="linear-models.html#model-validation"><i class="fa fa-check"></i><b>3.4.3</b> Model Validation</a></li>
<li class="chapter" data-level="3.4.4" data-path="linear-models.html"><a href="linear-models.html#regularization-2"><i class="fa fa-check"></i><b>3.4.4</b> Regularization</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="linear-models.html"><a href="linear-models.html#conceptual-review-questions-for-chapter-3"><i class="fa fa-check"></i>Conceptual Review: Questions for Chapter 3</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/jeffamaxey/soa-exam-pa" target="blank">Github Repository</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Society of Actuaries Exam PA</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="linear-models" class="section level1 hasAnchor" number="3">
<h1><span class="header-section-number">Chapter 3</span> Linear Models<a href="linear-models.html#linear-models" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<div id="a-primer-on-predictive-analytics" class="section level2 hasAnchor" number="3.1">
<h2><span class="header-section-number">3.1</span> A Primer on Predictive Analytics<a href="linear-models.html#a-primer-on-predictive-analytics" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>This subsection streamlines the material scattered in different parts of the PA modules (which are notoriously difficult to digest) and presents a coherent introduction to predictive analytics.</p>
<p>The fundamental concepts introduced in this section are universally applicable in the sense that they apply to essentially all types of models, and will be illustrated in the context of specific types of models (generalized linear models and decision trees, in particular) in this and later chapters.</p>
<div id="basic-terminology" class="section level3 hasAnchor" number="3.1.1">
<h3><span class="header-section-number">3.1.1</span> Basic Terminology<a href="linear-models.html#basic-terminology" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><strong>Predictive Analytics in a Nutshell:</strong></p>
<p>What exactly is predictive analytics? To answer this question, it may help to understand the three main categories of predictive modeling problems suggested by the PA modules:</p>
<ul>
<li><strong>Descriptive:</strong> Descriptive analytics focuses on what happened in the <strong><em>past</em></strong> and aims to “describe” or explain the observed trends by identifying the relationships between different variables in the data.
<ul>
<li><strong>Example:</strong> If you saw an increase in the lapse rate among the policyholders of a certain line of business, what kind of policyholders had the highest tendency to lapse? This is a question that can be answered by descriptive analytics.</li>
</ul></li>
<li><strong>Predictive:</strong> Predictive analytics focuses on what will happen in the <strong><em>future</em></strong> and is concerned with making accurate “predictions”.
<ul>
<li><strong>Example:</strong> For a prospective policyholder with certain characteristics, what is their predicted probability of lapse? The ability to make such a prediction will be useful for identifying future policyholders who will have a lower probability of lapse and contribute to the profitability of an insurer.</li>
</ul></li>
<li><strong>Prescriptive:</strong> Prescriptive analytics uses a combination of optimization and simulation to investigate and quantify impact of different “prescribed” actions in different scenarios.
<ul>
<li><strong>Example:</strong> If we reduce the premium by a certain amount, how will this affect the lapse rate?</li>
</ul></li>
</ul>
<p>Not surprisingly, Exam PA is predominantly concerned with <em>predictive analytics</em>, although the three modeling approaches are often mutually complementary, not contradictory.</p>
<p>There is always an output (or outcome) of interest, which can be numeric (salary, premium, injury rates) or categorical (positive/negative, email/spam), and we have at our disposal a collection of input variables that may offer potentially useful information for predicting or understanding the output.</p>
<p>This “input-output” setting is characteristic of predictive analytics in general, and our job is to develop a model teasing out the (possibly complex, overlapping) contributions of inputs to the outputs.</p>
<p><strong>Classification of Variables:</strong></p>
<p>Predictive analytics requires data, often with a large number of observations and variables. Generally speaking, there are two ways to classify variables in a predictive analytics context: By their role in the study (intended use) or by their nature (characteristics).</p>
<ul>
<li><p><strong>By Role:</strong> The variable that we are interested in predicting is called the <strong><em>target variable</em></strong>, or simply the <strong><em>target</em></strong> <strong>(a.k.a response variable, dependent variable, output variable, outcome variable).</strong></p>
<p>Despite the target variable being our interest, in most situations we cannot change the target variable directly, but we have control over some associated variables which can be used to predict the target. These variables go by different names, such as <strong><em>predictors, explanatory variables, independent variables, input variables</em></strong>.</p>
<p>In an actuarial context, predictors are also known as risk factors or risk drivers. In the remainder of this manual and in Exam PA, we will mostly use the term “predictors” and “features” (to be defined in Subsection 3.1.4).</p>
<p>Throughout your study of predictive analytics, it is useful to think of a predictive model as the following functional relationship between the target variable <span class="math inline">\(Y\)</span> and the set of predictors <span class="math inline">\(X = (X_{1}, ..., X_{p})\)</span> (collected as a vector): <span class="math display">\[
        \begin{equation}
        Y_{i} = f(X_{i}) + \epsilon_{i} (\#eq:linear-relationship)
        \end{equation}
        \]</span> Where:</p>
<ul>
<li><p>The subscript <span class="math inline">\(i\)</span> signifies the <span class="math inline">\(i\)</span>th observation in your dataset, so <span class="math inline">\(Y_{i}\)</span> is the value of the target variable for the <span class="math inline">\(i\)</span>th observation and <span class="math inline">\(X_{i} = (X_{i1},..., X_{ip})\)</span> is the corresponding vector of predictor values.</p></li>
<li><p>The symbol <span class="math inline">\(f\)</span> is a fixed (non-random) but unknown real function connecting the predictors and the target variables. Without the subscript <span class="math inline">\(i\)</span> (it is <span class="math inline">\(f\)</span> rather than <span class="math inline">\(f_{i}\)</span>), the function applies to all observations in the data, hence “systematic”. Largely synonymous with the model, this function carries the systematic information that the predictors offer about the target variable, and allows us to differentiate, or discriminate, the observations of the target variable on the basis of those of the predictors.</p>
<p>Different types of predictive models are distinguished by the structural form of this function, e.g., linear for linear models and piece-wise constant for decision trees, as we will see in later chapters.</p></li>
<li><p>The symbol <span class="math inline">\(\epsilon_{i}\)</span> is a zero-mean random error term carrying information that is specific to the <span class="math inline">\(i\)</span>th observation, hence “idiosyncratic” and the presence of the subscript <span class="math inline">\(i\)</span>. It can be regarded as a catch-all for what the systematic component of the model misses, e.g., the true relationship between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> is probably more complex than <a href="#eq:linear-relationship">(<strong>??</strong>)</a>, there are other variables associated with <span class="math inline">\(Y\)</span> omitted by the model.</p>
<p>Although <a href="#eq:linear-relationship">(<strong>??</strong>)</a> looks abstract and the exam will not test it directly, it will provide a useful framework for thinking about predictive analytics.</p>
<p>For convenience, we will refer to <span class="math inline">\(f\)</span> and <span class="math inline">\(\epsilon\)</span> respectively as the <span class="math inline">\(\fbox{signal function}\)</span> and the <span class="math inline">\(\fbox{noise}\)</span> which are engineering terms.</p>
<p>We are interested in the signal, but the data we have is “contaminated” with noise. The goal of predictive analytics is to filter out the noise and use a variety of tools and techniques to learn as much about the signal as possible from the data. This knowledge about the signal can then provide us with a basis for understanding the data-generating process underlying the population of interest, and making predictions for the target variable.</p></li>
</ul></li>
</ul>
<!-- -->
<ul>
<li><p><strong>By Nature:</strong> Variables can also be classified as <em>numeric</em> variables or <em>categorical</em> variables. Such a classification has important implications for developing an effective predictive model that aligns with the character of the target variable and predictors to produce realistic output.</p>
<ol style="list-style-type: decimal">
<li><p><strong>Numeric Variables:</strong> Numeric variables take the form of numbers with a well-defined order (e.g., 1 must be less than 2) and an associated range. They can be further classified as:</p>
<ul>
<li><p><strong>Discrete variables:</strong> Restricted to only certain numeric values in that range, e.g., non-negative integers.</p></li>
<li><p><strong>Continuous:</strong> Can assume any value in a continuum, at least in theory.</p></li>
</ul></li>
<li><p><strong>Categorical Variables:</strong> Categorical variables take predefined values in a countable collection of “categories”. These categories, which often have no numeric order (i.e., we cannot say which category is larger or smaller), are called the <strong>levels</strong> or <strong>classes</strong> of the variable. Here are some examples of properties that can be described by categorical variables:</p>
<ul>
<li><p>Gender (male, female, or non-binary)</p></li>
<li><p>Smoking status (smoking or non-smoking)</p></li>
<li><p>Risk group (preferred, standard, rated, or un-insurable)</p></li>
<li><p>Marital status (single, married, divorced, or widowed)</p></li>
</ul>
<p>An important special case of a categorical variable is a <strong>binary</strong> variable, which only takes two possible levels (often yes or no), indicating whether an event has taken place or whether a characteristic is present.</p>
<p>In many datasets in practice, categorical variables are represented by numbers, but note that coding the levels of a categorical variable by numbers does not make the variable numeric. Consider, for instance, the variable <span class="math inline">\(X\)</span> which represents smoking status and is encoded as: <span class="math display">\[
X = \begin{cases} &amp; \text{0, for a non-smoker,} \\ &amp; \text{1, for a smoker,} \\ &amp; \text{2, for an individual with unknown smoking status.}  \end{cases}
\]</span></p>
<p>Even though the three levels “non-smoker”, “smoker”, and “unknown” have been coded as “0”, “1”, and “2”, respectively, the three numbers are merely labels without an implicit order and cannot be compared in an algebraic fashion. For example, “1” is not less than “2” in a meaningful fashion in this case.</p></li>
</ol>
<p>In predictive modeling, the type of model to use is largely determined by the nature of the target variable, not the predictors.</p>
<p>In other words, the distinction between continuous and categorical variables is relatively unimportant when they serve as predictors of a model, but we need to take the distinction properly into account when they serve as the target variable. Some predictive models (e.g., linear models) will work well only for continuous target variables while some (e.g., generalized linear models and decision trees) apply to both numeric and categorical target variables.</p></li>
</ul>
<p><strong>Supervised vs. Unsupervised Problems:</strong></p>
<p>Predictive analytics problems can also be classified into <em>supervised</em> and <em>unsupervised</em> problems, depending on the presence of a target variable and the objective of analysis.</p>
<ul>
<li><p><strong>Supervised Learning Problems:</strong> They refer to those for which there is a target variable “supervising” or guiding our analysis, and our goal is to understand the relationship between the target variable and the predictors, and/or make accurate predictions for the target based on the predictors.</p>
<ul>
<li><p><strong>Types of Supervised Learning Methods:</strong></p>
<ul>
<li><p>Generalized Linear Models (GLMs)</p></li>
<li><p>Decision Trees</p></li>
</ul></li>
</ul></li>
<li><p><strong>Unsupervised Learning Problems:</strong> For unsupervised learning methods, there is no target variable supervising our analysis (we are therefore “unsupervised”), and we are interested in extracting relationships and structures between different variables in the data.</p>
<ul>
<li><p><strong>Types of Unsupervised Learning Methods:</strong></p>
<ul>
<li><p>Principal Components Analysis (PCA)</p></li>
<li><p>Cluster Analysis</p></li>
</ul></li>
</ul></li>
</ul>
<p>Note that supervised and unsupervised learning methods are sometimes used in conjunction with one another. As we will see in Chapter 6, unsupervised learning methods can be used for the purposes of data exploration and producing potentially useful features for predicting the target variable more accurately.</p>
<p><strong>Regression vs. Classification Problems:</strong></p>
<p>In predictive analytics, it is customary to refer to supervised learning problems with a numeric target variable as <strong><em>regression</em></strong> problems (an exception is logistic regression, for which the target variable is binary; see Chapter 4. In contrast, when the target variable is categorical in nature, we are dealing with <strong><em>classification</em></strong> problems. A predictive model for predicting a categorical target variable involves “classifying” its observations to a certain level and is aptly called a <em>classifier</em>.</p>
<p>Both regression and classification problems are of importance in predictive modeling in general. The two kinds of predictive analytics problems have their unique features and will covered in detail in Part II of this manual.</p>
</div>
<div id="the-model-building-process" class="section level3 hasAnchor" number="3.1.2">
<h3><span class="header-section-number">3.1.2</span> The Model Building Process<a href="linear-models.html#the-model-building-process" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Now that we have a first taste of what predictive analytics is like, in this important and rather lengthy subsection we will walk through the main steps involved in the construction and evaluation of a predictive model.</p>
<p>In practice, model building typically requires a sequence of complex and inter-related decisions. The whole process is iterative and often more of an art than a science. The framework here is necessarily simplified and focuses on the most important steps in Exam PA, but is rich enough to show you what it takes to build a good model in real life.</p>
<div id="stage-1-problem-definition" class="section level4 hasAnchor" number="3.1.2.1">
<h4><span class="header-section-number">3.1.2.1</span> Stage 1: Problem Definition<a href="linear-models.html#stage-1-problem-definition" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>The first step in any model building exercise is to clearly formulate the business problems to which predictive analytics will be applied.</p>
<div id="characteristics-of-predictive-modeling-problems" class="section level5 unnumbered hasAnchor">
<h5>Characteristics of Predictive Modeling Problems<a href="linear-models.html#characteristics-of-predictive-modeling-problems" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<p>Before we decide to apply predictive analytics to solve a business problem, we should ensure that it is indeed a problem that should be addressed by predictive modeling.</p>
<p>A conceptual exam task testing the syllabus learning outcome, “Describe the characteristics of predictive modeling problems.”, may ask:</p>
<p>Some common characteristics of predictive modeling problems include:</p>
<table>
<caption><span id="tab:unnamed-chunk-4">TABLE 3.1: </span>Characteristics of Predictive Modeling Problems</caption>
<thead>
<tr class="header">
<th align="left">Characteristic</th>
<th align="left">Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left"><strong>Issue</strong></td>
<td align="left">There is a clearly identified and defined business issue that needs to be addressed.</td>
</tr>
<tr class="even">
<td align="left"><strong>Questions</strong></td>
<td align="left">The issue can be addressed with a few well-defined questions (What data do we need? What is the target or outcome? What is the success criteria / how will the model performance be evaluated?)</td>
</tr>
<tr class="odd">
<td align="left"><strong>Data</strong></td>
<td align="left">Good and useful data is available for answering the questions above.</td>
</tr>
<tr class="even">
<td align="left"><strong>Impact</strong></td>
<td align="left">The predictions will likely drive actions or increase understanding.</td>
</tr>
<tr class="odd">
<td align="left"><strong>Better Solution</strong></td>
<td align="left">Predictive analytics likely produces a solution better than any existing approach.</td>
</tr>
<tr class="even">
<td align="left"><strong>Update</strong></td>
<td align="left">We can continue to monitor and update the models when new data becomes available.</td>
</tr>
</tbody>
</table>
<p>A typical predictive modeling problem will have most, but not necessarily all of these characteristics.</p>
</div>
<div id="problem-definition" class="section level5 unnumbered hasAnchor">
<h5>Problem Definition<a href="linear-models.html#problem-definition" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<p>After deciding to use predictive analytics to address the business issue of interest, we should define the problem as clearly as possible.</p>
<p>It is important to get to the root cause of the business issue and make it specific enough to be solvable.</p>
<p>The following strategies suggested can help us come up with a meaningful problem definition and give our project a higher chance of success:</p>
<ul>
<li><p><strong>Hypotheses:</strong> It is useful to use our prior knowledge of the business problem to ask questions and develop hypotheses that can prove or disprove in the course of our analytic work. Doing so helps us gain a clearer understanding of the business issue and guide our efforts in a clearly defined way. With the questions and hypotheses, we know where to focus on.</p></li>
<li><p><strong>KPI’s:</strong> We also need ways to assess the outcome by selecting appropriate key performance indicators (KPI’s), which will provide a quantitative basis to measure the success of the project. Naturally, these KPI’s should align with the overall business strategy as far as possible and show the client what key numbers will change as a result of your predictive analytic work.</p></li>
</ul>
</div>
<div id="constraints" class="section level5 unnumbered hasAnchor">
<h5>Constraints<a href="linear-models.html#constraints" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<p>As soon as we have defined the business problem clearly, it is important to evaluate the feasibility of solving the business problem and implementing the predictive analytic solution.</p>
<p>We want to make sure that the solutions we produce will work. Some considerations and constraints we should keep in ind when evaluating and prioritizing business problems include:</p>
<ul>
<li>The availability of easily accessible and high-quality data.</li>
<li>Implementation issues such as the presence of necessary IT infrastructure and technology to fit complex models efficiently, the timeline for completing the project, and the cost and effort required to maintain the selected model.</li>
</ul>
<p>Do we have the resources to implement complex models without freezing or crashing? If a model is operationally prohibitive to execute, then it makes sense to trade some prediction performance for ease of implementation. After all, if we cannot actually implement and apply the model in practice, then it is basically useless.</p>
</div>
</div>
<div id="stage-2-data-collection-and-validation" class="section level4 hasAnchor" number="3.1.2.2">
<h4><span class="header-section-number">3.1.2.2</span> Stage 2: Data Collection and Validation<a href="linear-models.html#stage-2-data-collection-and-validation" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Predictive analytics relies on models, which in turn are constructed from data. After defining the business problem properly, we will spend some time and effort collecting useful data that will underlie the predictive models we are going to build.</p>
<p>This data collection stage is more important and intricate than you probably first thought. You may have heard of the phrase “garbage in, garbage out”, better known as GIGO, which means that low-quality data is doomed to produce low-quality output.</p>
<p>Here we will learn some key considerations and some pitfalls to avoid related to the collection and use of data.</p>
<div id="data-design" class="section level5 unnumbered hasAnchor">
<h5>Data Design<a href="linear-models.html#data-design" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<p>To begin, we will contemplate here to collect the data to be used in the modeling stage.</p>
<p>At the data design stage, there are a few things we should take note of to maximize the usefulness of our data source, which is the entire set of observations that enter the dataset you collect.</p>
<ul>
<li><p><strong>Relevance:</strong> Intuitively, the more data we have, the more information available and the better, provided that the data is unbiased (e.g., representative of the environment where our predictive model will operate). To ensure that the data is relevant for our business problem, it is necessary to source the data from the right population and time frame.</p>
<ul>
<li><p><strong>Population:</strong> As obvious as this may seem, it is important that the data source aligns with the true population we are interested in. If the data source is not a reasonably good proxy of a representative sample, then taking a larger dataset does not help, we are only representing the bias over again and again.</p></li>
<li><p><strong>Time Frame:</strong> When selecting an appropriate time frame for our data, it is advisable to choose the time period which best reflects the business environment in which we will be implementing our models. In general, recent history is more predictive of the near future than distant history, so data one year ago should be more useful than data 10 years ago, other things equal.</p></li>
</ul>
<p><strong>Example 3.1.1: Problems with a Facebook Poll</strong></p>
<p>In November 2020, the Divide States of America (DSA) held its 60th presidential election, with Donald Dump running against Joe Hidden. To gauge public sentiments towards the election, Great News Network (GNN), a/an information outlet, set up a poll on Facebook allowing users to vote for their preferred presidential candidate. Any Facebook users could take part in the poll, which was open between August 2020 and September 2020.</p>
<p>Evaluate the quality of the data collected by GNN with respect to the population, time frame, and/or other items.</p>
<p><strong><em>Solution:</em></strong> There are quite a few problems with the data collected by GNN, some of which are:</p>
<ul>
<li><p><strong>Population:</strong> The votes were limited to Facebook users while the intended audience was supposed to be all eligible citizens of DSA. Users of social media outlets tend to be younger generations, so there may be a systematic mismatch between the voters of the poll and the general population.</p></li>
<li><p><strong>Time Frame:</strong> The objective of the poll was to understand voters’ preferences at the time of the November 2020 election. The poll, conducted between August 2020 and September 2020, was premature in a way and might fail to gauge public sentiments accurately (a lot could happen in October). It may be desirable for the poll to extend to October, right before the November election took place.</p></li>
<li><p><strong>Quality:</strong> By design, the data collected is susceptible to <strong><em>contamination</em></strong>. An individual may register for several accounts and take part in the poll several times. The repetitive votes are allowed by the poll could substantially bias the polling results.</p></li>
</ul>
<p><strong>Example 3.1.2 Considerations with Time Frame (Based on Exercise 2.1.1 in the PA Modules)</strong></p>
<p>Suppose that you are constructing a model to predict which retirement option a customer will choose. You have data from 1990 to 2020 containing your customers choices. In 2010, your company introduced a digital platform, which increased not only the variety of retirement options available, but also the richness of the supporting data available to customers when they made their decisions.</p>
<p>Discuss the pros and cons of including all of the data from 1990 relative to including only the data from 2010 for constructing your predictive model.</p>
<p><strong><em>Solution:</em></strong></p>
<ul>
<li><p><strong>Pros:</strong> Including all of the data from 1990 means more data. Other things equal, having more data is generally desirable as it makes model training more robust and less vulnerable to noise.</p></li>
<li><p><strong>Cons:</strong> In predictive modeling, it is important to select the period of time that best reflects the environment in which we will be implementing our model. In this case, something happened in 2010 which significantly affected the behavior of the outcome (retirement options) we are modeling. This makes the data prior to 2010 less likely to be predictive of current and future behavior of customers. Including such (outdated) data may make the predictions of retirement options in the current era less accurate.</p></li>
</ul></li>
<li><p><strong>Sampling:</strong> Sampling is the process of taking a subset of observations from the data source to generate our dataset.</p>
<p>Why do we need to bother with sampling? Why not use the entire data source? In reality, the underlying population is often too big to handle, and we need a smaller, much more manageable subset to form our data.</p>
<p>The observations sampled should closely resemble the business environment in which our predictive model will be applied in the future. In fact, a good time frame, discussed above, can be seen as a form of sampling, which the same goal of making sure that our data is representative.</p>
<p>There are a number of sampling methods commonly used to draw an appropriate sample:</p>
<ul>
<li><p><strong>Random Sampling:</strong> This is the simplest sampling scheme by which we “randomly” draw observations from the underlying population without replacement until we have the required number of observations. Each record is equally likely to be sampled.</p>
<ul>
<li><p>One common way to achieve <strong>random sampling</strong> is through a survey or questionnaire. Individuals in the population are invited to respond to the survey and contribute to the collected dataset.</p>
<p>However, voluntary surveys are known to be vulnerable to <strong><em>respondent bias</em></strong>. Only those interested will respond, which means that the respondents of a survey may make up a population different from the the population of interest.</p>
<p>For example, university students who respond to end-of-semester course evaluations tend to be those who feel strongly about the courses they take. These students with strong opinions may substantially bias the results of the course evaluations which are supposed to be for the general students.</p>
<p>Surveys also tend to suffer from a <strong><em>low response rate</em></strong>, unless there are financial incentives for people to respond.</p></li>
</ul></li>
<li><p><strong>Stratified Sampling:</strong> Stratified sampling involves dividing the underlying population into a number of non-overlapping “strata” or groups in a non-random fashion, and randomly sampling (with or without replacement) a set number of observations from each stratum.</p>
<p>This sampling method has the notable advantage of ensuring that every stratum is properly represented in the collected data. In Exam PA, the strata are usually formed with respect to the distribution of the target variable.</p>
<p>Here are a few special cases of <strong>stratified sampling</strong>:</p>
<ul>
<li><p><strong><em>Oversampling and Undersampling:</em></strong> These are sampling methods designed specifically for unbalanced data, and we will learn about them in detail in Subsection 4.1.4.</p></li>
<li><p><strong>Systematic Sampling:</strong> By systematic sampling, we draw observations according to a set pattern and there is no random mechanism controlling which observations are sampled.</p>
<p>For a population with 100,000 units, we may order them in a well-defined way and sample every tenth observation to get a smaller, more manageable sample of 10,000 units. Once the sampling rule has been set, the sampled observations are pre-determined.</p></li>
</ul></li>
</ul></li>
<li><p><strong>Granularity:</strong> Granularity refers to how precisely a variable in a dataset is measured, or equivalently, how detailed the information contained by the variable is. As an example, location, in ascending order of granularity, can be recorded in different degrees of precision:</p>
<ul>
<li><p>By Country (least granular)</p></li>
<li><p>By State</p></li>
<li><p>By City</p></li>
<li><p>By Zip Code</p></li>
<li><p>By Address (most granular)</p></li>
</ul>
<p>From country to address, we have more and more specific information about the location of an individual.</p>
<p>At the data design stage, it is a good idea to use a relatively high level of granularity. It is always possible to go from a higher level of granularity to a lower level of granularity down the track, but not the other way around.</p></li>
</ul>
</div>
<div id="data-quality-issues" class="section level5 unnumbered hasAnchor">
<h5>Data Quality Issues<a href="linear-models.html#data-quality-issues" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<p>Once we have collected the data from a data source representative of the population of interest using an appropriate sampling method, we have to take an important step before jumping straight into doing predictive modeling: <strong><em>Data Validation</em></strong>, the process of ensuring the quality and appropriateness of the data available.</p>
<p>If the collected data falls short of our standards, we may have to go back to the data collection step and search for an alternative dataset.</p>
<ul>
<li><p><strong>Reasonableness:</strong> For a dataset to be useful, the data values should, at a minimum, be reasonable, which can be checked by exploratory data analysis. Do the key statistics for the variables make sense in the context of the business problem? If one of the variables is income and one observation has a negative income, then almost certainly this is a recording error that should be fixed before we launch into model construction.</p></li>
<li><p><strong>Consistency:</strong> We also need to ensure that the records in the data are inputted consistently, meaning that the same basis and rules have been applied to all values, so that they can be directly compared with one another.</p>
<ul>
<li><strong><em>Numeric Variables:</em></strong> For numeric variables, the same unit should be used across all rows of the data for consistency. If one of the variables is weight, the nits values should be measured either all in kilograms or all in pounds. If there is a monetary variable, such as income, then the same currency should be used throughout.</li>
</ul></li>
<li><p><strong><em>Categorical Variables:</em></strong> We should make sure that the factor levels of categorical variables are defined and recorded consistently over time with no coding changes, like either the full name (e.g., Iowa) or the two-character abbreviation (e.g., IA) for the state of policy issue.</p></li>
</ul>
<pre><code>To check consistency, it is necessary to gain knowledge of how the data entries are recorded. To this end, the data dictionary (see the next point) that accompanies each PA exam project will be useful.

If we want to combine multiple datasets to form new variables (columns) or new observations (rows), then the consistency checks above extend to all constituent datasets to make sure that the concatenation works properly.

If we need to join datasets by a certain variable (column), it is especially important to ensure that the format of that variable is consistent across both datasets to ease matching, e.g., name should be recorded either as [first name] or [last name], [first name] so that each person can be identified uniquely. The possible existence of a middle name adds further complication.</code></pre>
<!-- -->
<ul>
<li><p><strong>Sufficient Documentation:</strong> A good dataset should also be sufficiently documented so that other users can easily gain an accurate understanding of different aspects of the data. The PA modules suggest that effective documentation should at least include the following information:</p></li>
<li><p>A description of the dataset overall, including the data source (how and when the dataset was collected).</p></li>
<li><p>A description of each variable in the data, including its name, definition, and format (range of values for numeric variables and the possible levels for categorical variables).</p></li>
<li><p>Notes about any past updates or other irregularities of the dataset.</p></li>
<li><p>A statement of accountability for the correctness of the dataset.</p></li>
<li><p>A description of the governance processes used to manage the dataset.</p></li>
</ul>
<p>Exam PA exam project describes the dataset in the “Business Problem” section and provides you with a <em>data dictionary</em>, which lists all the variables in the data together with their information. Here is a sample data dictionary for a hypothetical group of insurance policies:</p>
<table class="table table-striped table-hover table-condensed table-responsive" style="width: auto !important; margin-left: auto; margin-right: auto;">
<caption>
<span id="tab:unnamed-chunk-5">TABLE 3.2: </span>Sample Data Dictionary
</caption>
<thead>
<tr>
<th style="text-align:left;position: sticky; top:0; background-color: #FFFFFF;">
VariableName
</th>
<th style="text-align:left;position: sticky; top:0; background-color: #FFFFFF;">
Description
</th>
<th style="text-align:left;position: sticky; top:0; background-color: #FFFFFF;">
Values
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
<code>age</code>
</td>
<td style="text-align:left;">
The age of a policyholder
</td>
<td style="text-align:left;">
Integer, 20 to 65
</td>
</tr>
<tr>
<td style="text-align:left;">
<code>gender</code>
</td>
<td style="text-align:left;">
The gender of a policyholder
</td>
<td style="text-align:left;">
3 levels, “F”, “M”, and “0”
</td>
</tr>
<tr>
<td style="text-align:left;">
<code>type</code>
</td>
<td style="text-align:left;">
The risk type of a policyholder
</td>
<td style="text-align:left;">
2 levels, “standard” and “substandard”
</td>
</tr>
<tr>
<td style="text-align:left;">
<code>n_claims</code>
</td>
<td style="text-align:left;">
The number of claims submitted
</td>
<td style="text-align:left;">
Integer, 0 to 10
</td>
</tr>
<tr>
<td style="text-align:left;">
<code>claims</code>
</td>
<td style="text-align:left;">
The total amount of claims
</td>
<td style="text-align:left;">
Numeric, 0 to 30,451.8
</td>
</tr>
</tbody>
</table>
</div>
<div id="other-data-issues" class="section level5 unnumbered hasAnchor">
<h5>Other Data Issues<a href="linear-models.html#other-data-issues" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<p>Even if a dataset is representative of the population of interest and is of sufficient quality, there are additional regulatory and practical issues related to the collection and use of data that we need to be aware of.</p>
<ul>
<li><p><strong><em>PII</em></strong>: It is not uncommon that datasets contain personally identifiable information (PII). Examples of such include name, Social Security number, address, photographs and bio-metric needs.</p>
<ul>
<li>Anonymous: Needs to comply with the pertinent laws since processing of data is still required.</li>
<li>Data Security: Ensure that personal data receives sufficient protection such as encryption and access restrictions</li>
<li>Terms of Use: Be well aware of all terms and conditions and the privacy policy related to the collection and use of data.</li>
</ul></li>
<li><p><strong>Variables with Legal/Ethical Concerns</strong>: Race, Gender, Ethnicity, Age, Income, Disability Status may be controversial variables that are a part of the dataset, and differential treatment based on these variables may lead to unfair “discrimination” and regulation violations.</p>
<p>Proxies of Prohibited is occupation, which can be strongly correlated with gender, and coal mine workers. If we use occupation as a predictor, then even though we are not feeding gender directly into our model, we are still using the information about the gender indirectly.</p></li>
<li><p><strong>Target Leakeage</strong>: In predictive modeling, target leakage refers to the phenomenon that some predictors in a model include (“leak”) information about the target variable <strong><em>that will not be available when the model is applied in practice</em></strong>. These predictors are typically strongly associated with the target variable, but their values are not known until or after the target variable is observed. Remember that our aim when doing predictive analytics is to use the other variables to predict the target variable <strong><em>before</em></strong> it is observed, so these “predictors” cannot serve as predictors in reality. If we include these variables in the model construction process, then our model will appear to perform extraordinarily well.</p>
<p>Examples of Target Leakage:</p>
<ul>
<li><p>Suppose you are interest in predicting whether or not a policyholder of an insurance policy will incur a loss or not in a future period (binary target variable). If the number of incurred losses is one of the predictors you use, then it appears that you will be able to make the perfect predictions: <em><strong>By definition, an incurred loss must have arisen if the number of incurred losses is one or more</strong>. The number of incurred losses simply would not be available at the time we predict the occurrence of incurred losses in practice.</em></p></li>
<li><p>An extreme form of target leakage that is commonly tested on the exam is when the target variable itself is included as a predictor, or is used to develop new variables (e.g., principal components) for predicting the same target variable.</p>
<p>Exercise 3.1.7. *&amp; (Detecting target leakage) Consider the persinj dataset in Chapter 2 again, and treat amt as the target variable. The data dictionary is reproduced below:</p>
<table>
<colgroup>
<col width="50%" />
<col width="50%" />
</colgroup>
<thead>
<tr class="header">
<th>Variable</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><code>amt</code></td>
<td>Settled claim amount (continuous numeric variable)</td>
</tr>
<tr class="even">
<td><code>inj</code></td>
<td>Injury code, with seven levels: 1 (no injury), 2, 3, 4, 5, 6 (fatal), 9 (not recorded).</td>
</tr>
<tr class="odd">
<td><code>legrep</code></td>
<td>legal representation (0 = no, 1 = yes)</td>
</tr>
<tr class="even">
<td><code>op_time</code></td>
<td>Operational time (standardized amount of time elapsed between the when the injury was reported and the time when the claim was settled)</td>
</tr>
</tbody>
</table>
<ol style="list-style-type: decimal">
<li>Identify a variable in the dataset that may pose the problem of target leakage.</li>
<li>Suggest how to retain the variable in part (a) for constructing meaningful predictive models for amt.</li>
</ol>
<p><strong>Solution.</strong></p>
<ol style="list-style-type: decimal">
<li>The <code>op_time</code> variable may pose the problem of target leakage when amt is the target variable. According to the data dictionary, the value of op_time would not be known until a claim was settled, at which time the value of amt would be observed. Due to this timing issue, <code>op_time</code> ma not serve as a predictor of <code>amt</code> in practice.</li>
<li>We can first build a predictive model for <code>op_time</code> based on the variables other than <code>amt</code> and then use a separate predictive model to predict the <code>amt</code> based on the predicted value of <code>op_time</code> and/or other variables.</li>
</ol></li>
</ul></li>
</ul>
</div>
</div>
<div id="stage-3-exploratory-data-analysis" class="section level4 hasAnchor" number="3.1.2.3">
<h4><span class="header-section-number">3.1.2.3</span> Stage 3: Exploratory Data Analysis<a href="linear-models.html#stage-3-exploratory-data-analysis" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>After we have collected and validated the data, the next step is to perform <strong>Exploratory Data Analysis</strong>, which was discussed in Section @ref(2.2) with a focus on R programming and will further be illustrated in the case studies sections in the later part of this manual. The goals of exploratory data analysis are the same as those presented in Section @ref(2.2):</p>
<dl>
<dt><strong>Exploratory Data Analysis</strong></dt>
<dd>
<p>With the use of descriptive statistics and graphical displays, clean the data for incorrect, unreasonable, and inconsistent entries, and understand the characteristics of and the key relationships among variables in the data.</p>
<p>The observations we make may suggest an appropriate type of predictive model to use (e.g., GLMs vs. Decision Trees) and the best form for the predictors to enter the model.</p>
</dd>
</dl>
</div>
<div id="stage-4-model-construction-evaluation-and-selection-important" class="section level4 hasAnchor" number="3.1.2.4">
<h4><span class="header-section-number">3.1.2.4</span> Stage 4: Model Construction, Evaluation, and Selection [IMPORTANT!]<a href="linear-models.html#stage-4-model-construction-evaluation-and-selection-important" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Having defined the problem, collected some useful data, and taken a first look at the key variables, it is time for us to move onto the modelling phase and construct our predictive models in earnest.</p>
<div id="should-we-use-all-available-data-to-train-our-models" class="section level5 hasAnchor" number="3.1.2.4.1">
<h5><span class="header-section-number">3.1.2.4.1</span> <strong>Should we use all available data to train our models?</strong><a href="linear-models.html#should-we-use-all-available-data-to-train-our-models" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<p>You may be tempted to fit your predictive models directly to the whole set of data. Remember that one of the main goals when doing predictive modeling is to construct models that make good predictions. The fundamental question is is:</p>
<p>How do we go about designing a predictive model that performs well when it is applied to <strong><em>new, previously unseen</em></strong> data? In other words, how do we make sure that the model calibrated on <strong><em>past</em></strong> data excels in predicting <strong><em>future</em></strong> target values on the basis of <strong><em>future</em></strong> predictor values?</p>
<p>If we use all of the collected data for model fitting, that will leave no independent data for assessing the prediction performance of our models. It does not help much to evaluate the models on the same data on which they were built. After all, the models have seen those data and are not making genuine predictions. To ensure that our predictive models do a good job of describing the past data they have seen and, more importantly, prediction new, <em>future</em> observations, we need an unconventional way of leveraging our available data.</p>
</div>
<div id="training-test-set-split" class="section level5 hasAnchor" number="3.1.2.4.2">
<h5><span class="header-section-number">3.1.2.4.2</span> <strong>Training / Test Set Split</strong><a href="linear-models.html#training-test-set-split" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<p>To construct a model that thrives on future data, it is common practice in predictive analytics to partition our data into a few parts, each of which plays a different role in the model development and evaluation process.</p>
<p>The simplest way is to split the full data into two parts, the training set and the test set; see Figure <a href="#fig:figure-schematic-diagram"><strong>??</strong></a> for a schematic diagram.</p>
<div class="float" id="figure-schematic-diagram">
<img src="assets/images/figure-3-1-1.jpeg" alt="Figure 3.1.1: A schematic diagram of the training / test set split." />
<div class="figcaption"><strong>Figure 3.1.1:</strong> A schematic diagram of the training / test set split.</div>
</div>
<dl>
<dt><strong>Training Set</strong></dt>
<dd>
<p>The larger part of the full data (70-80%), the <em>training set</em>, or <em>training data</em>, is where you “train”, fit, or develop your predictive model to estimate the signal function <span class="math inline">\(f\)</span> and, if needed, the model parameters.</p>
<p>The training is typically done by optimizing a certain objective function that describes how well the model matches the training observations. The fitted model is denoted by <span class="math inline">\(\hat{f}\)</span> , where the hat emphasizes that it is an estimate of <span class="math inline">\(f\)</span>. After the model has been trained, it is ready for making predictions on the other part of the full data.</p>
</dd>
<dt><strong>Test Set</strong></dt>
<dd>
<p>Following the fitting, you will apply the trained model to make a prediction for each observation in the <em>test set</em>, or <em>test data</em>, and assess the prediction performance of the model according to certain performance metrics.</p>
<p>Observations in the test set <u><strong>did not participate in the model training process</strong></u> and will provide a much more objective ground for evaluating prediction performance when the model is applied to new, future data.</p>
<p>In contrast, evaluating the model on the training set will give an overly optimistic and somewhat misleading picture of its true predictive performance – the model has already seen the training observations and is merely fitting, not predicting those observations.</p>
</dd>
</dl>
<p><strong>How to Split the Data?</strong></p>
<p>There are many ways to do the training/test split.</p>
<ul>
<li>It can be done <strong><em>purely randomly</em></strong> according to pre-specified proportions,</li>
<li>Using special <strong><em>statistical techniques</em></strong> that ensure the distributions of the target variable in the two sets are comparable (stratified sampling).</li>
<li>If one of the variables is a <strong><em>time-variable</em></strong>, such as calendar year or month, and the behavior of the target variable over time is of interest, then one more way to make the split is on the basis of time, allocating older observations to the training set, and the more recent observations to the test set. Such “out-of-time” validation is conducted often for time series data, also known as longitudinal data, and is use for evaluating how well a model extrapolates time trends observed in the past to future periods.</li>
</ul>
<p><strong>How many observations should we assign to the two sets?</strong></p>
<p>The relative size of the training and test sets involves a trade-off:</p>
<ul>
<li>Having more training data will make for a more robust predictive model more capable of learning the patterns in the data and less susceptible to noise.</li>
<li>If too little data is set aside for the test set, however, the assessment of the prediction performance of the trained model on new, unseen observations will be less reliable.</li>
</ul>
<p><strong>How to use the training/test set split to rank competing models?</strong></p>
<p>If we have multiple candidate models, then we can perform the following steps:</p>
<ol style="list-style-type: decimal">
<li>Split the data into training / test sets..
<ul>
<li>The same training and test sets should be used across all candidate models.</li>
</ul></li>
<li>Fit the models to the training set.</li>
<li>Evaluate the quality of these models on the test set; and</li>
<li>Choose the one with the best test set performance according to a certain model selection criterion.</li>
</ol>
<p>When the “test set” above is for <em>selecting</em> the best model and is more commonly referred to as the <strong><em>validation set</em></strong>.</p>
<dl>
<dt><strong>Test /Validation Set</strong></dt>
<dd>
<p>In many predictive modeling textbooks, the test set is where we obtain an independent measure of the prediction performance of your <strong><em>chosen model</em></strong> <strong>when</strong> it is applied to data not involved in training or <strong><em>selecting</em></strong> the final model. The test set is held out until the end.</p>
<p>The usage in this manual conforms to what you will likely see in PA exam projects, where we are more concerned with finding the best predictive model and less with exactly how well it performs on independent data. There is usually no independent test data set aside for final evaluation.</p>
</dd>
</dl>
</div>
<div id="common-performance-metrics" class="section level5 hasAnchor" number="3.1.2.4.3">
<h5><span class="header-section-number">3.1.2.4.3</span> Common Performance Metrics<a href="linear-models.html#common-performance-metrics" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<p>After making the training / test set split and fitting the predictive model (to the training set), it is time for us to evaluate the performance of the model on the training set or test set with respect to an appropriate metric, which measures thee extent to which the model model matches the observations in the data.</p>
<p>This requires a <strong>loss function</strong>, which quantifies the discrepancy between the actual predicted values for each observation of the target variable mathematically. Here are the three most commonly used loss functions:</p>
<table>
<thead>
<tr class="header">
<th>Name</th>
<th>Loss Function</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Square Loss</td>
<td><span class="math inline">\(L(y_{i},\hat{y}_{i}) = (y_{i}-\hat{y}_{i})^2\)</span></td>
</tr>
<tr class="even">
<td>Absolute Loss</td>
<td><span class="math inline">\(L(y_{i},\hat{y}_{i}) = |y_{i}-\hat{y}_{i}|\)</span></td>
</tr>
<tr class="odd">
<td>Zero-one Loss</td>
<td><span class="math inline">\(L(y_{i},\hat{y}_{i}) = 1, if y &lt;&gt; \hat{y}_{i}\)</span></td>
</tr>
</tbody>
</table>
<p>Here, <span class="math inline">\(y_{i}\)</span> and <span class="math inline">\(\hat{y}_{i}\)</span> are respectively the observed and predicted values (produced by a predictive model) of the target variable for the ith observation in the data at hand. In each case, the loss function is the smallest when <span class="math inline">\(y_{i}\)</span> = <span class="math inline">\(\hat{y}_{i}\)</span> , and grows with the difference between <span class="math inline">\(y_{i}\)</span> and <span class="math inline">\(\hat{y}_{i}\)</span>.</p>
<p>The precise choice of the loss function depends closely on the nature of the target variable (numerical or categorical), or equivalently the nature of the prediction problem (regression or classification).</p>
<p><strong>Regression Problems:</strong> When the target variable is numeric, we can assess the performance of a model by looking at the size of the discrepancy between each observed value of the target variable and the corresponding predicted value, <span class="math inline">\(y_{i} - \hat{y}_{i}\)</span>. For a given observation, we call this discrepancy the <strong><em>residual</em></strong> if the observation is in the training set, and <strong><em>prediction error</em></strong> if the observation is in the test set. A commonly used metric that aggregates all such discrepancies is the square loss function <span class="math inline">\(L(y_{i},\hat{y}_{i}) = (y_{i}-\hat{y}_{i})^2\)</span> and provides an overall performance measure is the <strong>Root Mean Squared Error (RMSE),</strong> defined on the training and test sets as:</p>
<div class="float">
<img src="assets/images/03-figure-rsme-equation.png" alt="Equation: Root Mean Squared Error Equation" />
<div class="figcaption"><strong><em>Equation: Root Mean Squared Error Equation</em></strong></div>
</div>
<p>By design, the individual residuals or prediction errors are squared (so that +/i discrepancies will not cancel out), summed, and averaged, followed by taking a square root, to yield the RMSE.</p>
<p>The <strong><em>smaller</em></strong> the RMSE, the <strong><em>better</em></strong> the fit of the model to the training / test data.</p>
<ul>
<li><p><strong>Mean Squared Error (MSE):</strong> In your prior studies, you may have seen the concept of <strong>Mean Squared Error (MSE)</strong>, which is simply the square of the RMSE:</p>
<p><img src="assets/images/03-figure-mse-equation.png" /></p>
<p>Due to the square relations, whether you use RMSE or MSE does not really matter, but the advantage of the RMSE over the MSE is that the <strong>RMSE has the same unit as the target variable, making it easier to interpret</strong>.</p></li>
<li><p><strong>Mean Absolute Error (MAE):</strong> Instead of the square loss function, we could have used the absolute loss function <span class="math inline">\(L(y_{i},\hat{y}_{i}) = |y_{i}-\hat{y}_{i}|\)</span>, in which case the resulting metric is called the mean absolute error (MAE) or mean absolute derivation (MAD):</p>
<p><img src="assets/images/03-figure-mean-absolute-error.png" /></p>
<p>While the loss function places a much smaller weight on large loss than the square loss function and therefore makes the fitted model more robust against outliers, the square loss function is more frequently used i practice because it is differentiable and eases model fitting, which involves optimizing objective functions.</p></li>
<li><p><strong>Fitting vs. Evaluation:</strong> The loss function that defines the performance metric can be the same or different as the one that defines the objective function for training the model. We can for example, use the square loss function to estimate the model parameters (e.g., the method of least squares for linear models) and use the absolute loss function to measure the model performance.</p></li>
</ul>
<p><strong>Exercise 3.1.8 (Calculation of test RMSE)</strong></p>
<p>You have fitted a predictive model and applied it to the following test set with five observations:</p>
<table>
<caption><em><strong>Table:</strong> Test Set of Five Observations</em></caption>
<thead>
<tr class="header">
<th align="center">Observation Number (i)</th>
<th align="center"><span class="math inline">\(y_{i}\)</span></th>
<th align="center"><span class="math inline">\(\hat{y}_{i}\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">1</td>
<td align="center">2</td>
<td align="center">4</td>
</tr>
<tr class="even">
<td align="center">2</td>
<td align="center">5</td>
<td align="center">3</td>
</tr>
<tr class="odd">
<td align="center">3</td>
<td align="center">6</td>
<td align="center">9</td>
</tr>
<tr class="even">
<td align="center">4</td>
<td align="center">8</td>
<td align="center">3</td>
</tr>
<tr class="odd">
<td align="center">5</td>
<td align="center">4</td>
<td align="center">6</td>
</tr>
</tbody>
</table>
<p>Calculate the test RMSE and test MAE.</p>
<p><strong><em>Solution:</em></strong> The test RMSE is the square root of the average squared prediction error</p>
<p>Test RMSE = <span class="math inline">\([(1/5)*((2-4)^2 + (5-3)^2 + (6-9)^2 + (8-3)^2 + (4-6)^2)]^{1/2}\)</span> = <strong>3.0332</strong></p>
<p>Test MAE = <span class="math inline">\((1/5)*(|2-4| + |5-3| + |6-9)| + |8-3| + |4-6|)\)</span> = <strong>2.8</strong></p>
<p><strong>Classification Problems:</strong> For categorical target variables. the predictions are simply labels, or certain factor levels of the target variable, and they cannot be handled algebraically, so the difference may not be well-defined or may not make sense even if it is defined.</p>
<p>Instead of using the square loss or absolute loss function, we use the zero-one loss function (which is actually an indicator function) to come <span class="math inline">\(y_i\)</span> and <span class="math inline">\(\hat{y}_{i}\)</span> meaningfully:</p>
<p><img src="assets/images/03-classification-error-rate.png" /></p>
<p>Both RMSE and classification error rate are universal performance metrics that apply to general regression and classification problems, respectively. They can be computed on the training set as ewll as the test set, but as discussed above, we are mainly interested in these performance metrics on the test set, because they measure how well the model fits the test observations, or equivalently how well the model makes predictions on future, unseen data. The goodness of fit of the model to the training set is usually of secondary importance.</p>
<p>In the rest of this manual, we will use the generic term training error to mean the training RMSE or training classification error rate (depending on whether the problem is regression or classification), and test error to mean the test (R)MSE or test classification error rate, as long as there is no confusion.</p>
<p><strong>Cross Validation</strong></p>
<p>In predictive analytics, there is a remarkably useful and widely applicable technique, known as <strong>Cross Validation (CV)</strong>, that is an enhanced version of the training/test set split described above. The power of this technique is that it provides means to assess the prediction performance of a model <strong><em>without</em></strong> <strong><em>using using additional test data</em></strong>. In Exam PA, it also serves an important purpose:</p>
<dl>
<dt><strong>Purpose of Cross Validation</strong></dt>
<dd>
<p>To select the values of <em>hyper-parameters (tuning parameters)</em>, which are parameters that control some aspect of the fitting process itself.</p>
</dd>
</dl>
<p>As we mentioned above, model fitting typically involves optimizing a certain objective function, and hyper-parameters often play a role either in the mathematical expression of the objective function, or in the set of constraints defining the optimization problem.</p>
<p>We will encounter concrete examples of hyperparameters in the context of GLMs and Decision Trees in later chapters.</p>
<p>In fact, almost all predictive models in modern use contain one or more hyperparameters that serve as indices of model complexity and need to be tuned carefully to achieve optimal model performance, speaking to their importance in predictive analytics.</p>
<hr />
<p><strong>Example 3.1.9 (Which Types of are Hyperparameters?)</strong></p>
<p>An actuary is fitting the simple linear regression model <span class="math inline">\(Y = \beta_{0}+\beta_{1}X+\epsilon\)</span> to a set of training data <span class="math inline">\(n_{tr}\)</span> observations. The estimates of <span class="math inline">\(\beta_{0}\)</span> and <span class="math inline">\(\beta_{1}\)</span> are determined by minimizing the function, where <span class="math inline">\(\lambda\)</span> is a given positive constant, for <span class="math inline">\(\beta_{0}\)</span> and <span class="math inline">\(\beta_{1}\)</span> .</p>
<p>Determine which of the parameters is a hyperparameter.</p>
<p><strong>Solution:</strong> The estimates <span class="math inline">\(\beta_{0}\)</span> and <span class="math inline">\(\beta_{1}\)</span> are output of the minimization problem above, while <span class="math inline">\(\lambda\)</span> is an input that goes into the minimization problem. Therefore, only <span class="math inline">\(\lambda\)</span> is a hyperparameter.</p>
<hr />
<p>The tricky part of hyperparameters is that we have to provided their values in advanced; they are not optimized as part of the model fitting algorithm, so it is impossible to select their values given the training set alone.</p>
<p>In theory, we can further divide the training data into two parts. One part for training the model for a given set of hyperparameters under consideration, and one part for evaluating the predictive performance of the model.</p>
<p>Then we select the combination of hyperparameters that gives rise to the best performance. In practice, this will reduce the size of the training set and undesirably undermine the reliability of the model development process.</p>
<p><strong>Cross Validation (CV)</strong> is an ingenious method for tuning hyperparameters without having to further divide the training set. Given the training set, CV (more formally called <strong><em>k-fold CV</em></strong>) works by performing a series of splits <em>repeatedly</em> across the dataset. The procedure is as follows:</p>
<ul>
<li><strong>Step 1:</strong> For a given positive integer <em>k</em>, randomly split the training data into <em>k</em> folds of approximately equal size. The value of <em>k</em> can range between 2 (smallest) and <span class="math inline">\(n_{tr}\)</span> (largest). A common choice for <em>k</em> in practice is 10, which is the default value in many model fitting functions in R.</li>
<li><strong>Step 2:</strong>
<ul>
<li><p>One of the <em>k</em> folds is left out and the predictive model is fitted to the remaining <em>k-1</em> folds. Then the fitted model is used to make a prediction for each observation in the left-out fold and a performance metric is computed on that fold. This is a valid estimate of the prediction performance of the model because observations in the excluded fold are not part of the model training process. In other words, they are something unseen by the model, and we are really making predictions.</p></li>
<li><p>Then repeat this process with each fold left out in turn to get <em>k</em> performance values, <span class="math inline">\(V_{1}, ...,V_{k}\)</span> (e.g., RMSE for a numeric target and classification error rate for a categorical target).</p></li>
<li><p>The overall prediction performance of the model can be estimated as the average of the <em>k</em> performance values, known as the <strong>CV error:</strong></p>
<p><span class="math display">\[
\fbox{CV error = (V_{1}+...+V_{k})/k}
\]</span></p></li>
</ul></li>
</ul>
<p>If we have a set of candidate hyperparameter values, then we can perform Step 2 above for each set of values under consideration and select the combination that produces the best model performance.</p>
<p>Then the model is fitted to the <strong><em>entire training set</em></strong> (i.e., the <em>k</em> folds together) using this optimal combination of hyperparameter values and evaluated on the test set.</p>
<p>In fact, CV is such a useful technique that it is automatically built into some of the model fitting algorithms such as regularization and decision trees for tuning model parameters for best performance.</p>
<hr />
<p><strong>Exercise 3.1.10 (Mechanics of <em>k</em>-fold CV)</strong></p>
<p>An actuary has a dataset with n = 50 observations and p = 22 predictors. He is using 10-fold cross-validation to select from a variety of available models.</p>
<p>Calculate the approximate number of observations used for the training models in each round of cross-validation procedure.</p>
<p><strong>Solution:</strong> 10-fold CV involves (randomly) dividing the data into 10-folds of approximately equal size, each with about 50/10 = 5 observations. In each round, one fold is left out and the other 9 folds, with approximately 9(5) = 45 observations, are used to train a model.</p>
<hr />
<p><strong>Selecting the Best Model</strong></p>
<p>After fitting each candidate model under consideration to the training set (possibly using CV to tune its hyperparameters) and evaluating its performance on the test set, it is time to select the model that is the “best” in a certain sense. Here are some important considerations that come into play in the selection of the best model:</p>
<ul>
<li><strong>Prediction Performance:</strong> Choosing the model with the best prediction performance is relatively easy and objective. All we have to do is <u><strong><em>pick the model with the smallest test-RMSE</em></strong></u> for a regression problem, or <u><strong><em>smallesttest classification error rate</em></strong></u> for a classification problem. We can also use alternative evaluation metrics that capture more specific aspects of prediction performance if appropriate.</li>
<li><strong>Interpretability:</strong> A model that makes good predictions is not always preferable, if it is like a black box and its end users have little idea what goes into the decision-making process. For the final model to earn trust from stakeholders with no expertise in predictive analytics, it should be reasonably interpretable, meaning that the model predictions should be easily explained in terms of the predictors and lead to specific actions or insights.</li>
<li><strong>Ease of Implementation:</strong> Other things equal, the easier for a model to be implemented (computationally, financially, or logistically), the better the model. If the model requires prohibitive resources to construct and maintain (e.g., it takes forever to fit the model, or it is costly to collect values of the necessary predictors), then its end users may not afford to use it in the first place, even if it makes superb predictions.</li>
</ul>
<p>As we will see in the case studies sections, it is not uncommon for prediction performance to be in conflict with interpretability and ease oimplementation, so the selection of the best model in practice almost always involves a compromise between these conflicting criteria, and the business problem can have quite a strong influence on the “final winner”.</p>
<ul>
<li><p>If the focus of the project you receive in Exam PA is on prediction, then it is worth selecting a model that produces good predictions of the target variable even if the model is rather non-transparent or costly to implement.</p></li>
<li><p>If interpretation is the primary concern, then it is natural to select a relatively simple, interpretable model that clearly shows the relationship between the target variable and the predictors. Using such a model, we can better understand the association between the two groups of variables and make inference about the underlying data-generating mechanism, thereby making better decisions. Its prediction performance is of secondary importance.</p></li>
</ul>
</div>
</div>
<div id="stage-5-model-validation" class="section level4 hasAnchor" number="3.1.2.5">
<h4><span class="header-section-number">3.1.2.5</span> Stage 5: Model Validation<a href="linear-models.html#stage-5-model-validation" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>After coming up with a tentative model, which appears to be the best among all models you have constructed, we should not rush to recommend it to the client. Instead, we should <em>validate</em> it by inspecting it for any obvious deficiencies and ensuring that the assumptions behind the model are largely satisfied. If found, the deficiencies can be used to improve the specification of the model and we will go back to Stage 4. As mentioned at the beginning of this subsection, model building is often an iterative process.</p>
<p>There are different kinds of model validation techniques. They may be performed on either the training set or the test set, and they may be model-dependent in the sense that they are specific to particular types of predictive models.</p>
<ul>
<li><p><strong><em>Training Set:</em></strong> For GLMs, there is a set of model diagnostic tools to check the model assumptions based on the training set. We will learn these tools in Subsections 3.2.2 and 4.1.4.</p></li>
<li><p><strong><em>Test Set:</em></strong> A general model validation technique applicable to both GLMs and Decision Trees is to compare the predicted values and the observed values of the target variable on the test set. The comparison can be quantitative (summarized by a number) or graphical (in form of a plot). If we use graphical means and if the model works well, then the plot of the predicted against the observed values not only should fall on the 45-degree straight line passing through the origin quite closely, the deviation from the line should have no systematic patterns. Here are two example plots:</p>
<div class="float">
<img src="assets/images/03-model-validation.png" alt="Figure: Model Validation Graphs" />
<div class="figcaption">Figure: Model Validation Graphs</div>
</div></li>
</ul>
<p><strong>For this validation technique to be meaningful, it is vitally important that we compare the predicted values and the observed values on the test set, not the training set.</strong></p>
<p>If we perform the comparison on the training set, then we are only assessing the goodness-of-fit of the model to the training data, not its prediction performance on unseen data. A model may produce actual and “predicted” values on the training set that are in close agreement simply because it is a complex and over-fitted model.</p>
<p>Another validation technique used in some old past PA exams is to compare the selected model to an existing baseline model, again on the test set.</p>
<p>This model is usually a primitive model, such as an Ordinary Least Squares Linear Model (if the selected model is a GLM) or a model that has no predictors and simply uses the average of the target variable on the training set for prediction. The model will provide a benchmark which any selected model should beat as a minimum requirement.</p>
<p>After the selected model has been validated, it can be recommended to the client for their consideration and comments.</p>
</div>
<div id="stage-6-model-maintenance" class="section level4 hasAnchor" number="3.1.2.6">
<h4><span class="header-section-number">3.1.2.6</span> Stage 6: Model Maintenance<a href="linear-models.html#stage-6-model-maintenance" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>If the client gladly accepts the selected model, then we can put it to actual use and maintain it over time so that it remains useful in the future. Here are some of the steps we can take:</p>
<ul>
<li><p><strong>Adjust the Business Problem:</strong> At times, it may help to alter the business problem to take the following into account:</p>
<ul>
<li>A surprise discovery, e.g., a certain outcome turns out to be far more common or impactful than initially thought</li>
<li>Changes in external factors, e.g., market conditions, regulations that may cause initial assumptions to shift, and justify modifying the business problem to incorporate the new conditions.</li>
</ul></li>
<li><p><strong>Consult with Subject Matter Experts:</strong></p>
<ul>
<li>If there are new findings that do not fit your current understanding of the business problem or modeling issues that cannot be easily resolved purely on predictive analytics</li>
<li>To understand the limitations on what can reasonably be implemented.</li>
</ul></li>
<li><p><strong>Gather Additional Data:</strong></p>
<ul>
<li>Gather new observations and retrain the model to ensure that it will continue to be predictive in new environments, and to improve its robustness and prediction performance.</li>
<li>Gather new variables, or additional predictors.</li>
</ul></li>
<li><p><strong>Apply New Types of Models:</strong> Try new types of models with different strengths and weaknesses when new technology or implementation possibilities are available.</p></li>
<li><p><strong>Refine Existing Models:</strong> Try new combinations or transformations of predictors, different training/test splits, alternative hyperparameter values, performance metrics etc.</p></li>
<li><p><strong>Field Test Proposed Model:</strong> Implement the recommended model in the exact way it will be used to gain users’ confidence. This is particularly important when the business problem, data, or type of model is new.</p></li>
</ul>
</div>
</div>
<div id="bias-variance-trade-off" class="section level3 hasAnchor" number="3.1.3">
<h3><span class="header-section-number">3.1.3</span> Bias-Variance Trade-Off<a href="linear-models.html#bias-variance-trade-off" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>This subsection and the next will delve into the model construction stage (Stage 4) more carefully and introduce, in a statistical framework, the technical considerations that go into designing a good predictive model.</p>
<p><strong>Key Idea: A complex model does not equal a good predictive model</strong></p>
<p>If you are just beginning to learn predictive analytics, you may be tempted to create a super sophisticated model with a huge number of predictors, some of which may be only marginally related to the target variable, in attempt to maximize the prediction performance of the model.</p>
<p>After all, you don’ want the model to miss any potentially useful information. To your surprise, such a complex model does not necessarily make good predictions, contrary to your original intention. As will be discussed in this subsection and substantiated in the rest of the manual, an extremely important mentality to keep in mind when doing predictive analytics is:</p>
<dl>
<dt>Imporatnt Mentality for Predictive Analytics</dt>
<dd>
<p>Complexity does <strong><em>not</em></strong> guarantee good prediction performance</p>
</dd>
</dl>
<p><strong>Goodness of fit vs. Prediction Accuracy</strong></p>
<p>While complexity does not correlate very well with prediction performance (measured by the test error), it is intimately related to the goodness of fit to the training data (measured by the training error). In other words, goodness of fit to the training data is not quite the same as prediction accuracy, and the two types of error behave very differently with complexity, as Figure 3.1.3 below shows:</p>
<p><img src="assets/images/03-figure-goodness-fit-prediction-performance.png" /></p>
<p>In terms of the training and test errors, we can describe the problems with a model that is either too simple or too complex using the following 2x2 table:</p>
<table>
<colgroup>
<col width="33%" />
<col width="33%" />
<col width="33%" />
</colgroup>
<thead>
<tr class="header">
<th>Scenario</th>
<th>Small Test Error</th>
<th>Large Test Error</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Small Training Error</strong></td>
<td>Model is optimal</td>
<td>Model is overfitted</td>
</tr>
<tr class="even">
<td><strong>Large Training Error</strong></td>
<td>Check your code for errors, this is too good to be true and rarely happens in real life</td>
<td>Model is underfitted</td>
</tr>
</tbody>
</table>
<p>The two undesirable scenarios in the last column of the table, both involve having a large test error, deserve a separate discussion.</p>
<p><strong>Case 1: Underfitting</strong></p>
<p>In the left tail of Figure 3.1.3, the model is too simple to capture the training data effectively. Without learning enough about the signal of the data, an <strong><em>underfitted</em></strong> model matches both the training and test data poorly, and is marked by relatively large training error as well as a relatively large test error. This is akin to a lazy student barely studies (“underfitted”) and, not surprisingly, bombs their math test.</p>
<p><strong>Case 2: Overfitting</strong></p>
<p>In the right tail of Figure 3.1.3, the model, which is overwhelmingly complex, is overfitting the data. It is fitting the data too hard and goes to extraordinary lengths to capture:</p>
<ul>
<li>The signal, which represents general relationships between the target variable and predictors, applicable to both the trianing and test data.</li>
<li>The noise, which represents patterns that are just caused by random chance rather than by true, general properties of the unknown function <span class="math inline">\(f\)</span>. These patterns are specific to the training set (i.e., they do not exist in the test data), but are mistreated as if they were signal.</li>
</ul>
<hr />
<p><strong>Decomposition of the Expected Test Error</strong></p>
<p>To better understand the U-shape behavior that characterizes the test MSE, it is instructive to break it down into different components, and examine how each of these components behaves. Although such a decomposition is mainly of theoretical nature and you don’t often calculate the individual components of the formula in Exam PA, keeping the decomposition in mind will help you understand what it takes to produce good predictions. In particular, it reinforces the practically important idea that more complex models are not necessarily superior models.</p>
<p>For simplicity, we will consider a numeric target variable so that the <strong>(R)MSE</strong> is an appropriate performance metric (categorical target variables admit a similar decomposition).</p>
<p>For a given set of predictor values <span class="math inline">\(X_{0}\)</span>, the decomopsition reads:</p>
<p><img src="assets/images/03-expected-test-error-bias-variance.png" /></p>
</div>
<div id="feature-generation-and-selection" class="section level3 hasAnchor" number="3.1.4">
<h3><span class="header-section-number">3.1.4</span> Feature Generation and Selection<a href="linear-models.html#feature-generation-and-selection" class="anchor-section" aria-label="Anchor link to header"></a></h3>
</div>
</div>
<div id="linear-models-conceptual-foundation" class="section level2 hasAnchor" number="3.2">
<h2><span class="header-section-number">3.2</span> Linear Models: Conceptual Foundation<a href="linear-models.html#linear-models-conceptual-foundation" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="model-formulation" class="section level3 hasAnchor" number="3.2.1">
<h3><span class="header-section-number">3.2.1</span> Model Formulation<a href="linear-models.html#model-formulation" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div id="model-equation" class="section level4 hasAnchor" number="3.2.1.1">
<h4><span class="header-section-number">3.2.1.1</span> Model Equation<a href="linear-models.html#model-equation" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<ul>
<li>Target variable <span class="math inline">\(Y\)</span> is <strong>numeric</strong>.</li>
<li><span class="math inline">\(Y\)</span> is related to predictors <span class="math inline">\(X_1, X_2, \dots, X_p\)</span> via the approximately <strong>linear</strong> relationship</li>
</ul>
<p><span class="math display">\[
Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \dots + \beta_p X_p + \varepsilon.
\]</span></p>
<ul>
<li>Linear signal function <span class="math inline">\(f(X)\)</span>.</li>
</ul>
<div id="some-terms" class="section level5 hasAnchor" number="3.2.1.1.1">
<h5><span class="header-section-number">3.2.1.1.1</span> Some terms<a href="linear-models.html#some-terms" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<ul>
<li><span class="math inline">\(p = 1\)</span>: <a href="#">Simple linear regression model</a></li>
<li><span class="math inline">\(p \geq 2\)</span>: <a href="#">Multiple linear regression model</a></li>
<li><span class="math inline">\(\beta_0\)</span>: Intercept</li>
<li><span class="math inline">\(\beta_j\)</span>: Coefficient (or slope) of the <span class="math inline">\(j\)</span>th predictor</li>
</ul>
</div>
</div>
<div id="training-data-y_i-x_i1-x_i2-dots-x_ip_i1n_tr" class="section level4 hasAnchor" number="3.2.1.2">
<h4><span class="header-section-number">3.2.1.2</span> Training Data: <span class="math inline">\(\{(Y_i, X_{i1}, X_{i2}, \dots, X_{ip})\}_{i=1}^{n_{tr}}\)</span><a href="linear-models.html#training-data-y_i-x_i1-x_i2-dots-x_ip_i1n_tr" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<ul>
<li><strong>Tabular (data frame) format</strong>:</li>
</ul>
<table style="width:100%;">
<colgroup>
<col width="38%" />
<col width="26%" />
<col width="35%" />
</colgroup>
<thead>
<tr class="header">
<th>Training Observation</th>
<th>Target <span class="math inline">\(Y\)</span></th>
<th>Predictors</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td></td>
<td><span class="math inline">\(Y\)</span></td>
<td><span class="math inline">\(X_1\)</span> <span class="math inline">\(X_2\)</span> … <span class="math inline">\(X_p\)</span></td>
</tr>
<tr class="even">
<td>1</td>
<td><span class="math inline">\(Y_1\)</span></td>
<td><span class="math inline">\(X_{11}\)</span> <span class="math inline">\(X_{12}\)</span> … <span class="math inline">\(X_{1p}\)</span></td>
</tr>
<tr class="odd">
<td>2</td>
<td><span class="math inline">\(Y_2\)</span></td>
<td><span class="math inline">\(X_{21}\)</span> <span class="math inline">\(X_{22}\)</span> … <span class="math inline">\(X_{2p}\)</span></td>
</tr>
<tr class="even">
<td>…</td>
<td>…</td>
<td>…</td>
</tr>
<tr class="odd">
<td><span class="math inline">\(n_{tr}\)</span></td>
<td><span class="math inline">\(Y_{n_{tr}}\)</span></td>
<td><span class="math inline">\(X_{n_{tr},1}\)</span> <span class="math inline">\(X_{n_{tr},2}\)</span> … <span class="math inline">\(X_{n_{tr},p}\)</span></td>
</tr>
</tbody>
</table>
<ul>
<li><strong>Equation format</strong>:</li>
</ul>
<p><span class="math display">\[
Y_i = \beta_0 + \beta_1 X_{i1} + \beta_2 X_{i2} + \dots + \beta_p X_{ip} + \varepsilon_i, \quad i = 1, \dots, n_{tr}
\]</span></p>
<ul>
<li><strong>A default assumption</strong>:</li>
</ul>
<p><span class="math display">\[
\varepsilon_i \sim \text{i.i.d. } N(0, \sigma^2)
\]</span></p>
</div>
<div id="model-quantities" class="section level4 hasAnchor" number="3.2.1.3">
<h4><span class="header-section-number">3.2.1.3</span> Model Quantities<a href="linear-models.html#model-quantities" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<div id="on-the-training-set" class="section level5 hasAnchor" number="3.2.1.3.1">
<h5><span class="header-section-number">3.2.1.3.1</span> On the training set<a href="linear-models.html#on-the-training-set" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<ul>
<li>Fitted value <span class="math inline">\(\hat{Y}_i = \beta_0 + \beta_1 X_1 + \cdots + \beta_p X_{ip}\)</span></li>
<li>Residual <span class="math inline">\(e_i = Y_i - \hat{Y}_i\)</span> (useful for “diagnosing” a linear model)</li>
<li>Residual SS (RSS): <span class="math inline">\(RSS = \sum_{i} e_i^2\)</span> (the lower, the better the fit to the training set)</li>
</ul>
</div>
<div id="on-the-test-set" class="section level5 hasAnchor" number="3.2.1.3.2">
<h5><span class="header-section-number">3.2.1.3.2</span> On the test set<a href="linear-models.html#on-the-test-set" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<ul>
<li>Predicted value <span class="math inline">\(\hat{Y}^* = \beta_0 + \beta_1 X_1^* + \cdots + \beta_p X_p^*\)</span>, where <span class="math inline">\((1, X_1^*, \ldots, X_p^*)\)</span> is a vector of predictor values of interest.</li>
<li>Prediction error <span class="math inline">\(Y^* - \hat{Y}^*\)</span> (the lower, the more predictive the model)</li>
</ul>
</div>
</div>
<div id="model-quantities-cont." class="section level4 hasAnchor" number="3.2.1.4">
<h4><span class="header-section-number">3.2.1.4</span> Model Quantities (Cont.)<a href="linear-models.html#model-quantities-cont." class="anchor-section" aria-label="Anchor link to header"></a></h4>
<div id="additional-quantities-computed-on-the-training-set" class="section level5 hasAnchor" number="3.2.1.4.1">
<h5><span class="header-section-number">3.2.1.4.1</span> Additional quantities computed on the training set<a href="linear-models.html#additional-quantities-computed-on-the-training-set" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<ul>
<li><strong>t-statistic</strong>: <span class="math inline">\(t(\hat{\beta_j}) = \frac{\hat{\beta_j}}{\text{standard error of } \hat{\beta_j}}\)</span>
<ul>
<li>Can be used to test the significance of <span class="math inline">\(X_j\)</span> in the presence of all other predictors.</li>
<li>The larger in magnitude, the more significant.</li>
</ul></li>
<li><strong>F-statistic</strong>
<ul>
<li>Can be used to assess the joint significance of all of the <span class="math inline">\(p\)</span> predictors, <span class="math inline">\(X_1, \ldots, X_p\)</span>.</li>
<li>The larger, the more significant (but it doesn’t tell which predictors are significant).</li>
</ul></li>
<li><strong>Coefficient of determination</strong>
<ul>
<li>On a scale from 0 to 1.</li>
<li>The larger, the better the fit to the training set.</li>
</ul></li>
</ul>
</div>
</div>
</div>
<div id="model-evaluation-and-validation" class="section level3 hasAnchor" number="3.2.2">
<h3><span class="header-section-number">3.2.2</span> Model Evaluation and Validation<a href="linear-models.html#model-evaluation-and-validation" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div id="general-performance-metric" class="section level4 hasAnchor" number="3.2.2.1">
<h4><span class="header-section-number">3.2.2.1</span> General Performance Metric<a href="linear-models.html#general-performance-metric" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<ul>
<li><span class="math inline">\(Y\)</span> is numeric <span class="math inline">\(\Rightarrow\)</span> Test (R)MSE</li>
</ul>
<p><span class="math display">\[
\sqrt{\frac{1}{n_{\text{test}}} \sum_{i=1}^{n_{\text{test}}} (Y_i - \hat{Y}_i)^2}
\]</span></p>
<p>can be used.</p>
<div id="other-metrics-on-the-test-set" class="section level5 hasAnchor" number="3.2.2.1.1">
<h5><span class="header-section-number">3.2.2.1.1</span> Other metrics on the test set<a href="linear-models.html#other-metrics-on-the-test-set" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<ul>
<li>Test <strong>loglikelihood</strong> (Exam STAM)</li>
<li>Test <strong>coefficient of determination</strong> <span class="math inline">\(R^2\)</span></li>
</ul>
</div>
<div id="all-of-them-are-equivalent-because" class="section level5 hasAnchor" number="3.2.2.1.2">
<h5><span class="header-section-number">3.2.2.1.2</span> All of them are equivalent because…<a href="linear-models.html#all-of-them-are-equivalent-because" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<p>They are all functions of</p>
<p><span class="math display">\[
\sum_{i=1}^{n_{\text{test}}} (Y_i - \hat{Y}_i)^2.
\]</span> #### Performance Metrics based on Penalized Likelihood</p>
<ul>
<li><p>Common structure: <strong>Model fit</strong> (ensures good fit) + <strong>Penalty measuring model complexity</strong> (prevents overfitting)</p></li>
<li><p><strong>Idea</strong>: To balance goodness of fit to the training data with model complexity<br />
</p></li>
<li><p><strong>Criterion</strong>: The smaller, the better the model</p></li>
</ul>
</div>
<div id="aic-akaike-information-criterion" class="section level5 hasAnchor" number="3.2.2.1.3">
<h5><span class="header-section-number">3.2.2.1.3</span> AIC (Akaike Information Criterion)<a href="linear-models.html#aic-akaike-information-criterion" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<ul>
<li>Definition: <span class="math inline">\(\text{AIC} = -2l + 2p\)</span> <span class="math inline">\((p = \#\text{parameters})\)</span></li>
</ul>
</div>
<div id="bic-bayesian-information-criterion" class="section level5 hasAnchor" number="3.2.2.1.4">
<h5><span class="header-section-number">3.2.2.1.4</span> BIC (Bayesian Information Criterion)<a href="linear-models.html#bic-bayesian-information-criterion" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<ul>
<li>Definition: <span class="math inline">\(\text{BIC} = -2l + p \ln n_{\text{tr}}\)</span></li>
<li>vs. AIC: A heavier penalty per parameter (<span class="math inline">\(\because \ln n_{\text{tr}} &gt; 2 \Leftrightarrow n_{\text{tr}} \geq 8\)</span>)</li>
</ul>
</div>
</div>
<div id="from-past-pa-exams" class="section level4 hasAnchor" number="3.2.2.2">
<h4><span class="header-section-number">3.2.2.2</span> From Past PA Exams<a href="linear-models.html#from-past-pa-exams" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<div id="june-2019-task-6-select-features-using-aic-or-bic" class="section level5 hasAnchor" number="3.2.2.2.1">
<h5><span class="header-section-number">3.2.2.2.1</span> June 2019, Task 6: Select features using AIC or BIC<a href="linear-models.html#june-2019-task-6-select-features-using-aic-or-bic" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<p>AIC and BIC are among the available techniques for feature selection. Briefly <strong>describe</strong> them and outline the <strong>differences</strong> in the two criteria. Make a recommendation as to which one should be used for this problem.</p>
</div>
<div id="june-17-2020-task-7-select-features-using-stepwise-selection" class="section level5 hasAnchor" number="3.2.2.2.2">
<h5><span class="header-section-number">3.2.2.2.2</span> June 17, 2020, Task 7: Select features using stepwise selection<a href="linear-models.html#june-17-2020-task-7-select-features-using-stepwise-selection" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<p>When using the stepAIC function, there are two decisions to make: <strong>forward vs. backward</strong> and <strong>AIC vs. BIC</strong>. <strong>Describe</strong> each decision and the <strong>implications</strong> in choosing one option versus the other.</p>
</div>
</div>
<div id="model-diagnostics-based-on-residuals" class="section level4 hasAnchor" number="3.2.2.3">
<h4><span class="header-section-number">3.2.2.3</span> Model Diagnostics based on Residuals<a href="linear-models.html#model-diagnostics-based-on-residuals" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Recall the model assumption by default:</p>
<p><span class="math display">\[
\begin{cases}
Y = X \beta + \varepsilon \\
\varepsilon_i \sim \text{i.i.d. } N(0, \sigma^2)
\end{cases}
\Rightarrow
\begin{cases}
e_i \approx \text{i.i.d. } N(0, \sigma^2) \\
e_i \text{ have no systematic patterns}
\end{cases}
\]</span></p>
<div id="residual-analysis" class="section level5 hasAnchor" number="3.2.2.3.1">
<h5><span class="header-section-number">3.2.2.3.1</span> Residual analysis<a href="linear-models.html#residual-analysis" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<ul>
<li>Do the residuals behave like this?
<ul>
<li>If <strong>yes</strong>, model looks good!</li>
<li>If <strong>no</strong>, try to <strong>improve the model</strong>!</li>
</ul></li>
</ul>
</div>
</div>
<div id="two-particularly-useful-plots-residuals-vs-fitted-plot" class="section level4 hasAnchor" number="3.2.2.4">
<h4><span class="header-section-number">3.2.2.4</span> Two Particularly Useful Plots: “Residuals vs Fitted” Plot<a href="linear-models.html#two-particularly-useful-plots-residuals-vs-fitted-plot" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p><strong>Uses</strong> - Check <strong>model specification</strong> (are the predictors entered properly?) - Check <strong>homogeneity</strong> of the error variance (<strong>homoscedasticity</strong>)</p>
<div id="good" class="section level5 unnumbered hasAnchor">
<h5>Good<a href="linear-models.html#good" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<p><img src="assets/images/good_plot.png" alt="Good Residuals Plot" /> - No regular patterns - Same amount of variability</p>
</div>
<div id="bad" class="section level5 unnumbered hasAnchor">
<h5>Bad<a href="linear-models.html#bad" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<p><img src="assets/images/bad_plot.png" alt="Bad Residuals Plot" /> - A clear funnel shape <span class="math inline">\(\Rightarrow\)</span> error variance increases with fitted value</p>
</div>
</div>
<div id="two-particularly-useful-plots-q-q-plot" class="section level4 hasAnchor" number="3.2.2.5">
<h4><span class="header-section-number">3.2.2.5</span> Two Particularly Useful Plots: Q-Q Plot<a href="linear-models.html#two-particularly-useful-plots-q-q-plot" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p><strong>What it is</strong>: Graphs the <strong>empirical quantiles</strong> of the standardized residuals against theoretical <span class="math inline">\(N(0, 1)\)</span> quantiles.</p>
<p><strong>Use</strong>: Check the <strong>normality</strong> of the random errors</p>
<div id="good-1" class="section level5 unnumbered hasAnchor">
<h5>Good<a href="linear-models.html#good-1" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<p><img src="assets/images/good_qq_plot.png" alt="Good Q-Q Plot" /> - Points <strong>fall</strong> on 45° line closely</p>
</div>
<div id="bad-1" class="section level5 unnumbered hasAnchor">
<h5>Bad<a href="linear-models.html#bad-1" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<p><img src="assets/images/bad_qq_plot.png" alt="Bad Q-Q Plot" /> - Points <strong>deviate</strong> from 45° line substantially</p>
</div>
</div>
</div>
<div id="feature-generation" class="section level3 hasAnchor" number="3.2.3">
<h3><span class="header-section-number">3.2.3</span> Feature Generation<a href="linear-models.html#feature-generation" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div id="big-picture" class="section level4 unnumbered hasAnchor">
<h4>Big Picture<a href="linear-models.html#big-picture" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p><strong>Given</strong>: A list of potentially useful predictors (given on the exam)</p>
<div id="practical-questions-to-be-addressed" class="section level5 unnumbered hasAnchor">
<h5>Practical questions to be addressed<a href="linear-models.html#practical-questions-to-be-addressed" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<ul>
<li>How do we want these predictors to enter the model equation to achieve what effects?</li>
</ul>
<p><span class="math display">\[
Y = \beta_0 + \text{[how to fill in this part?]} + \varepsilon
\]</span></p>
<ul>
<li>How to handle <strong>categorical</strong> predictors in a linear model?</li>
</ul>
</div>
<div id="feature-generation-for-linear-models" class="section level5 unnumbered hasAnchor">
<h5>Feature generation for linear models<a href="linear-models.html#feature-generation-for-linear-models" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<p>Generate <strong>new features</strong> to improve the <strong>flexibility</strong> of a linear model<br />
<span class="math inline">\(\Rightarrow\)</span> prediction accuracy <span class="math inline">\(\uparrow\)</span></p>
<hr />
</div>
</div>
<div id="numeric-predictors" class="section level4 hasAnchor" number="3.2.3.1">
<h4><span class="header-section-number">3.2.3.1</span> Numeric Predictors<a href="linear-models.html#numeric-predictors" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Let <span class="math inline">\(X_j\)</span> be a numeric predictor.</p>
<div id="simplest-form-assign-a-single-regression-coefficient" class="section level5 unnumbered hasAnchor">
<h5>Simplest form: Assign a single regression coefficient<a href="linear-models.html#simplest-form-assign-a-single-regression-coefficient" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<ul>
<li><strong>Model equation</strong>:<br />
<span class="math display">\[
Y = \beta_0 + \dots + \beta_j X_j + \dots + \varepsilon
\]</span></li>
<li><strong>Interpretation</strong>:
<ul>
<li><span class="math inline">\(\beta_j = \frac{\partial \mathbb{E}[Y]}{\partial X_j}\)</span><br />
</li>
<li><span class="math inline">\(\beta_j\)</span> = the expected change in <span class="math inline">\(Y\)</span> <strong>per unit increase</strong> in <span class="math inline">\(X_j\)</span>, holding all other predictors fixed.</li>
</ul></li>
</ul>
</div>
<div id="polynomial-regression" class="section level5 unnumbered hasAnchor">
<h5>Polynomial regression<a href="linear-models.html#polynomial-regression" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<ul>
<li><strong>Motivation</strong>: To accommodate <strong>non-linear</strong> relationships between <span class="math inline">\(Y\)</span> (e.g., #pedestrians) and <span class="math inline">\(X_j\)</span> (hour)</li>
<li><strong>Model equation</strong>:<br />
<span class="math display">\[
Y = \beta_0 + \beta_1 X_j + \beta_2 X_j^2 + \dots + \beta_m X_j^m + \dots + \varepsilon
\]</span>
<ul>
<li><span class="math inline">\(\quad \quad \quad \quad \quad \quad \uparrow \quad \quad \quad \quad \quad \quad \quad \uparrow\)</span><br />
</li>
<li><span class="math inline">\(\quad \quad \quad\)</span> new features</li>
</ul></li>
<li><strong>Interpretation</strong>:
<ul>
<li><span class="math inline">\(\frac{\partial \mathbb{E}[Y]}{\partial X_j} = \beta_1 + 2\beta_2 X_j + \dots + m\beta_m X_j^{m-1}\)</span></li>
<li>Coefficients are <strong>harder to interpret!</strong></li>
</ul></li>
</ul>
<hr />
</div>
</div>
<div id="categorical-predictors" class="section level4 hasAnchor" number="3.2.3.2">
<h4><span class="header-section-number">3.2.3.2</span> Categorical Predictors<a href="linear-models.html#categorical-predictors" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<div id="binarization" class="section level5 unnumbered hasAnchor">
<h5>Binarization<a href="linear-models.html#binarization" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<p>Can we assign a regression coefficient directly to a categorical predictor?</p>
<p>For Smoking = <span class="math display">\[
\begin{cases}
\text{Smoker} \\
\text{Non-smoker} \\
\text{Unknown}
\end{cases}
\]</span> can we use <span class="math inline">\(Y = \beta_0 + \beta_1 \times \text{Smoking} + \varepsilon\)</span>?</p>
<p>No! Because <span class="math inline">\(\beta_1 \times \text{Smoker}\)</span>, <span class="math inline">\(\beta_1 \times \text{Non-smoker}\)</span>, and <span class="math inline">\(\beta_1 \times \text{Unknown}\)</span> don’t make sense!</p>
</div>
<div id="need-an-extra-operation-binarization" class="section level5 unnumbered hasAnchor">
<h5>Need an extra operation: Binarization<a href="linear-models.html#need-an-extra-operation-binarization" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<p>A <strong>categorical predictor</strong> <span class="math inline">\(\rightarrow\)</span> A collection of <strong>binary/dummy/indicator variables</strong> indicating one and only one level (= 1 for that level, = 0 otherwise)</p>
<p>Dummy variables are numeric <span class="math inline">\(\Rightarrow\)</span> we can enter them directly in model equation.</p>
<hr />
</div>
<div id="illustrative-example-smoking" class="section level5 unnumbered hasAnchor">
<h5>Illustrative Example: Smoking<a href="linear-models.html#illustrative-example-smoking" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<div id="dummy-variables" class="section level6 unnumbered hasAnchor">
<h6>3 dummy variables<a href="linear-models.html#dummy-variables" class="anchor-section" aria-label="Anchor link to header"></a></h6>
<table>
<colgroup>
<col width="26%" />
<col width="22%" />
<col width="27%" />
<col width="23%" />
</colgroup>
<thead>
<tr class="header">
<th>Level of Smoking</th>
<th>SmokingSmoker</th>
<th>SmokingNon-smoker</th>
<th>SmokingUnknown</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Smoker</td>
<td>1</td>
<td>0</td>
<td>0</td>
</tr>
<tr class="even">
<td>Non-smoker</td>
<td>0</td>
<td>1</td>
<td>0</td>
</tr>
<tr class="odd">
<td>Unknown</td>
<td>0</td>
<td>0</td>
<td>1</td>
</tr>
</tbody>
</table>
</div>
<div id="sample-dataset" class="section level6 unnumbered hasAnchor">
<h6>Sample dataset<a href="linear-models.html#sample-dataset" class="anchor-section" aria-label="Anchor link to header"></a></h6>
<table>
<colgroup>
<col width="8%" />
<col width="17%" />
<col width="22%" />
<col width="27%" />
<col width="23%" />
</colgroup>
<thead>
<tr class="header">
<th>Obs.</th>
<th>Smoking</th>
<th>SmokingSmoker</th>
<th>SmokingNon-smoker</th>
<th>SmokingUnknown</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1</td>
<td>Smoker</td>
<td>1</td>
<td>0</td>
<td>0</td>
</tr>
<tr class="even">
<td>2</td>
<td>Unknown</td>
<td>0</td>
<td>0</td>
<td>1</td>
</tr>
<tr class="odd">
<td>3</td>
<td>Non-smoker</td>
<td>0</td>
<td>1</td>
<td>0</td>
</tr>
<tr class="even">
<td>4</td>
<td>Smoker</td>
<td>1</td>
<td>0</td>
<td>0</td>
</tr>
</tbody>
</table>
<hr />
</div>
</div>
<div id="illustrative-example-smoking-cont." class="section level5 unnumbered hasAnchor">
<h5>Illustrative Example: Smoking (Cont.)<a href="linear-models.html#illustrative-example-smoking-cont." class="anchor-section" aria-label="Anchor link to header"></a></h5>
<p>Possible model configurations:</p>
<p><span class="math display">\[
Y = \beta_0 + \beta_1 \times \text{SmokingSmoker} + \beta_2 \times \text{SmokingUnknown} + \varepsilon
\]</span> <span class="math display">\[
Y = \beta_0 + \beta_1 \times \text{SmokingNon-smoker} + \beta_2 \times \text{SmokingUnknown} + \varepsilon
\]</span> <span class="math display">\[
Y = \beta_0 + \beta_1 \times \text{SmokingSmoker} + \beta_2 \times \text{SmokingNon-smoker} + \varepsilon
\]</span></p>
<p><strong>Question</strong>: Why not use all three dummy variables?</p>
<ul>
<li><strong>Intuitively</strong>: When two of them are known, the third brings <strong>no additional information</strong>.</li>
<li><strong>Technically</strong>: Including all three results in <strong>perfect collinearity</strong> and a <strong>rank-deficient model</strong>.</li>
</ul>
<p><strong>General rule for a linear model</strong>:<br />
A categorical predictor with <span class="math inline">\(k\)</span> levels should be represented by <span class="math inline">\(k - 1\)</span> binary variables.<br />
Left out level = <strong>baseline level</strong>.</p>
<hr />
</div>
<div id="baseline-level" class="section level5 unnumbered hasAnchor">
<h5>Baseline Level<a href="linear-models.html#baseline-level" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<ul>
<li><strong>R’s default</strong>: The first level in <strong>alpha-numerical order</strong> (e.g., “Non-smoker” for smoking)<br />
</li>
<li><strong>Common practice</strong>: The <strong>most common</strong> (populous) level</li>
</ul>
<div id="interpretation-of-regression-coefficients" class="section level6 unnumbered hasAnchor">
<h6>Interpretation of regression coefficients<a href="linear-models.html#interpretation-of-regression-coefficients" class="anchor-section" aria-label="Anchor link to header"></a></h6>
<p>Consider <span class="math display">\[
\mathbb{E}[Y] = \beta_0 + (\beta_1 D_1 + \dots + \beta_j D_j + \dots + \beta_{k-1} D_{k-1}) + \dots
\]</span> where this represents a <span class="math inline">\(k\)</span>-level categorical predictor.</p>
<ul>
<li>In <span class="math inline">\(j\)</span>th level: <span class="math inline">\(\mathbb{E}[Y] = \beta_0 + \beta_j (1)\)</span></li>
<li>In baseline level: <span class="math inline">\(\mathbb{E}[Y] = \beta_0\)</span></li>
<li><strong>Interpretation</strong>: <span class="math inline">\(\beta_j = \mathbb{E}[Y]\)</span> at <span class="math inline">\(j\)</span>th level <span class="math inline">\(- \mathbb{E}[Y]\)</span> at baseline level.</li>
</ul>
<p>The <span class="math inline">\(\beta_j\)</span>’s measure how much the target mean changes over different factor levels compared to the baseline. <strong>They are not change in the target mean per unit change in the dummy variables</strong>.</p>
<hr />
</div>
</div>
</div>
<div id="what-is-interaction" class="section level4 hasAnchor" number="3.2.3.3">
<h4><span class="header-section-number">3.2.3.3</span> What is Interaction?<a href="linear-models.html#what-is-interaction" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<ul>
<li><strong>Restriction</strong>: In the basic model form, <span class="math inline">\(Y = \beta_0 + \beta_1 X_1 + \dots + \beta_p X_p + \varepsilon\)</span>, the expected effect of each <span class="math inline">\(X_j\)</span> on <span class="math inline">\(Y\)</span> is <strong>independent</strong> of the values of other predictors.</li>
</ul>
<div id="interesting-example-from-introduction-to-statistical-learning" class="section level5 unnumbered hasAnchor">
<h5>Interesting example from <em>Introduction to Statistical Learning</em><a href="linear-models.html#interesting-example-from-introduction-to-statistical-learning" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<ul>
<li><span class="math inline">\(Y\)</span> = number of units produced in a factory</li>
<li><span class="math inline">\(X_1\)</span> = number of production lines in the factory</li>
<li><span class="math inline">\(X_2\)</span> = number of workers in the factory</li>
</ul>
<p><strong>Observation</strong>: The effect of <span class="math inline">\(X_1\)</span> on <span class="math inline">\(\mathbb{E}[Y]\)</span> is likely to <strong>depend on</strong> <span class="math inline">\(X_2\)</span>.</p>
<p><strong>Reason</strong>: If <span class="math inline">\(X_2 = 0\)</span>, then increasing <span class="math inline">\(X_1\)</span> will not raise <span class="math inline">\(Y\)</span> much. (No one is working!)</p>
<p><strong>Interaction</strong>: When the effect of one predictor on the target variable <strong>depends on</strong> the value/level of another predictor.</p>
<p><strong>Suggestion</strong>: Consider <span class="math inline">\(\mathbb{E}[Y] = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \beta_3 X_1 X_2\)</span>.</p>
<ul>
<li><strong>Note</strong>:<br />
<span class="math inline">\(\mathbb{E}[Y] = \beta_0 + (\beta_1 + \beta_3 X_2) X_1 + \beta_2 X_2 = \beta_0 + \beta_1 X_1 + (\beta_2 + \beta_3 X_1) X_2\)</span>
<ul>
<li>Here, <span class="math inline">\((\beta_1 + \beta_3 X_2)\)</span> <strong>depends on</strong> <span class="math inline">\(X_2\)</span><br />
</li>
<li>Similarly, <span class="math inline">\((\beta_2 + \beta_3 X_1)\)</span> <strong>depends on</strong> <span class="math inline">\(X_1\)</span></li>
</ul></li>
<li><span class="math inline">\(X_1 X_2\)</span> is the <strong>interaction term</strong>.</li>
</ul>
<hr />
</div>
</div>
<div id="interactions-between-continuous-and-categorical-predictors" class="section level4 unnumbered hasAnchor">
<h4>Interactions between Continuous and Categorical Predictors<a href="linear-models.html#interactions-between-continuous-and-categorical-predictors" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<div id="model-equation-1" class="section level5 unnumbered hasAnchor">
<h5>Model equation:<a href="linear-models.html#model-equation-1" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<p><span class="math display">\[
\mathbb{E}[Y] = \beta_0 + \beta_1 X_1 \text{ (continuous)} + \beta_2 X_2 \text{ (binary)} + \beta_3 X_1 X_2
\]</span></p>
<p><span class="math display">\[
=
\begin{cases}
\beta_0 + \beta_1 X_1, &amp; \text{if } X_2 = 0, \\
(\beta_0 + \beta_2) + (\beta_1 + \beta_3) X_1, &amp; \text{if } X_2 = 1.
\end{cases}
\]</span></p>
</div>
<div id="graphical-illustration-of-interaction" class="section level5 unnumbered hasAnchor">
<h5><strong>Graphical Illustration of Interaction</strong><a href="linear-models.html#graphical-illustration-of-interaction" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<div class="float">
<img src="assets/images/interaction_plot.png" alt="Graphical illustration" />
<div class="figcaption">Graphical illustration</div>
</div>
<p>Breakdown of Graph: - If <span class="math inline">\(X_2 = 0\)</span>: Slope = <span class="math inline">\(\beta_1\)</span>, Intercept = <span class="math inline">\(\beta_0\)</span> - If <span class="math inline">\(X_2 = 1\)</span>: Slope = <span class="math inline">\(\beta_1 + \beta_3\)</span>, Intercept = <span class="math inline">\(\beta_0 + \beta_2\)</span></p>
<ul>
<li><strong>Different intercepts</strong> due to <span class="math inline">\(\beta_2 X_2\)</span></li>
<li><strong>Different slopes</strong> due to <span class="math inline">\(\beta_3 X_1 X_2\)</span><br />
<span class="math inline">\(\Rightarrow\)</span> <strong>interaction effect</strong></li>
</ul>
<hr />
</div>
</div>
<div id="interactions-between-two-categorical-predictors" class="section level4 unnumbered hasAnchor">
<h4>Interactions between Two Categorical Predictors<a href="linear-models.html#interactions-between-two-categorical-predictors" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<div id="model-equation-2" class="section level5 unnumbered hasAnchor">
<h5>Model equation:<a href="linear-models.html#model-equation-2" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<p><span class="math display">\[
\mathbb{E}[Y] = \beta_0 + \beta_1 X_1 \text{ (binary)} + \beta_2 X_2 \text{ (binary)} + \beta_3 X_1 X_2
\]</span></p>
<div class="float">
<img src="assets/images/interactions-two-categorical-predictors.png" alt="Four Different Target Means" />
<div class="figcaption">Four Different Target Means</div>
</div>
<hr />
</div>
</div>
</div>
<div id="feature-selection" class="section level3 hasAnchor" number="3.2.4">
<h3><span class="header-section-number">3.2.4</span> Feature Selection<a href="linear-models.html#feature-selection" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<hr />
<div id="featuremodel-selection-big-picture" class="section level4 unnumbered hasAnchor">
<h4>Feature/Model Selection: Big Picture<a href="linear-models.html#featuremodel-selection-big-picture" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<ul>
<li><p><strong>Given</strong>: A full linear model of <span class="math inline">\(p\)</span> (potentially useful) features: <span class="math display">\[
Y = \beta_0 + \beta_1 X_1 + \dots + \beta_p X_p + \varepsilon.
\]</span></p></li>
<li><p><strong>Task</strong>: Select the <strong>really useful</strong> features, i.e., <span class="math display">\[
Y = \text{?} + \varepsilon.
\]</span></p></li>
<li><p><strong>Motivation</strong>:<br />
Remove features with limited predictive power<br />
<span class="math inline">\(\Downarrow\)</span><br />
Prevent overfitting and <strong>simplify model</strong></p></li>
</ul>
<div id="naive-suggestion" class="section level5 unnumbered hasAnchor">
<h5>Naive suggestion<a href="linear-models.html#naive-suggestion" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<ul>
<li>Fit the full model and drop all insignificant features at once.</li>
</ul>
</div>
<div id="caveat" class="section level5 unnumbered hasAnchor">
<h5>Caveat<a href="linear-models.html#caveat" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<ul>
<li>Absence of one feature can affect the significance of other features.</li>
</ul>
<hr />
</div>
</div>
<div id="best-subset-selection" class="section level4 unnumbered hasAnchor">
<h4>Best Subset Selection<a href="linear-models.html#best-subset-selection" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<div id="idea" class="section level5 unnumbered hasAnchor">
<h5><strong>Idea</strong><a href="linear-models.html#idea" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<ul>
<li>Fit all possible <span class="math inline">\(2^p\)</span> linear models constructed from the <span class="math inline">\(p\)</span> features</li>
<li>Choose the <strong>“best subset”</strong> of predictors (w.r.t. AIC, BIC, etc.) to form the best model</li>
</ul>
</div>
<div id="merits" class="section level5 unnumbered hasAnchor">
<h5><strong>Merits</strong><a href="linear-models.html#merits" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<ul>
<li>Conceptually simple</li>
<li>Screens all <span class="math inline">\(2^p\)</span> models</li>
</ul>
</div>
<div id="demerits" class="section level5 unnumbered hasAnchor">
<h5><strong>Demerits</strong><a href="linear-models.html#demerits" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<ul>
<li><strong>Computationally prohibitive</strong>; generally not feasible when <span class="math inline">\(p \geq 10\)</span><br />
(Note: <span class="math inline">\(2^{10} = 1,024\)</span>, <span class="math inline">\(2^{20} = 1,048,576\)</span>!!)</li>
</ul>
</div>
<div id="need-more-efficient-feature-selection-methods" class="section level5 unnumbered hasAnchor">
<h5><strong>Need more efficient feature selection methods!</strong><a href="linear-models.html#need-more-efficient-feature-selection-methods" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<ul>
<li><strong>More efficient solution</strong>: Stepwise selection<br />
To look at a <strong>restricted subset</strong> of all possible models.</li>
</ul>
<hr />
</div>
</div>
<div id="stepwise-selection-algorithms" class="section level4 hasAnchor" number="3.2.4.1">
<h4><span class="header-section-number">3.2.4.1</span> Stepwise Selection Algorithms<a href="linear-models.html#stepwise-selection-algorithms" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<dl>
<dt><strong>Backward Stepwise Selection</strong></dt>
<dd>
<p>Start with a <strong>full model</strong> with all <span class="math inline">\(p\)</span> features</p>
<p><span class="math inline">\(\downarrow\)</span> Go Backward</p>
<p><strong>Drop</strong>, one at a time, the feature to improve the model the most w.r.t. AIC or BIC</p>
<p><span class="math inline">\(\downarrow\)</span> Repeat until</p>
<p>Stop when no features can be <strong>dropped</strong> to improve the model.</p>
</dd>
<dt><strong>Forward Stepwise Selection</strong></dt>
<dd>
<p>Start with a <strong>intercept-only</strong> model (<span class="math inline">\(Y=\beta_0+\epsilon\)</span>)</p>
<p><span class="math inline">\(\downarrow\)</span> Go Forward</p>
<p><strong>Add</strong>, one at a time, the feature to improve the model the most w.r.t. AIC or BIC</p>
<p><span class="math inline">\(\downarrow\)</span> Repeat until</p>
<p>Stop when no features can be <strong>added</strong> to improve the model.</p>
</dd>
</dl>
<hr />
</div>
<div id="discussion" class="section level4 unnumbered hasAnchor">
<h4><strong>Discussion</strong><a href="linear-models.html#discussion" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<ul>
<li><strong>Backward</strong>: Once dropped, the feature will be gone forever<br />
</li>
<li><strong>Forward</strong>: Once added, the feature will stay forever<br />
</li>
<li><strong>Max. # models fitted</strong>: <span class="math display">\[
1 + \frac{p(p+1)}{2} \quad (p=20 \Rightarrow 211, \text{ vs. } 2^{20} &gt; 1 \text{ million})
\]</span></li>
<li><strong>Drawback</strong>: No guarantee final model is best</li>
</ul>
<div id="two-important-decisions-selection-criterion-and-selection-process" class="section level5 unnumbered hasAnchor">
<h5><strong>Two important decisions: Selection criterion and selection process</strong><a href="linear-models.html#two-important-decisions-selection-criterion-and-selection-process" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<div id="aicbic" class="section level6 unnumbered hasAnchor">
<h6><strong>AIC/BIC</strong><a href="linear-models.html#aicbic" class="anchor-section" aria-label="Anchor link to header"></a></h6>
<ul>
<li>Prefer a <strong>simpler model</strong> <span class="math inline">\(\Rightarrow\)</span> BIC (<strong>more conservative</strong>)</li>
<li>Don’t want to miss important predictors <span class="math inline">\(\Rightarrow\)</span> AIC</li>
<li>No universally superior choice; <strong>keep business problem in mind</strong> (key!)</li>
</ul>
</div>
<div id="backwardforward" class="section level6 unnumbered hasAnchor">
<h6><strong>Backward/Forward</strong><a href="linear-models.html#backwardforward" class="anchor-section" aria-label="Anchor link to header"></a></h6>
<ul>
<li><strong>Forward selection</strong> tends to produce a <strong>simpler model</strong>.</li>
<li><strong>Backward selection</strong> tends to produce a more <strong>complex model</strong>.</li>
<li>Again, no universally superior choice</li>
</ul>
<hr />
</div>
</div>
</div>
</div>
<div id="regularization" class="section level3 hasAnchor" number="3.2.5">
<h3><span class="header-section-number">3.2.5</span> Regularization<a href="linear-models.html#regularization" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div id="how-does-regularization-a.k.a.-shrinkage-work" class="section level4 unnumbered hasAnchor">
<h4><strong>How Does Regularization (a.k.a. Shrinkage) Work?</strong><a href="linear-models.html#how-does-regularization-a.k.a.-shrinkage-work" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<div id="stepwise-selection" class="section level5 unnumbered hasAnchor">
<h5><strong>Stepwise Selection</strong><a href="linear-models.html#stepwise-selection" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<ul>
<li>Search through a <strong>list</strong> of candidate models <span class="math inline">\(\rightarrow\)</span> final model</li>
<li>Set <span class="math inline">\(\beta_j = 0\)</span> for non-predictive features in full model</li>
</ul>
</div>
<div id="regularization-1" class="section level5 unnumbered hasAnchor">
<h5><strong>Regularization</strong><a href="linear-models.html#regularization-1" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<ul>
<li>Fit a <strong>single model</strong> with all <span class="math inline">\(p\)</span> features using a “special technique”</li>
<li><span class="math inline">\(\beta_j \approx 0\)</span> for non-predictive features <span class="math inline">\(\Rightarrow\)</span> smaller effect on target</li>
</ul>
</div>
<div id="ordinary-least-squares-ols" class="section level5 unnumbered hasAnchor">
<h5><strong>Ordinary Least Squares (OLS)</strong><a href="linear-models.html#ordinary-least-squares-ols" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<p><span class="math display">\[
\min_{\beta_0, \beta_1, \dots, \beta_p} \sum_{i=1}^{n_{tr}} \left[ Y_i - \left( \beta_0 + \beta_1 X_{i1} + \beta_2 X_{i2} + \dots + \beta_p X_{ip} \right) \right]^2
\]</span> - <strong>RSS</strong> (Residual Sum of Squares)</p>
</div>
<div id="loss-penalty-formulation" class="section level5 unnumbered hasAnchor">
<h5><strong>Loss + Penalty Formulation</strong><a href="linear-models.html#loss-penalty-formulation" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<p><span class="math display">\[
\min_{\beta_0, \beta_1, \dots, \beta_p} \text{RSS} + \lambda f_R(\beta)
\]</span></p>
<p><strong>Where:</strong></p>
<ul>
<li><span class="math inline">\(\lambda\)</span>: <strong>Regularization parameter</strong>; the larger, the heavier the regularization</li>
<li><span class="math inline">\(f_R(\cdot)\)</span>: <strong>Penalty function</strong>; reflects the size of slope coefficients</li>
<li>Adds a <strong>regularization penalty</strong> to the loss function</li>
</ul>
<p><span class="math inline">\(Idea\)</span><strong>: Make a Trade-Off Between:</strong></p>
<ul>
<li><strong>Goodness-of-Fit to Training Data</strong> (captured by <strong>RSS</strong>): Smaller <span class="math inline">\(\rightarrow\)</span> Better -</li>
<li><strong>Model Complexity</strong> (captured by <strong>Regularization Penalty</strong>): Smaller <span class="math inline">\(\rightarrow\)</span> Better</li>
</ul>
<hr />
</div>
</div>
<div id="how-does-regularization-a.k.a.-shrinkage-work-cont." class="section level4 unnumbered hasAnchor">
<h4><strong>How does Regularization (a.k.a. Shrinkage) Work? (Cont.)</strong><a href="linear-models.html#how-does-regularization-a.k.a.-shrinkage-work-cont." class="anchor-section" aria-label="Anchor link to header"></a></h4>
<ul>
<li><strong>Objective function</strong>: <span class="math inline">\(\text{RSS} + \lambda f_R(\beta)\)</span></li>
<li><strong>Output</strong>: A <strong>family</strong> of coefficient estimates <span class="math inline">\(\{\hat{\beta}_\lambda = (\hat{\beta}_{0,\lambda}, \hat{\beta}_{1,\lambda}, \dots, \hat{\beta}_{p,\lambda}) : \lambda \geq 0\}\)</span></li>
</ul>
<div id="effects-of-lambda-bias-variance-trade-off-again" class="section level5 unnumbered hasAnchor">
<h5><strong>Effects of</strong> <span class="math inline">\(\lambda\)</span> (Bias-Variance Trade-Off Again!)<a href="linear-models.html#effects-of-lambda-bias-variance-trade-off-again" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<ul>
<li><strong>Case 1</strong>: When <span class="math inline">\(\lambda = 0\)</span>, coefficient estimates = ordinary least squares estimates.</li>
<li><strong>Case 2</strong>: <span class="math inline">\(\lambda \uparrow \Rightarrow |\hat{\beta}_{j,\lambda}| \downarrow \Rightarrow\)</span> flexibility <span class="math inline">\(\downarrow\)</span>
<ul>
<li>Bias<span class="math inline">\(^2\)</span> <span class="math inline">\(\uparrow\)</span></li>
<li>Variance <span class="math inline">\(\downarrow\)</span></li>
<li><strong>Hopefully</strong> <span class="math inline">\(\Rightarrow\)</span> prediction performance <span class="math inline">\(\uparrow\)</span></li>
</ul></li>
<li><strong>Case 3</strong>: When <span class="math inline">\(\lambda \to \infty\)</span>, <span class="math inline">\(\hat{\beta}_{j,\lambda} \to 0\)</span> for all <span class="math inline">\(j = 1, \dots, p\)</span> (intercept-only model).</li>
</ul>
</div>
<div id="different-indexes-of-model-complexity" class="section level5 unnumbered hasAnchor">
<h5><strong>Different Indexes of Model Complexity</strong><a href="linear-models.html#different-indexes-of-model-complexity" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<ul>
<li><strong>Stepwise selection</strong>: <span class="math inline">\(p\)</span> (# features)</li>
<li><strong>Regularization</strong>: <span class="math inline">\(\lambda\)</span></li>
</ul>
<hr />
</div>
</div>
<div id="different-choices-of-f_rbeta-penalty-function" class="section level4 unnumbered hasAnchor">
<h4><strong>Different Choices of</strong> <span class="math inline">\(f_R(\beta)\)</span> (Penalty Function)<a href="linear-models.html#different-choices-of-f_rbeta-penalty-function" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<ul>
<li><strong>Ridge Regression</strong>: <span class="math inline">\(f_R(\beta) = \sum_{j=1}^{p} \beta_j^2\)</span></li>
<li><strong>Lasso</strong>: <span class="math inline">\(f_R(\beta) = \sum_{j=1}^{p} |\beta_j|\)</span></li>
<li><strong>Elastic Net</strong>: <span class="math inline">\(f_R(\beta) = (1 - \alpha) \sum_{j=1}^{p} \beta_j^2 + \alpha \sum_{j=1}^{p} |\beta_j|\)</span>
<ul>
<li><span class="math inline">\(\alpha \in [0, 1]\)</span> is the mixing coefficient</li>
<li><span class="math inline">\(\alpha = 0\)</span>: Ridge regression</li>
<li><span class="math inline">\(\alpha = 1\)</span>: Lasso</li>
</ul></li>
</ul>
<div id="feature-selection-property-of-elastic-net" class="section level5 unnumbered hasAnchor">
<h5><strong>Feature Selection Property of Elastic Net</strong><a href="linear-models.html#feature-selection-property-of-elastic-net" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<ul>
<li>For lasso (and elastic net with <span class="math inline">\(\alpha \neq 0\)</span>):
<ul>
<li><span class="math inline">\(\lambda\)</span> large enough <span class="math inline">\(\Rightarrow\)</span> some <span class="math inline">\(\hat{\beta}_{j,\lambda} = 0\)</span> exactly <span class="math inline">\(\Rightarrow\)</span>
<ul>
<li><strong>features dropped</strong></li>
<li><strong>model simplified!</strong></li>
</ul></li>
</ul></li>
</ul>
<p><strong>Note:</strong> Ridge regression never drops the features entirely. If the business problem wants you to identify the key factors effecting the target variable, then it makes sense to use the lasso or elastic net with a positive <span class="math inline">\(\alpha\)</span> so that your model will drop the features with limited predictive power and become easier to interpret.</p>
<hr />
</div>
</div>
<div id="hyperparameter-tuning" class="section level4 unnumbered hasAnchor">
<h4><strong>Hyperparameter Tuning</strong><a href="linear-models.html#hyperparameter-tuning" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p><span class="math inline">\(\lambda\)</span> and <span class="math inline">\(\alpha\)</span> are hyperparameters, so they are not determined as part of the optimization procedure.</p>
<div id="tuning-lambda-and-alpha-by-cv" class="section level5 unnumbered hasAnchor">
<h5><strong>Tuning <span class="math inline">\(\lambda\)</span> and <span class="math inline">\(\alpha\)</span> by CV</strong><a href="linear-models.html#tuning-lambda-and-alpha-by-cv" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<ul>
<li>Divide training data into <span class="math inline">\(k\)</span> folds (e.g., <span class="math inline">\(k = 10\)</span>).</li>
<li>For each pair of candidate values of <span class="math inline">\((\lambda, \alpha)\)</span>, train the model on all but one fold and measure performance on the left-out fold.
<ul>
<li>Repeat and compute the average (R)MSE.</li>
</ul></li>
</ul>
<table>
<thead>
<tr class="header">
<th>Combination</th>
<th><span class="math inline">\(\alpha\)</span></th>
<th><span class="math inline">\(\lambda\)</span></th>
<th>CV error (RMSE)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1</td>
<td>0.1</td>
<td>0.01</td>
<td>XXX</td>
</tr>
<tr class="even">
<td>2</td>
<td>0.1</td>
<td>0.02</td>
<td>XXX</td>
</tr>
<tr class="odd">
<td>3</td>
<td>0.1</td>
<td>0.03</td>
<td>XXX</td>
</tr>
<tr class="even">
<td>…</td>
<td>…</td>
<td>…</td>
<td>…</td>
</tr>
</tbody>
</table>
<ul>
<li>Choose the pair with the <strong>lowest CV error</strong>.</li>
</ul>
</div>
</div>
</div>
</div>
<div id="case-study-1-fitting-linear-models-in-r" class="section level2 hasAnchor" number="3.3">
<h2><span class="header-section-number">3.3</span> Case Study 1: Fitting Linear Models in R<a href="linear-models.html#case-study-1-fitting-linear-models-in-r" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="exploratory-data-analysis" class="section level3 hasAnchor" number="3.3.1">
<h3><span class="header-section-number">3.3.1</span> Exploratory Data Analysis<a href="linear-models.html#exploratory-data-analysis" class="anchor-section" aria-label="Anchor link to header"></a></h3>
</div>
<div id="simple-linear-regression" class="section level3 hasAnchor" number="3.3.2">
<h3><span class="header-section-number">3.3.2</span> Simple Linear Regression<a href="linear-models.html#simple-linear-regression" class="anchor-section" aria-label="Anchor link to header"></a></h3>
</div>
<div id="multiple-linear-regression" class="section level3 hasAnchor" number="3.3.3">
<h3><span class="header-section-number">3.3.3</span> Multiple Linear Regression<a href="linear-models.html#multiple-linear-regression" class="anchor-section" aria-label="Anchor link to header"></a></h3>
</div>
<div id="evaluation-of-linear-models" class="section level3 hasAnchor" number="3.3.4">
<h3><span class="header-section-number">3.3.4</span> Evaluation of Linear Models<a href="linear-models.html#evaluation-of-linear-models" class="anchor-section" aria-label="Anchor link to header"></a></h3>
</div>
</div>
<div id="case-study-2-feature-selection-and-regularization" class="section level2 hasAnchor" number="3.4">
<h2><span class="header-section-number">3.4</span> Case Study 2: Feature Selection and Regularization<a href="linear-models.html#case-study-2-feature-selection-and-regularization" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="preparatory-work" class="section level3 hasAnchor" number="3.4.1">
<h3><span class="header-section-number">3.4.1</span> Preparatory Work<a href="linear-models.html#preparatory-work" class="anchor-section" aria-label="Anchor link to header"></a></h3>
</div>
<div id="model-construction-and-feature-selection" class="section level3 hasAnchor" number="3.4.2">
<h3><span class="header-section-number">3.4.2</span> Model Construction and Feature Selection<a href="linear-models.html#model-construction-and-feature-selection" class="anchor-section" aria-label="Anchor link to header"></a></h3>
</div>
<div id="model-validation" class="section level3 hasAnchor" number="3.4.3">
<h3><span class="header-section-number">3.4.3</span> Model Validation<a href="linear-models.html#model-validation" class="anchor-section" aria-label="Anchor link to header"></a></h3>
</div>
<div id="regularization-2" class="section level3 hasAnchor" number="3.4.4">
<h3><span class="header-section-number">3.4.4</span> Regularization<a href="linear-models.html#regularization-2" class="anchor-section" aria-label="Anchor link to header"></a></h3>
</div>
</div>
<div id="conceptual-review-questions-for-chapter-3" class="section level2 unnumbered hasAnchor">
<h2>Conceptual Review: Questions for Chapter 3<a href="linear-models.html#conceptual-review-questions-for-chapter-3" class="anchor-section" aria-label="Anchor link to header"></a></h2>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="data-exploration-and-visualization.html" class="navigation navigation-prev navigation-unique" aria-label="Previous page"><i class="fa fa-angle-left"></i></a>

    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": true,
"facebook": false,
"twitter": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": {}
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": null,
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["soa-exam-pa.pdf", "soa-exam-pa.epub"],
"search": {
"engine": "lunr",
"options": null
},
"toc": {
"collapse": "subsection",
"scroll_highlight": true
},
"theme": "journal",
"toolbar": {
"position": "fixed"
},
"info": true
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
