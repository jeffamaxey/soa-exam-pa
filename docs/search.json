[{"path":"index.html","id":"preface","chapter":"Preface","heading":"Preface","text":"Note: book follows Actex Exam PA study manual author Ambrose Lo, contains study notes readings Actex manual. online version Actex manual available highly recommended purchase actexmadriver.com.\n\nonline version SOA Exam PA Study Notes, guide currently early development intended released Github later date.document designed capture study notes reading Actex Study Manual Exam PA, encapsulates chapters SOA Exam Syllabus preparation taking Predictive Analytics Exam (Exam PA). Throughout guide, goal gain deeper understanding basics R programming, theory relevent case studies predictive analytics, provide final preparation tactics form discussion past exams, sample mock exams, formula sheet.","code":""},{"path":"introduction.html","id":"introduction","chapter":"Introduction","heading":"Introduction","text":"","code":""},{"path":"introduction.html","id":"getting-started","chapter":"Introduction","heading":"Getting Started","text":"continue, make sure software need book:R: don’t R installed already, may reading wrong book; assume basic familiarity R throughout book.\n’d like learn use R, ’d recommend R Data Science designed get running R minimum fuss.R: don’t R installed already, may reading wrong book; assume basic familiarity R throughout book.\n’d like learn use R, ’d recommend R Data Science designed get running R minimum fuss.RStudio: RStudio free open source integrated development environment (IDE) R.\ncan write use Shiny apps R environment (including R GUI ESS), RStudio nice features specifically authoring, debugging, deploying Shiny apps.\nrecommend giving try, ’s required successful Shiny book.\ncan download RStudio Desktop https://www.rstudio.com/products/rstudio/downloadRStudio: RStudio free open source integrated development environment (IDE) R.\ncan write use Shiny apps R environment (including R GUI ESS), RStudio nice features specifically authoring, debugging, deploying Shiny apps.\nrecommend giving try, ’s required successful Shiny book.\ncan download RStudio Desktop https://www.rstudio.com/products/rstudio/download","code":""},{"path":"introduction.html","id":"system-requirements","chapter":"Introduction","heading":"System Requirements","text":"book written RStudio using bookdown.website hosted Github Pages, automatically updated every commit Github Actions.\ncomplete source available GitHub.version book built R version 4.3.3 (2024-02-29 ucrt) following packages:\nTABLE 0.1: R Packages\n","code":""},{"path":"basics-of-r-programming.html","id":"basics-of-r-programming","chapter":"1 Basics of R Programming","heading":"1 Basics of R Programming","text":"","code":""},{"path":"basics-of-r-programming.html","id":"data-types-in-r","chapter":"1 Basics of R Programming","heading":"1.1 Data Types in R","text":"","code":""},{"path":"basics-of-r-programming.html","id":"exercise-1.2.2---calculating-least-squares-estimates-by-matrix-manipulation","chapter":"1 Basics of R Programming","heading":"1.1.1 Exercise 1.2.2 - Calculating least squares estimates by matrix manipulation","text":"fitting multiple linear regression model:\\[\n\\begin{equation}\n\\widehat{Y} = \\beta_{0} + \\beta_{1}X_{1} + \\beta_{2}X_{2} + \\epsilon\n\\end{equation}\n\\]following dataset:Perform following two tasks R:() matrix form, model \\[\n  \\begin{equation}\n  Y = X\\beta + \\epsilon\n  \\end{equation}\n  \\]Set response vector Y design matrix X corresponding dataset .(b) Use matrix formula \\[\\widehat{\\beta} = (X^TX)^{-1}X^TY\\] Exam SRM calculate least squares estimates \\(\\beta_{0}\\), \\(\\beta_{1}\\), \\(\\beta_{2}\\).Solution: R commands collected . Let’s look code line line.() response vector can created following command:setup design matrix, first create three columns:first column vectorof 1’s corresponding intercept model. created rep(x, n) function, , simplest form, repeats replicates x vector n times.two columns represent values \\(X_{1}\\) \\(X_{2}\\).Now combine three vectors via c(x0, x1, x2) use matrix() function produce design matrix.(b) Using matrix formula \\(\\widehat{\\beta} = (X^TX)^{-1}X^TY\\), can calculate least squares estimates \\(\\beta_{0}\\), \\(\\beta_{1}\\), \\(\\beta_{2}\\).\\(\\widehat{\\beta}_{0}\\) = 5.2505543, \\(\\widehat{\\beta}_{1}\\) = 0.8137472, \\(\\widehat{\\beta}_{2}\\) = 1.5166297.Remark: Chapter 3, learn use lm() function fit linear model coef() function extract least squares estimates.","code":"\nY <- c(14, 11, 14, 18, 12, 9)\nx0 <- rep(1, 6) # same as c(1, 1, 1, 1, 1, 1)\nx1 <- c(2, 3, 0, 8, 4, 1)\nx2 <-  c(4, 2, 6, 4, 3, 2)\nX <- matrix(c(x0, x1, x2), nrow=6)\nX\n#>      [,1] [,2] [,3]\n#> [1,]    1    2    4\n#> [2,]    1    3    2\n#> [3,]    1    0    6\n#> [4,]    1    8    4\n#> [5,]    1    4    3\n#> [6,]    1    1    2\nB <- solve(t(X) %*% X) %*% t(X) %*% Y\nB\n#>        [,1]\n#> [1,] 5.2506\n#> [2,] 0.8137\n#> [3,] 1.5166"},{"path":"basics-of-r-programming.html","id":"data-frames","chapter":"1 Basics of R Programming","heading":"1.1.2 Data Frames","text":"Importance Data Frames: Exam PA (predictive modeling general), data frames useful mainly two reasons:(Data frames = datasets) statistical point view, eac column data frame can interpreted observed values variable (’s column must type data) row represents set measurements taken observations (e.g., record, case, instance) corresponding variables, much like Excel Spreadsheet. variables across different columns allowed different types capture different kinds information.(Data frames = datasets) statistical point view, eac column data frame can interpreted observed values variable (’s column must type data) row represents set measurements taken observations (e.g., record, case, instance) corresponding variables, much like Excel Spreadsheet. variables across different columns allowed different types capture different kinds information.(Structure vs. Unstructured Data) data management prespective, data frames efficient storage devices structrued data, data fit tabular arrangement. row--column configuration structured data makes easy search , extract, compare information across rows, limit scope information can expressed structured data. lend information naturally fit tabular arrangement. information, texts, images, audio, videos, known unstructured data. Although unstructured data flexible potentially useful, harder access carefully pre-processed (manipulated) can serve useful inputs predictive model.(Structure vs. Unstructured Data) data management prespective, data frames efficient storage devices structrued data, data fit tabular arrangement. row--column configuration structured data makes easy search , extract, compare information across rows, limit scope information can expressed structured data. lend information naturally fit tabular arrangement. information, texts, images, audio, videos, known unstructured data. Although unstructured data flexible potentially useful, harder access carefully pre-processed (manipulated) can serve useful inputs predictive model.","code":""},{"path":"basics-of-r-programming.html","id":"exercise-1.2.3---listing-the-components-of-a-list","chapter":"1 Basics of R Programming","heading":"1.1.3 Exercise 1.2.3 - Listing the components of a list","text":"Exercise 1.2.2, considered multiple linear regression model Y two predictors \\(X_{1}\\) \\(X_{2}\\). learn Chapter 3, can easily fit model following commands involving lm() function:resulting object, named m, list various components.List components m choose components apears contain least squares coefficient estimates. Check answers code.Solution: can see list components m typing command m$. RSudio show prompt listing components m.Alternatively formally, can apply names() function m.12 components, coefficients component appears relevant. Accessing component indeed produces least squares coefficient estimates:","code":"\nY <- c(14, 11, 14, 18, 12, 9)\nX1 <- c(2, 3, 0, 8, 4, 1)\nX2 <- c(4, 2, 6, 4, 3, 2)\nm <- lm(Y ~ X1 + X2)\nnames(m)\n#>  [1] \"coefficients\"  \"residuals\"     \"effects\"      \n#>  [4] \"rank\"          \"fitted.values\" \"assign\"       \n#>  [7] \"qr\"            \"df.residual\"   \"xlevels\"      \n#> [10] \"call\"          \"terms\"         \"model\"\nm$coefficients\n#> (Intercept)          X1          X2 \n#>      5.2506      0.8137      1.5166"},{"path":"basics-of-r-programming.html","id":"functions","chapter":"1 Basics of R Programming","heading":"1.1.4 Functions","text":"basic structure function R:Example function:","code":"FUNCTION_NAME <- function(ARGUMENT_1, ARGUMENT_2,...) {\n  STATEMENT_1\n  STATEMENT_2\n  ...\n  return(VALUE)\n}\nsumDiff <- function(x, y) {\n  s <- x + y\n  d <- x - y\n  return(list(sum=s, diff=d))\n}\n\nsumDiff(1, 2)$sum\n#> [1] 3\nsumDiff(1, 2)$diff\n#> [1] -1\nsumDiff(1:3, 1:3)\n#> $sum\n#> [1] 2 4 6\n#> \n#> $diff\n#> [1] 0 0 0"},{"path":"basics-of-r-programming.html","id":"basic-data-management","chapter":"1 Basics of R Programming","heading":"1.2 Basic Data Management","text":"Now basic undertanding common data types structures R, ready look elementary data management problems commonly encountered Exam PA (see complicated ones ATPA). Even imported data R usual observation--variable format, often data management issues resolved predictive model can constructed meaningfully. illustrate data mangement issues, focus small-scale dataset involving eight actuaries hypothetical company set code :row dataset, called actuary, information actuary’s name, gender, age, number exams passed, three evaluation ratings scale 1 10 assigned supervisors according three performance measures, salary.Although dataset encounter real life likely hundreds thousands observations, demonstration pruposes deliberately chosen toy dataset like actuary can appreciate changes made dataset visual inspection (small size dataset makes possible). data management techniques going illustrate, however, general efficient apply equally well large datasets.Using toy dataset, demonstrate accomplish following data management tasks R:(Deletion): delete variable x, simply column row numbers useful predictive purposes?(Deletion): delete variable x, simply column row numbers useful predictive purposes?(Missing/abnormal values): entries dataset missing make sense. deal missing abnormal entries?(Missing/abnormal values): entries dataset missing make sense. deal missing abnormal entries?(Creation + Sorting): create new numeric variable averages three evaluation ratings provides overall performance measure actuary? Based overall performance measure, actuary best performance?(Creation + Sorting): create new numeric variable averages three evaluation ratings provides overall performance measure actuary? Based overall performance measure, actuary best performance?(Identification): identify Associates (passed 7 9 exams) Fellows (passed 10 exams)? generally, create new categorical variables represents whether actuary Associate, Fellow, pre-ASA student?(Identification): identify Associates (passed 7 9 exams) Fellows (passed 10 exams)? generally, create new categorical variables represents whether actuary Associate, Fellow, pre-ASA student?(Merge): add new observations variables existing dataset?(Merge): add new observations variables existing dataset?(Compound): create compound variable represents gender smoking status actuary?(Compound): create compound variable represents gender smoking status actuary?","code":"\nx <- 1:8\nname <- c(\"Embryo Luo\", \"\", \"Peter Smith\", NA, \"Angela Peterson\", \"Emily Johnston\", \"Barbara Scott\", \"Benjamin Eng\")\ngender <- c(\"M\", \"F\", \"M\", \"F\", \"F\", \"F\", \"?\", \"M\")\nage <- c(-1, 25, 22, 50, 30, 42, 29, 36)\nexams <- c(10, 3, 0, 4, 6, 7, 5, 9)\nQ1 <- c(10, NA, 4, 7, 8, 9, 8, 7)\nQ2 <- c(9, 9, 5, 7, 8, 10, 9, 8)\nQ3 <- c(9 ,7, 5, 8, 10, 10, 7, 8)\nsalary <- c(300000, NA, 80000, NA, NA, NA, NA, NA)\nactuary <- data.frame(x, name, gender, age, exams, Q1, Q2, Q3, salary)\nactuary\n#>   x            name gender age exams Q1 Q2 Q3 salary\n#> 1 1      Embryo Luo      M  -1    10 10  9  9  3e+05\n#> 2 2                      F  25     3 NA  9  7     NA\n#> 3 3     Peter Smith      M  22     0  4  5  5  8e+04\n#> 4 4            <NA>      F  50     4  7  7  8     NA\n#> 5 5 Angela Peterson      F  30     6  8  8 10     NA\n#> 6 6  Emily Johnston      F  42     7  9 10 10     NA\n#> 7 7   Barbara Scott      ?  29     5  8  9  7     NA\n#> 8 8    Benjamin Eng      M  36     9  7  8  8     NA"},{"path":"basics-of-r-programming.html","id":"task-1-removing-unimportant-variables","chapter":"1 Basics of R Programming","heading":"1.2.1 Task 1: Removing Unimportant Variables","text":"course data analysis, may want remove variables turn redundant add little value analysis. actuary data, x variable vector row numbers provide useful information understanding eight actuaries.delete variable data frame, can use $ notation learned access variable assign special symbol NULL.","code":"\nactuary$x <- NULL\nactuary\n#>              name gender age exams Q1 Q2 Q3 salary\n#> 1      Embryo Luo      M  -1    10 10  9  9  3e+05\n#> 2                      F  25     3 NA  9  7     NA\n#> 3     Peter Smith      M  22     0  4  5  5  8e+04\n#> 4            <NA>      F  50     4  7  7  8     NA\n#> 5 Angela Peterson      F  30     6  8  8 10     NA\n#> 6  Emily Johnston      F  42     7  9 10 10     NA\n#> 7   Barbara Scott      ?  29     5  8  9  7     NA\n#> 8    Benjamin Eng      M  36     9  7  8  8     NA"},{"path":"basics-of-r-programming.html","id":"task-2-missing-values","chapter":"1 Basics of R Programming","heading":"1.2.2 Task 2: Missing Values","text":"data analysis, say missing value value available (hence missing) particular entry dataset whatever reason.R, missing values denoted special symbol NA, stands available.can see actuary dataset quite missing values:Note although name second observation left blank (indicated empty character string ““) missing value gender coded ”?“, treated technically R missing values. need identify entries manually.Missing values problmeatic data analsis many functions R return missing value arguments contain missing values. many methods dealing missing values, simple sophisticated (sophisticated ones e introduced ATPA). cases, one method can apply. simple guidelines:Removing observations: missing observations variable small part data, case Q1, deleting missing observation results virtually loss information sensible move.Removing variables: majority observation variable missing ,case salary, variable contributes little information help us understand data makes sense remove entire variable.removing observations nd/variables simple, relies assumption missing values randomly (rather systematically) generated, removal unlikely introduce bias impair effectiveness predictive model build.sake demonstration, handle missing values actuary dataset follows:Removing salary variable.Removing second fourth rows.Treating gender factor three levels, \"M\", \"F\", \"?\". (requires adjustment. like, can rename “?” level `“Unknown”.)removal salary straight forward can exactly way Task 1.Now lets proceed remove second fourth observations (rows). two commond methods, rely logical subsetting:Using .na() function: .na() function returns object size argument. elements object equal TRUE corresponding elements original object equal NA equal FALSE otherwise. Effectively, answers question hether element function’s argument NA .Using complete.cases() na.omit() functions: two functions global .na() remove rows containing missing values.gotten rid missing values, entries new dataset clearly amiss.First, age first actuary, Embryo Luo, -1, possible likely result data entry error.First, age first actuary, Embryo Luo, -1, possible likely result data entry error.Second, actuary called Peter Smith exams passed. may error, highly questionable – day age, difficult land full-time actuarial job exams passed. general, abormal values easily detected built-R functions often identified aid human input.Second, actuary called Peter Smith exams passed. may error, highly questionable – day age, difficult land full-time actuarial job exams passed. general, abormal values easily detected built-R functions often identified aid human input.deal two observations? Peter Smith problematic since completely sure whether exams entry error . judicious retain entry, discuss potential inaccuracy response, real PA exam asked discuss concerns data might .age entry Embryo Luo troublesome. now, keep proceed. However, going fit predictive model using age predictor, makes sense exclude Embryo Luo analysis make educated guess age, e.g., imput age using average age actuaries.","code":"\nactuary$salary <- NULL\nactuary\n#>              name gender age exams Q1 Q2 Q3\n#> 1      Embryo Luo      M  -1    10 10  9  9\n#> 2                      F  25     3 NA  9  7\n#> 3     Peter Smith      M  22     0  4  5  5\n#> 4            <NA>      F  50     4  7  7  8\n#> 5 Angela Peterson      F  30     6  8  8 10\n#> 6  Emily Johnston      F  42     7  9 10 10\n#> 7   Barbara Scott      ?  29     5  8  9  7\n#> 8    Benjamin Eng      M  36     9  7  8  8\nis.na(actuary$Q1)\n#> [1] FALSE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE\nactuary.1 <- actuary[!is.na(actuary$Q1),]\nactuary.1\n#>              name gender age exams Q1 Q2 Q3\n#> 1      Embryo Luo      M  -1    10 10  9  9\n#> 3     Peter Smith      M  22     0  4  5  5\n#> 4            <NA>      F  50     4  7  7  8\n#> 5 Angela Peterson      F  30     6  8  8 10\n#> 6  Emily Johnston      F  42     7  9 10 10\n#> 7   Barbara Scott      ?  29     5  8  9  7\n#> 8    Benjamin Eng      M  36     9  7  8  8\ncc <- complete.cases(actuary)\nactuary.2 <- actuary[cc, ]\nactuary.2\n#>              name gender age exams Q1 Q2 Q3\n#> 1      Embryo Luo      M  -1    10 10  9  9\n#> 3     Peter Smith      M  22     0  4  5  5\n#> 5 Angela Peterson      F  30     6  8  8 10\n#> 6  Emily Johnston      F  42     7  9 10 10\n#> 7   Barbara Scott      ?  29     5  8  9  7\n#> 8    Benjamin Eng      M  36     9  7  8  8\nactuary.3 <- na.omit(actuary)\nactuary.3\n#>              name gender age exams Q1 Q2 Q3\n#> 1      Embryo Luo      M  -1    10 10  9  9\n#> 3     Peter Smith      M  22     0  4  5  5\n#> 5 Angela Peterson      F  30     6  8  8 10\n#> 6  Emily Johnston      F  42     7  9 10 10\n#> 7   Barbara Scott      ?  29     5  8  9  7\n#> 8    Benjamin Eng      M  36     9  7  8  8\nactuary.n <- actuary.3 # cleansed dataset\nactuary.n\n#>              name gender age exams Q1 Q2 Q3\n#> 1      Embryo Luo      M  -1    10 10  9  9\n#> 3     Peter Smith      M  22     0  4  5  5\n#> 5 Angela Peterson      F  30     6  8  8 10\n#> 6  Emily Johnston      F  42     7  9 10 10\n#> 7   Barbara Scott      ?  29     5  8  9  7\n#> 8    Benjamin Eng      M  36     9  7  8  8\n\n# Number of rows with missing values\nnrow(actuary) - nrow(actuary.n)\n#> [1] 2"},{"path":"basics-of-r-programming.html","id":"task-3-adding-new-variables","chapter":"1 Basics of R Programming","heading":"1.2.3 Task 3: Adding New Variables","text":"opposite removing existing variable (Task 1) create new variable interest, may serve predictors predictive model.add new variable data frame, can use dollar sign $ notation name define new variable. generic syntax :Even new variable exist original data frame, R automatically expland data frame accomodate new variable.illustration run following code create new variable, called S added actuary.n data, measure avaerage three evaluation ratings actuary.Task 1, crucial specify name dataset host new variable. drop \"actuary.n\" left use command:new variable S exist current workspace, new variable actuary.n data frame.previous code, manually averaged three evaluation ratings actuary. However, cumbersome type ratings variable one one three periods. case, apply() function can make work lot easier powerful function general.apply() function allows us “apply” arbitrary function dimension matrix data frame, provided columns data structure. general syntax :\\[\n  \\begin{equation}\n  \\boxed{\\begin{align*}apply(DATA_FRAME, MARGIN, FUNCTION)\\end{align*}}\n  \\end{equation}\n  \\]MARGIN = 1 indciates function act rows data frame, MARGIN = 2 indicates function act columns.next code chunk calculates means Q1, Q2, Q3 using apply() function.","code":"DATA_FRAME$NEW_VARIABLE <- DEFINITION\nactuary.n$S <- (actuary.n$Q1 + actuary.n$Q2 + actuary.n$Q3) / 3\nactuary.n\n#>              name gender age exams Q1 Q2 Q3     S\n#> 1      Embryo Luo      M  -1    10 10  9  9 9.333\n#> 3     Peter Smith      M  22     0  4  5  5 4.667\n#> 5 Angela Peterson      F  30     6  8  8 10 8.667\n#> 6  Emily Johnston      F  42     7  9 10 10 9.667\n#> 7   Barbara Scott      ?  29     5  8  9  7 8.000\n#> 8    Benjamin Eng      M  36     9  7  8  8 7.667S <- (actuary.n$Q1 + actuary.n$Q2 + actuary.n$Q3) / 3\nactuary.n$S <- apply(actuary.n[, c(\"Q1\", \"Q2\", \"Q3\")], 1, mean)\nactuary.n\n#>              name gender age exams Q1 Q2 Q3     S\n#> 1      Embryo Luo      M  -1    10 10  9  9 9.333\n#> 3     Peter Smith      M  22     0  4  5  5 4.667\n#> 5 Angela Peterson      F  30     6  8  8 10 8.667\n#> 6  Emily Johnston      F  42     7  9 10 10 9.667\n#> 7   Barbara Scott      ?  29     5  8  9  7 8.000\n#> 8    Benjamin Eng      M  36     9  7  8  8 7.667"},{"path":"basics-of-r-programming.html","id":"task-4-using-logical-tests-to-identify-observations-with-certain-characteristics","chapter":"1 Basics of R Programming","heading":"1.2.4 Task 4: Using Logical Tests to Identify Observations with Certain Characteristics","text":"SOA exam curriculum, Associate identified individual passed seven nine exams Fellow passed ten exams. single associates fellows , can apply logical subsetting carefully formulated logical tests. small dataset like actuary.n, easy spot actuaries associate fellow However, subsetting techniques introduce generalize well large datasets encounted practice; datasets call systematic way search .Fellows easier identify; passed exactly ten exams. Using criterion form logical index vector row subscript, can “extract” fellows save information new data frame called actuary.FSA:Similarly, associates, can subset greater equal seven exams less equal 9 exams:","code":"\nactuary.FSA <- actuary.n[actuary.n$exams == 10, ]\nactuary.FSA\n#>         name gender age exams Q1 Q2 Q3     S\n#> 1 Embryo Luo      M  -1    10 10  9  9 9.333\nactuary.ASA <- actuary.n[actuary.n$exams >= 7 & actuary.n$exams <= 9, ]\nactuary.ASA\n#>             name gender age exams Q1 Q2 Q3     S\n#> 6 Emily Johnston      F  42     7  9 10 10 9.667\n#> 8   Benjamin Eng      M  36     9  7  8  8 7.667"},{"path":"basics-of-r-programming.html","id":"end-of-chapter-1-practice-problems","chapter":"1 Basics of R Programming","heading":"1.2.5 End of Chapter 1 Practice Problems","text":"","code":""},{"path":"basics-of-r-programming.html","id":"exercise-1.5.2-writing-a-function-to-calculate-loglikelihood","chapter":"1 Basics of R Programming","heading":"1.2.5.1 Exercise 1.5.2: Writing a Function to Calculate Loglikelihood","text":"\\(= 1, ..., n\\), let \\(Y_{}\\) Poisson random variable mean \\(\\mu_{}\\) fitted mean \\(\\widehat{\\mu}_{}\\) based given predictive model (e.g., Poisson regression model, studied Chapter 4) method maximum likelihood estimation.() Show maximum loglikelihood function :\\(\\boxed{l(\\widehat{\\mu}_{1},...,\\widehat{\\mu}_{n}) = {\\sum_{n=1}^{n}\\widehat{\\mu}_{}} +  constants}\\)provided \\(\\widehat{\\mu}_{}\\)’s well-defined.(b) Write R function called LL compute maximum loglikelihood function takes following two arguments:observed vector observed values \\(Y_{}\\)’s`predicted’ vector fitted values \\(Y_{}\\)’s(c) Use function part (b) calculate maximized loglikelihood function following data:Solution:() log-likelihood function :\\[\n  \\boxed{\n  \\begin{equation}\n  L(\\mu_{1}, ..., \\mu_{n}) \\propto {\\prod_{=1}^{n}e^{-\\mu_{}}\\mu_{}^{y_{}}} = e^{-\\sum_{=1}^{n}\\mu_{}}\\prod_{=1}^{n}\\mu_{}^{y_{}},\n  \\end{equation}\n  }\n  \\]log-likelihood function fitted means \\(\\widehat{\\mu}_{1}, ..., \\widehat{\\mu}_{n}\\) :\\[\n  \\boxed{\n  \\begin{equation}\n  l(\\mu_{1}, ..., \\mu_{n}) = LnL(\\widehat{\\mu}_{1}, ..., \\widehat{\\mu}_{n}) = \\sum_{=1}^{n}y_{}ln\\widehat{\\mu}_{} - \\sum_{=1}^{n}\\widehat{\\mu}_{} + constants\n  \\end{equation}\n  }\n  \\]\n(b) log-likelihood function involves \\(ln\\widehat{\\mu}_{}\\), defined \\(\\widehat{\\mu}_{} \\le 0\\). sidestep calculation \\(ln\\widehat{\\mu}_{}\\) non-positive \\(\\widehat{\\mu}_{}\\), need design function way \\(\\widehat{\\mu}_{}\\)’s converted small number 0.000001 \\(ln0.000001\\) computed instead. accomplished ifelse() construct. function defined:(c) given observed predicted vectors, maximized log-likelihood function equals 85.98277.","code":"\nobserved <- c(2, 3, 6, 7, 8, 9, 10, 12, 15) \npredicted <- c(2.516332, 2.516332, 7.451633, 7.451633, 7.451633, 7.451633, 12.386934, 12.386934, 12.386934)\nLL <- function(observed, predicted) {\n  predicted_pos <- ifelse(predicted <= 0, 0.000001, predicted)\n  return(sum(observed*log(predicted_pos) - predicted))\n}``` r\nobserved <- c(2, 3, 6, 7, 8, 9, 10, 12, 15) \npredicted <- c(2.516332, 2.516332, 7.451633, 7.451633, 7.451633, 7.451633, 12.386934, 12.386934, 12.386934)\n\nLL(observed, predicted)\n#> [1] 85.98\n```"},{"path":"basics-of-r-programming.html","id":"exercise-1.5.3-based-on-new-performance-measure-how-to-create-a-new-categorical-variable-that-classifies-each-actuary-as-excellent-good-unsatisfactory","chapter":"1 Basics of R Programming","heading":"1.2.5.2 Exercise 1.5.3 (Based on new performance measure, how to create a new categorical variable that classifies each actuary as “Excellent”, “Good”, “Unsatisfactory”)","text":"Consider actuary.n dataset Section 1.3. needed, run following code get set :Suppose boss six actuaries, like classify according following scheme:Write R commands perform classifications save classifications new variable called classify.Solution: Similar accomplished Task 4 (b) Section 1.3, use ifelse() make classifications:","code":"\nx <- 1:8\nname <- c(\"Embryo Luo\", \"\", \"Peter Smith\", NA, \"Angela Peterson\", \"Emily Johnston\", \"Barbara Scott\", \"Benjamin Eng\")\ngender <- c(\"M\", \"F\", \"M\", \"F\", \"F\", \"F\", \"?\", \"M\")\nage <- c(-1, 25, 22, 50, 30, 42, 29, 36)\nexams <- c(10, 3, 0, 4, 6, 7, 5, 9)\nQ1 <- c(10, NA, 4, 7, 8, 9, 8, 7)\nQ2 <- c(9, 9, 5, 7, 8, 10, 9, 8)\nQ3 <- c(9 ,7, 5, 8, 10, 10, 7, 8)\n#salary <- c(300000, NA, 80000, NA, NA, NA, NA, NA)\nactuary <- data.frame(x, name, gender, age, exams, Q1, Q2, Q3)\nactuary.n <- na.omit(actuary)\nactuary.n$S <- (actuary.n$Q1 + actuary.n$Q2 + actuary.n$Q3) / 3\nactuary.n\n#>   x            name gender age exams Q1 Q2 Q3     S\n#> 1 1      Embryo Luo      M  -1    10 10  9  9 9.333\n#> 3 3     Peter Smith      M  22     0  4  5  5 4.667\n#> 5 5 Angela Peterson      F  30     6  8  8 10 8.667\n#> 6 6  Emily Johnston      F  42     7  9 10 10 9.667\n#> 7 7   Barbara Scott      ?  29     5  8  9  7 8.000\n#> 8 8    Benjamin Eng      M  36     9  7  8  8 7.667\nactuary.n$classify <- ifelse(actuary.n$S >= 9, \"Excellent\", \n                             ifelse(actuary.n$S > 7, \"Good\", \"Unsatisfactory\"))\nactuary.n\n#>   x            name gender age exams Q1 Q2 Q3     S\n#> 1 1      Embryo Luo      M  -1    10 10  9  9 9.333\n#> 3 3     Peter Smith      M  22     0  4  5  5 4.667\n#> 5 5 Angela Peterson      F  30     6  8  8 10 8.667\n#> 6 6  Emily Johnston      F  42     7  9 10 10 9.667\n#> 7 7   Barbara Scott      ?  29     5  8  9  7 8.000\n#> 8 8    Benjamin Eng      M  36     9  7  8  8 7.667\n#>         classify\n#> 1      Excellent\n#> 3 Unsatisfactory\n#> 5           Good\n#> 6      Excellent\n#> 7           Good\n#> 8           Good"},{"path":"data-exploration-and-visualization.html","id":"data-exploration-and-visualization","chapter":"2 Data Exploration and Visualization","heading":"2 Data Exploration and Visualization","text":"Topic: Data Exploration VisualizationLearning Objectives: candidate able work various data types, understand principles data design, construct variety common visualizations common exploring data.Learning Outcomes:candidate able :\nApply key principles constructing graphs.\nApply univariate data exploration techniques.\nApply bivariate data exploration techniques.","code":""},{"path":"data-exploration-and-visualization.html","id":"chapter-02-overview","chapter":"2 Data Exploration and Visualization","heading":"Chapter Overview","text":"integral part predictive analytic exercise use graphical displays investigate characteristics variables interest, relation one another, visualize results predictive models constructed.regard, one key strengths R programming language offers versatile graphing capabilities, base installation add-packages.minimal amount code, can produce wide variety high-quality graphs. Exam PA, asked take advantage R’s graphing capabilities make sense different types graphical displays.Instead using R’s base graphical platform, make graphs using ggplot2 package, may new even used R . Compared R’s base graphics system, ggplot2 involves vastly different syntax based -called “grammar graphics” lends producing sophisticated graphs cumbersome create using base R graphics.section 2.1, learn basic structure ggplot, make simple informative plots, learn tweak appearance ggplot.Section 2.2 draws upon data visualization techniques covered Section 2.1 perform exploratory data analysis (EDA), use graphs summary statistics uncover patterns relationships set data, generate hypotheses can answered quantitatively predictive model later stage.","code":""},{"path":"data-exploration-and-visualization.html","id":"chapter-02-making-ggplots","chapter":"2 Data Exploration and Visualization","heading":"2.1 Making ggplots","text":"","code":""},{"path":"data-exploration-and-visualization.html","id":"chapter-02-making-ggplots-basic-features","chapter":"2 Data Exploration and Visualization","heading":"2.1.1 Basic Features","text":"Let’s begin installing loading ggplot2 package:last command, can use functions ggplot2 package end current R session.Skeleton: simplest form, ggplot consists two parts: core ggplot() function chain additional functions pasted together using plus (+) sign defining exact type plot made.ggplot() function: \\(\\boxed{ggplot()}\\) function initializes plot, defines source data using data argument (almost always data frame), , importantly, specifies variables data “mapped” visual elements plot mapping argument. Mappings ggplot specified using \\(\\boxed{aes()}\\) function, aes standing “aesthetics”. determine role different variables play plot. variables may, instance, correspond visual elements x- y-variables, color, size, shape, specified x, y, color, size, shape aesthetics, respectively.geom_() functions: Subsequent ggplot() function, put geometric objects, geoms short, include points, lines, bars, histograms, box-plots, many possibilities, means one geom functions. Placed layer layer, geoms determine kind plot drawn modify visual characteristics, taking data aesthetic mapping specified ggplot() function inputs.generic structure ggplot (uppercase letters placeholders.):Case Study: Personal Injury Insurance Data setTo illustrate data visualization exploration techniques, chapter look personal injury insurance data set. data set contains information 22,036 settled personal injury insurance claims. claims reported period July 1989 end 1999, claims settled zero payment excluded. variables data set described Table 2.1.TABLE 2.1: Data dictionary personal injury (persing) insurance claims dataset.Section 4.2, build model predict size personal injury insurance claims using variables data set. now, perform data exploration variables data set. insights gain go long way towards constructing good predictive model.first example, let’s make scatterplot two numeric variables persinj50 data, amt op_time. plot, produced code given Figure 2.1.1. code obeys two-part structure discussed earlier:ggplot() function: first line makes clear using persinj50 data, variables op_time amt mapped variables x-axis y-axis x y aesthetics, respectively. need name variables persinj50%op_time persinj50$amt data source already specified data argument.Geom: Given mappings, use \\(\\fbox{geom_point()}\\) make scatterplot amt (y-variable) op_time (x-variable). plot comprises 50 points corresponding 50 paired values two variables allows us see two variables comparison .Later, fine-tune plot different ways capture different sorts information.Now let’s see example using color aesthetic correctly. persinj50 data, legrep variable binary variable equal 1 injuries legal representation 0 without.color different injuries according presence legal representation, map color aesthetic legrep treated factor.resulting scatterplot, generated code provided, injuries without legal representation (legrep = 0) displayed red whereas legal representation (legrep = 1) displayed teal. legend produced accordingly.Notice genuine mapping legrep variable color, legrep = 0 mapped red color legrep = 1 mapped teal color. words, observations differentiated basis legrep variable color.Important Geoms Exam PATABLE 2.2: Important Geoms Exam PA Commonly Used Arguments.illustrate use geom_bar(), geom_boxplot(), geom_histogram() Section 2.2. now, let’s continue scatterplots just produced make informative.next figure, plot 50 observations persinj50 data set using large points (size = 2) small amount transparency (alpha = 0.5), classify according whether legal representation , fit separate smoothed curve kind injuries via geom_smooth() function. commands collected next code chunk:make single smoothed line data points, leave coloring points legrep, move color aesthetic geom_point() function.Faceting\\(\\fbox{Faceting}\\) another useful way categorize data distinct groups. grouping showcases two groups observations single plot, faceting displays observations separate plots (known “small multiple” plot) produced value faceting variable placed side--side, usually scale, facilitate comparison.","code":"\n#install.packages(\"ggplot2\")\nlibrary(ggplot2)ggplot(data = DATA, mapping = aes(AESTHETIC_1 = VARIABLE_1,\n                                  AESTHETIC_2 = VARIABLE_2,\n                                  ...)) + \n  geom_TYPE(...) + \n  geom_TYPE(...) +\n  OTHER_FUNCTIONS +\n  ...\npersinj <- read.csv(\"data/persinj.csv\")\n\n# Take out a subset of 50 observations from the full set of data\npersinj50 <- persinj[seq(1, nrow(persinj), length = 50), ]\npersinj50\n#>             amt inj legrep op_time\n#> 1         87.75   1      0     0.1\n#> 450      195.60   1      0     1.9\n#> 900     6777.93   1      1     3.8\n#> 1350    8738.44   1      0     5.7\n#> 1799    3555.73   6      0     7.5\n#> 2249    5158.06   2      0     9.4\n#> 2699    5612.93   1      1    11.2\n#> 3148    8994.05   1      0    13.1\n#> 3598    1570.34   1      1    14.9\n#> 4048    5735.00   1      0    16.8\n#> 4497   15702.67   1      0    18.7\n#> 4947   15602.52   1      1    20.6\n#> 5397    5445.00   1      1    22.5\n#> 5847   32935.42   1      1    24.4\n#> 6296    2568.08   1      1    26.3\n#> 6746    9349.03   1      0    28.2\n#> 7196    2726.51   1      0    30.0\n#> 7645   10907.30   2      0    31.8\n#> 8095   25149.49   2      1    33.8\n#> 8545   11683.60   1      0    35.7\n#> 8994   12800.75   1      1    37.6\n#> 9444   13500.00   1      1    39.4\n#> 9894    1742.62   9      0    41.2\n#> 10343  24373.94   1      0    43.1\n#> 10793  21907.70   1      1    45.0\n#> 11243  47229.71   1      1    46.8\n#> 11693   2562.70   1      0    48.6\n#> 12142  15459.63   1      1    50.5\n#> 12592  42720.13   2      0    52.4\n#> 13042  16163.86   1      1    54.3\n#> 13491  15446.68   1      0    56.2\n#> 13941 174364.45   2      1    58.1\n#> 14391  10838.48   1      1    60.0\n#> 14840   6500.00   1      1    61.9\n#> 15290  36577.13   4      0    63.9\n#> 15740 126588.00   1      1    65.7\n#> 16189  35599.60   1      1    67.7\n#> 16639  48254.71   1      1    69.7\n#> 17089  65178.91   1      1    71.8\n#> 17539  24684.92   1      1    73.9\n#> 17988 578733.84   2      1    75.8\n#> 18438  72916.85   1      1    77.7\n#> 18888  52935.50   1      0    79.6\n#> 19337  71392.93   3      1    81.6\n#> 19787 107462.51   1      1    83.8\n#> 20237 166986.28   2      1    86.0\n#> 20686  15605.74   1      0    88.3\n#> 21136 102440.48   2      1    90.8\n#> 21586 204022.11   3      1    93.8\n#> 22036 117562.73   1      1    99.1\nggplot(data = persinj50, mapping = aes(x = op_time, y = amt), caption=\"A basic scatterplot of `amt` against `op_time` in the `persinj50` dataset.\") +\n  geom_point(color = \"blue\")\nggplot(data = persinj50, mapping = aes(x = op_time, y = amt, color = factor(legrep))) +\n         geom_point()\nggplot(data = persinj50, mapping=aes(x=op_time, y=amt, color=factor(legrep), fill=factor(legrep))) + \n  geom_point(size=2, alpha=0.5) + \n  geom_smooth()\nggplot(persinj50, aes(x = op_time, y = amt)) +\n  geom_point(aes(color = factor(legrep)), size = 2, alpha = 0.5) +\n  geom_smooth()"},{"path":"data-exploration-and-visualization.html","id":"chapter-02-making-ggplots-customizing-your-plots","chapter":"2 Data Exploration and Visualization","heading":"2.1.2 Customizing Your Plots","text":"","code":"\nggplot(data = persinj50,\n       mapping = aes(\n         x = op_time,\n         y = amt,\n         color = factor(legrep),\n         fill = factor(legrep)\n       )) +\n  geom_point(size = 2, alpha = 0.5) +\n  geom_smooth() +\n  labs(title = \"Personal Injury Dataset\", x = \"Operational Time\", y = \"Claim Amount\") +\n  coord_cartesian(ylim = c(-200000, 300000))"},{"path":"data-exploration-and-visualization.html","id":"chapter-02-data-exploration","chapter":"2 Data Exploration and Visualization","heading":"2.2 Data Exploration","text":"section, apply data visualization techniques perform Exploratory Data Analysis (EDA), indispensable part predictive modeling exercise.EDA integral part predictive modeling serves two important purposes:Data Validation: allows us perform commonsense checks identify nonsensical data values (e.g., negative value age, impossible), potential data errors may lead unreasonable model results fixed. inappropriate data values removed, data becomes ready analysis.Data Validation: allows us perform commonsense checks identify nonsensical data values (e.g., negative value age, impossible), potential data errors may lead unreasonable model results fixed. inappropriate data values removed, data becomes ready analysis.Characteristics Variables: EDA helps us understand key characteristics variables data. understanding may suggest useful ways pre-process variables improve predictive performance interpreteability models construct, , importantly, decide appropriate type predictive model likely meet business needs.Characteristics Variables: EDA helps us understand key characteristics variables data. understanding may suggest useful ways pre-process variables improve predictive performance interpreteability models construct, , importantly, decide appropriate type predictive model likely meet business needs.Typically, EDA accomplished combination two kinds tools:Descriptive Statistics (Summary Statistics)\nPurpose: Quickly summarize different distributional properties variable(s) interest.\nExamples: Mean, Variance, Mode, Correlation, Table Frequency Counts\nPurpose: Quickly summarize different distributional properties variable(s) interest.Examples: Mean, Variance, Mode, Correlation, Table Frequency CountsGraphical Displays (Visual Displays)\nPurpose: Allows us get quick impression overall distribution variable(s) interest.Graphs often informative table summary statistics sometimes can reveal information missed otherwise, presence outliers.\nExamples: Histograms, Box Plots, Bar Charts, associated variants.\nPurpose: Allows us get quick impression overall distribution variable(s) interest.Graphs often informative table summary statistics sometimes can reveal information missed otherwise, presence outliers.Examples: Histograms, Box Plots, Bar Charts, associated variants.section, return full persinj data (22,036 observations) use illustrate creation interpretation descriptive statistics graphical displays.","code":""},{"path":"data-exploration-and-visualization.html","id":"chapter-02-data-exploration-univariate","chapter":"2 Data Exploration and Visualization","heading":"2.2.1 Univariate Data Exploration","text":"Let’s begin univariate data exploration – exploration sheds light distribution one variable time.specific statistics graphical tools depend whether variables analyzing numeric categorical.","code":""},{"path":"data-exploration-and-visualization.html","id":"chapter-02-data-exploration-univariate-numeric","chapter":"2 Data Exploration and Visualization","heading":"Numeric Variables","text":"Descriptive Statistics: Statistical summaries mainly used reveal two aspects distribution numeric variable:\nCentral Tendency: central tendency numeric variable, whether continuous discrete, often quantified mean median. two metrics capture typical “size” variable can readily produced R applying \\(\\fbox{summary()}\\) function variable interest.\nDispersion: Common measures dispersion include variance, standard deviation interquartile range (IQR) (defined difference 75% quantile 25% quantile variable(, measure spread values numeric variable range.\nDescriptive Statistics: Statistical summaries mainly used reveal two aspects distribution numeric variable:Central Tendency: central tendency numeric variable, whether continuous discrete, often quantified mean median. two metrics capture typical “size” variable can readily produced R applying \\(\\fbox{summary()}\\) function variable interest.Dispersion: Common measures dispersion include variance, standard deviation interquartile range (IQR) (defined difference 75% quantile 25% quantile variable(, measure spread values numeric variable range.Graphical Displays: visualize distribution numeric variables, histograms box-plots convenient graphical aids.\nHistograms divide observations several equally spaced bins (buckets) provide visual summary count relative frequency bin, allowing us learn overall shape distribution numeric variable observations lie.\nbox-plots visualize distribution numeric variables placing 25% quantile, median, 75% quantile “box”, rest data points constituting “whiskers”.\namount spacing different parts boxplot reflects degree dispersion skewness variable’s distribution.\n“Outliers”, defined data points 1.5 times interquartile range either edge box, shown large dotted points.\n\nbox-plots directly show actual shape variable’s distribution, offer useful graphical summary key numeric statistics allow visual comparison distributions different numeric variables (e.g., relative magnitude median dispersion) distribution numeric variable across different levels another categorical variable.Graphical Displays: visualize distribution numeric variables, histograms box-plots convenient graphical aids.Histograms divide observations several equally spaced bins (buckets) provide visual summary count relative frequency bin, allowing us learn overall shape distribution numeric variable observations lie.Histograms divide observations several equally spaced bins (buckets) provide visual summary count relative frequency bin, allowing us learn overall shape distribution numeric variable observations lie.box-plots visualize distribution numeric variables placing 25% quantile, median, 75% quantile “box”, rest data points constituting “whiskers”.\namount spacing different parts boxplot reflects degree dispersion skewness variable’s distribution.\n“Outliers”, defined data points 1.5 times interquartile range either edge box, shown large dotted points.\nbox-plots visualize distribution numeric variables placing 25% quantile, median, 75% quantile “box”, rest data points constituting “whiskers”.amount spacing different parts boxplot reflects degree dispersion skewness variable’s distribution.amount spacing different parts boxplot reflects degree dispersion skewness variable’s distribution.“Outliers”, defined data points 1.5 times interquartile range either edge box, shown large dotted points.“Outliers”, defined data points 1.5 times interquartile range either edge box, shown large dotted points.box-plots directly show actual shape variable’s distribution, offer useful graphical summary key numeric statistics allow visual comparison distributions different numeric variables (e.g., relative magnitude median dispersion) distribution numeric variable across different levels another categorical variable.Summary StatisticsIn persinj data, two numeric variables, amt op_time. following code, focus amt variable purposes illustration apply summary() function amt variable:summary() function applied numeric variable, six-number “summary” produced.Based summary output, make following observations:mean claim amount (38,367) way higher median (13,854) 75th percentile much away median 25th percentile, indicating distribution claim amount highly skewed right.mean claim amount (38,367) way higher median (13,854) 75th percentile much away median 25th percentile, indicating distribution claim amount highly skewed right.right skew suggests values right mean claim amount tend away mean left, heavy tail extends far right.right skew suggests values right mean claim amount tend away mean left, heavy tail extends far right.largest claim amount, 4,485,797, almost astronomical figure compared mean median.largest claim amount, 4,485,797, almost astronomical figure compared mean median.following exercise, calculate summary statistics two groups injuries classified legal representation, binary categorical variable. helps us understand effect legal representation claim amount.","code":"\n# Reload the data\npersinj <- read.csv(\"data/persinj.csv\")\n\n# Print a summary of the `amt` variable\nsummary(persinj$amt)\n#>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#>      10    6297   13854   38367   35123 4485797"},{"path":"data-exploration-and-visualization.html","id":"exercise-2.2.2---calculating-the-summary-statistics-for-two-groups-of-observations","chapter":"2 Data Exploration and Visualization","heading":"Exercise 2.2.2 - Calculating the summary statistics for two groups of observations","text":"Write R code calculate summary statistics claim amount separately injuries legal representation without legal representation. Comment central tendency dispersion claim amount two groups injuries.Solution: extract two groups injuries, can use method logical sub-setting split data set two subsets.subset called persinj.0 corresponding injuries without legal representation (legrep = 0).subset called persinj.1 corresponding injuries legal representation (legrep = 1).look summary statistics claim amount variable within two subsets.comparison apparent: Claims legal representation larger average, also spread . relative variability confirmed standard deviations two groups claim amounts computed sd() function.HistogramsNow turn visual representations. ggplot2, histogram constructed geom_histogram() function, requires x-aesthetic capturing numeric variable interest.next set code, make four histograms colored blue claim amount variable different choices \\(\\fbox{bins}\\) parameter, controls number bins histogram.","code":"\npersinj.0 <- persinj[persinj$legrep == 0, ]\npersinj.1 <- persinj[persinj$legrep == 1, ] \n\nsummary(persinj.0$amt)\n#>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#>      10    4061   11164   32398   29641 2798362\nsummary(persinj.1$amt)\n#>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#>      20    7305   15309   41775   38761 4485797\n\nsd(persinj.0$amt)\n#> [1] 77820\nsd(persinj.1$amt)\n#> [1] 97541"},{"path":"data-exploration-and-visualization.html","id":"problems-with-skewed-data-and-possible-solutions","chapter":"2 Data Exploration and Visualization","heading":"2.2.1.0.1 Problems with Skewed Data and Possible Solutions","text":"predictive modeling, often undesirable right-skewed target variable two reasons:Predictive Power\nobjective predictive analytics study association target variable predictors data wide range variable values.\nobservations target variable concentrated small-value range, difficult investigate effect predictors target variable globally, since simply don’t know enough target variable right tail.\nidea applies right-skewed predictor; predictor exhibits heavy right-skew, unable differentiate observations target variable effectively basis values predictor, concentrated small-value range.\nobjective predictive analytics study association target variable predictors data wide range variable values.observations target variable concentrated small-value range, difficult investigate effect predictors target variable globally, since simply don’t know enough target variable right tail.idea applies right-skewed predictor; predictor exhibits heavy right-skew, unable differentiate observations target variable effectively basis values predictor, concentrated small-value range.Model Fitting\nnumber predictive models (e.g., linear models, decision trees) fitted minimizing sum squared discrepancies observed values predicted values target variable.\ntarget variable right-skewed, outliers, extreme values, contribute substantially sum disproportionate effect (“leverage”) model, undesirable unless right tail main concern lies.\nnumber predictive models (e.g., linear models, decision trees) fitted minimizing sum squared discrepancies observed values predicted values target variable.target variable right-skewed, outliers, extreme values, contribute substantially sum disproportionate effect (“leverage”) model, undesirable unless right tail main concern lies.correct skewness, can apply monotone concave function shrink outliers relative smaller values symmetrize overall distribution.dampen effects extreme values model tend improve overall goodness--fit using original right-skewed variable.Two commonly used transformations dealing right-skewed variables :Log Transformation\ntransformation (typically w.r.t. base e) can applied long variable interest strictly positive. None variable values zero negative.\ntransformation (typically w.r.t. base e) can applied long variable interest strictly positive. None variable values zero negative.Square Root Transformation\nAlthough discussed Exam PA modules, square root transformation similar spirit log transformation, applicable even non-negative variables, whose values can zero.\nAlthough discussed Exam PA modules, square root transformation similar spirit log transformation, applicable even non-negative variables, whose values can zero.see effects log square root transformations action, following code produces histogram log claim amount square root claim amount.Based resulting histograms, make following observations:evident log transformation effectively removed right-skewness claim amount made resulting distribution much symmetric.evident log transformation effectively removed right-skewness claim amount made resulting distribution much symmetric.contrast, square root claim amount remains highly right-skewed.contrast, square root claim amount remains highly right-skewed.general, log transformation better job remedying right-skewness variable square root transformation, may overdo things make transformed variable left-skewed.general, log transformation better job remedying right-skewness variable square root transformation, may overdo things make transformed variable left-skewed.extreme skewness claim amount persisj data, log transformation appropriate transformation use adopt rest section. see log transformation makes much easier discover relationships variables.","code":""},{"path":"data-exploration-and-visualization.html","id":"outliers","chapter":"2 Data Exploration and Visualization","heading":"2.2.1.0.2 Outliers","text":"mentioned outliers discussion skewed data . Let’s digress slightly look outliers greater depth, tested recent PA exams.universal quantitative definition outliers. generally think outliers anomalous data points substantially differ overall pattern data appear strange (categorical variables, observations sparse factor levels can considered outliers).Outliers typically arise following two ways:Errors: Outliers can arise due errors data collection process, data entry errors. observations whose values ridiculous can safely dismissed erroneous, negative age current customer born 1600. outlier erroneous, reasonable correct error remove entirely.Errors: Outliers can arise due errors data collection process, data entry errors. observations whose values ridiculous can safely dismissed erroneous, negative age current customer born 1600. outlier erroneous, reasonable correct error remove entirely.Natural: Outliers can also arise naturally. necessarily errors, simply observations whose values far away rest data, possible theory. Examples include policyholder currently aged 140 actuary earning $10 million year.Natural: Outliers can also arise naturally. necessarily errors, simply observations whose values far away rest data, possible theory. Examples include policyholder currently aged 140 actuary earning $10 million year.Natural outliers harder deal erroneous outliers. possible options:","code":""},{"path":"data-exploration-and-visualization.html","id":"density-plots","chapter":"2 Data Exploration and Visualization","heading":"2.2.1.0.3 Density Plots","text":"moving next type graphical displays numeric variables, let’s also mention density plots, smoothed scaled versions histograms displaying “density” rather counts vertical axis.use geom_density() function make density plots log claim amount square root claim amount:Comparing histogram, can see density plots shape histograms can interpreted way. note vertical axis density plots shows “density” rather “count” area density curve always 1.","code":"\nlibrary(ggplot2)\nlibrary(gridExtra)\n\npersinj <- read.csv(\"data/persinj.csv\")\n\np1 <- ggplot(data=persinj, mapping=aes(x=log(amt))) +\n  geom_density()\np2 <- ggplot(data=persinj, mapping=aes(x=sqrt(amt))) +\n  geom_density()\n\ngrid.arrange(p1, p2, ncol=2)"},{"path":"data-exploration-and-visualization.html","id":"boxplots","chapter":"2 Data Exploration and Visualization","heading":"2.2.1.0.4 Boxplots","text":"Besides histograms, \\(\\fbox{boxplots}\\) also convenient graphical aids visualize distribution numeric variable.Boxplots constructed ggplot2 geom_boxplot() function, takes y aesthetic representing variable interest (x aesthetic optional, can added achieve splitting).following code draws boxplot claim amount log claim amount:raw claim amounts close 25% percentile, median, 75% percentile degenerate line, log transformation corrects skewness re-positions data points much easier visual inspection.Still quite number “outliers”, shown large dotted points.","code":"\nlibrary(ggplot2)\nlibrary(gridExtra)\n\np1 <- ggplot(data=persinj, mapping=aes(y=amt)) + \n  geom_boxplot()\n\np2 <- ggplot(data=persinj, mapping=aes(y=log(amt))) + \n  geom_boxplot()\n\ngrid.arrange(p1, p2, ncol=2)"},{"path":"data-exploration-and-visualization.html","id":"chapter-02-data-exploration-univariate-categorical","chapter":"2 Data Exploration and Visualization","heading":"Categorical Variables","text":"","code":""},{"path":"data-exploration-and-visualization.html","id":"case-1-given-raw-data-more-common-in-exam-pa","chapter":"2 Data Exploration and Visualization","heading":"2.2.1.0.5 Case 1: Given Raw Data (More common in Exam PA)","text":"Descriptive Statistics (Frequency Tables): Categorical variables, even coded numbers, always natural order, statistical summaries like mean median may make sense. understand distribution categorical variables, can look relative frequency levels frequency table, constructed table() function R.Descriptive Statistics (Frequency Tables): Categorical variables, even coded numbers, always natural order, statistical summaries like mean median may make sense. understand distribution categorical variables, can look relative frequency levels frequency table, constructed table() function R.Graphical Displays (Bar Charts): number levels categorical variable increases, frequency table becomes difficult read. cases, frequencies per se important; truly matters relative magnitude. regard, bar charts extract information frequency level variable. Looking bar chart, can easily tell levels popular ones minimal observations.Graphical Displays (Bar Charts): number levels categorical variable increases, frequency table becomes difficult read. cases, frequencies per se important; truly matters relative magnitude. regard, bar charts extract information frequency level variable. Looking bar chart, can easily tell levels popular ones minimal observations.persinj data set two categorical variables, injury code (inj) legal representation (legrep). Recall Table 2.1 inj seven levels, legrep binary.following code, make two frequency tables inj, one showing raw counts, one showing percentage counts seven levels inj.can see predominant group injuries hose injury code 1, followed codes 2, 9 3. three groups minimal observations.numbers frequency table can depicted bar chart created ggplot2 \\(\\fbox{geom_bar()}\\) function, takes x aesthetic representing categorical variable interest produces bar proportional number observations level variable.following code produces two bar charts injury code corresponding two frequency tables last code chunk:","code":"\ntable(persinj$inj)\n#> \n#>     1     2     3     4     5     6     9 \n#> 15638  3376  1133   189   188   256  1256\n\ntable(persinj$inj)/nrow(persinj)\n#> \n#>        1        2        3        4        5        6 \n#> 0.709657 0.153204 0.051416 0.008577 0.008531 0.011617 \n#>        9 \n#> 0.056998"},{"path":"data-exploration-and-visualization.html","id":"case-2-given-summarized-data","chapter":"2 Data Exploration and Visualization","heading":"2.2.1.0.6 Case 2: Given Summarized Data","text":"real applications, uncommon data grouped, summarized form advanced, make manageable.case persinj data set, instead information individual claim, data may already grouped certain categorical variables, inj code.code produces version persinj data, called persinj_by_inj, shows number observations level inj. code involves tidyverse package, featured past PA exams. Instead worrying somewhat convoluted code syntax, pay attention output, far important new exam format.output shows, example, 15,638 observations injury code 1, consistent frequency table previously produced.persinj_by_inj data set (longer access original persinj data set), can display counts injury code?geom_bar() function work well. map inj variable x aesthetic geom_bar() function, function keep track many times distinct value inj occurred. treat inj variable 7 distinct values {1, 2, 3, 4, 5, 6, 9}, appears , definitely want.Instead, \\(\\fbox{geom_col()}\\) function suit purpose. inj mapped x aesthetic count y aesthetic, function display counts value inj, following code shows:bar chart identical one left panel Figure 2.2.6 based original version persinj data set.summary:geom_bar() function visualizing distribution categorical variable given individual (raw) data, geom_col() function purpose, given grouped (summarized) data.","code":"\n#install.packages(\"tidyverse\")\n\nlibrary(tidyverse)\n\n\npersinj_by_inj <- persinj %>% \n  group_by(inj) %>% # grouped by inj\n  summarize(count=n()) # count the no. of obs. for each level of inj\n\npersinj_by_inj\n#> # A tibble: 7 × 2\n#>   inj   count\n#>   <fct> <int>\n#> 1 1     15638\n#> 2 2      3376\n#> 3 3      1133\n#> 4 4       189\n#> # ℹ 3 more rows\np1 <- ggplot(data=persinj_by_inj, mapping=aes(x=inj, y=count)) + \n  geom_col(fill=\"blue\")\n\ngrid.arrange(p1, ncol=1)"},{"path":"data-exploration-and-visualization.html","id":"chapter-02-data-exploration-bivariate","chapter":"2 Data Exploration and Visualization","heading":"2.2.2 Bivariate Data Exploration","text":"Data exploration becomes even intriguing challenging two variables analyzed together rather isolation. important advantage revealing relationships, patterns, outliers become apparent variables considered combination one another.subsection therefore focuses bivariate data exploration, pairs variables investigated either numerically graphically identify potentially interesting relationships can provide useful input predictive model.particular interest relationship target variable predictor variable given setting.three types bivariate combinations, depending type variables examined:","code":""},{"path":"data-exploration-and-visualization.html","id":"chapter-02-data-exploration-bivariate-numeric-vs-numeric","chapter":"2 Data Exploration and Visualization","heading":"Case 1: Numeric vs. Numeric","text":"Descriptive Statistics: easy way summarize linear relationship two numeric variables correlation coefficient, simply correlation,unit-free metric scale -1 +1.\nCase 1: correlation +1, two variables perfectly positively correlated.\nCase 2: correlation 0, two variables uncorrelated.\nCase 3: correlation -1, two variables perfectly negatively correlated.\nextreme correlation values almost never arise real data sets, provide useful benchmarks judging size typical correlation. larger correlation magnitude (e.g., closer +1 -1), stronger degree linear association two variables.\nfollowing code, use \\(\\fbox{cor()}\\) function compute correlation claim amount operational time, log-transformed claim amount operational time persinj data.\n\ncor(persinj$amt, persinj$op_time)\n#> [1] 0.3466\n\ncor(log(persinj$amt), persinj$op_time)\n#> [1] 0.6071\ntwo variables moderately positively correlated original scale, correlation becomes stronger claim amount log scale.\nmuch correlation compact summary extent two numeric variables move tandem, can capture linear relationships. zero correlation means two variables linearly related, may related subtle ways. complex relationships (e.g., quadratic) can revealed effectively graphical displays.Descriptive Statistics: easy way summarize linear relationship two numeric variables correlation coefficient, simply correlation,unit-free metric scale -1 +1.Case 1: correlation +1, two variables perfectly positively correlated.Case 2: correlation 0, two variables uncorrelated.Case 3: correlation -1, two variables perfectly negatively correlated.extreme correlation values almost never arise real data sets, provide useful benchmarks judging size typical correlation. larger correlation magnitude (e.g., closer +1 -1), stronger degree linear association two variables.following code, use \\(\\fbox{cor()}\\) function compute correlation claim amount operational time, log-transformed claim amount operational time persinj data.two variables moderately positively correlated original scale, correlation becomes stronger claim amount log scale.much correlation compact summary extent two numeric variables move tandem, can capture linear relationships. zero correlation means two variables linearly related, may related subtle ways. complex relationships (e.g., quadratic) can revealed effectively graphical displays.Exercise 2.2.3 (can say two variables zero correlation?)Consider following data set two variables:Determine following statements two variables true:() correlation X Y positive; dependent. (B) correlation X Y positive; unrelated. (C) correlation X Y zero; dependent. (D) correlation X Y zero; unrelated. (E) correlation X Y negative; dependent.Solution: following code, compute (sample) correlation X Y.zero correlation suggests X Y linearly unrelated. However, two variables perfectly dependent via quadratic relationship \\(Y = X^2\\).\n(Answer: (C))Graphical Displays: relationship numeric variables (one usually target variable) typically visualized \\(\\fbox{scatterplot}\\), values two variables graphed two-dimensional plane, saw Section 2.1.\nplot often gives us good sense nature relationship (e.g., increasing, decreasing, polynomial, periodic) two numeric variables sometimes yields insights correlations alone provide. explains scatterplots one commonly used graphical displays data exploration.\nfollowing code makes two scatterplots, one claim amount, one log claim amount, operational time (see Figure 2.2.7). set alpha small value due large number overlapping observations.\nplots exhibit increasing relationship, scatterplot log claim amount displays much conspicuous upward slopping trend, indicating log claim amount approximately positively linear operational time. manifestation merits log transformation uncovering relationships otherwise obscure.\n\np1 <- ggplot(data=persinj, mapping=aes(x=op_time, y=amt)) + \n  geom_point(alpha=0.05) + \n  geom_smooth(method=\"lm\", se=FALSE)\n\np2 <- ggplot(data=persinj, mapping=aes(x=op_time, y=log(amt))) + \n  geom_point(alpha=0.05) + \n  geom_smooth(method=\"lm\", se=FALSE)\n\ngrid.arrange(p1, p2, ncol=2, bottom=\"Scatterplots claim amount (left) log claim amount (right) operational time `persinj` dataset.\")\n\nAlthough scatterplot confined depicting relationship two numeric variables, effect third, categorical variable can incorporated investigated corating observations color, shape, visual elements according levels assumed third variable.\nway, can visually inspect relationship two numeric variables levels third, categorical variable. statistical language, phenomenon known \\(fbox{interaction}\\), study Section 3.2, important modeling issue keep mind constructing effective predictive model.\nNow run following code make scatterplot log claim amount operational time observations color-distinguished legal representation (note color aesthetic mapped legrep): see Figure 2.2.8.\n\np1 <- ggplot(persinj, aes(x=op_time, y=log(amt), color=legrep)) + \n  geom_point(alpha=0.25) + \n  geom_smooth(method=\"lm\", se=FALSE)\n\ngrid.arrange(p1, ncol=1, bottom=\" Scatterplot log claim amount operational time colored legal representation `persinj` dataset.\")\n\nscatterplot shows two smoothed lines corresponding two levels legrep markedly different slopes intercepts (keep mind log scale, small change intercept slope can matter lot original scale).\nwords, linear relationship log claim amount operational time depends materially whether legal representation present . can also roughly tell effect legal representation (log ) claim amount:\nInjuries legal representation (e.g., `legrep` = 1) tend produce higher claim amounts, except operational time extraordinarily large (90 higher), case legal representation seem noticeable impact claim amount.\nSection 4.2, formally assess extent interaction construct model properly takes interaction effect account.Graphical Displays: relationship numeric variables (one usually target variable) typically visualized \\(\\fbox{scatterplot}\\), values two variables graphed two-dimensional plane, saw Section 2.1.plot often gives us good sense nature relationship (e.g., increasing, decreasing, polynomial, periodic) two numeric variables sometimes yields insights correlations alone provide. explains scatterplots one commonly used graphical displays data exploration.following code makes two scatterplots, one claim amount, one log claim amount, operational time (see Figure 2.2.7). set alpha small value due large number overlapping observations.plots exhibit increasing relationship, scatterplot log claim amount displays much conspicuous upward slopping trend, indicating log claim amount approximately positively linear operational time. manifestation merits log transformation uncovering relationships otherwise obscure.Although scatterplot confined depicting relationship two numeric variables, effect third, categorical variable can incorporated investigated corating observations color, shape, visual elements according levels assumed third variable.way, can visually inspect relationship two numeric variables levels third, categorical variable. statistical language, phenomenon known \\(fbox{interaction}\\), study Section 3.2, important modeling issue keep mind constructing effective predictive model.Now run following code make scatterplot log claim amount operational time observations color-distinguished legal representation (note color aesthetic mapped legrep): see Figure 2.2.8.scatterplot shows two smoothed lines corresponding two levels legrep markedly different slopes intercepts (keep mind log scale, small change intercept slope can matter lot original scale).words, linear relationship log claim amount operational time depends materially whether legal representation present . can also roughly tell effect legal representation (log ) claim amount:Section 4.2, formally assess extent interaction construct model properly takes interaction effect account.","code":"\ncor(persinj$amt, persinj$op_time)\n#> [1] 0.3466\n\ncor(log(persinj$amt), persinj$op_time)\n#> [1] 0.6071\nX <- c(-1, 0, 1)\nY <- c(1, 0, 1)\ncor(X, Y)\n#> [1] 0\np1 <- ggplot(data=persinj, mapping=aes(x=op_time, y=amt)) + \n  geom_point(alpha=0.05) + \n  geom_smooth(method=\"lm\", se=FALSE)\n\np2 <- ggplot(data=persinj, mapping=aes(x=op_time, y=log(amt))) + \n  geom_point(alpha=0.05) + \n  geom_smooth(method=\"lm\", se=FALSE)\n\ngrid.arrange(p1, p2, ncol=2, bottom=\"Scatterplots of claim amount (left) and the log of claim amount (right) against operational time in the `persinj` dataset.\")\np1 <- ggplot(persinj, aes(x=op_time, y=log(amt), color=legrep)) + \n  geom_point(alpha=0.25) + \n  geom_smooth(method=\"lm\", se=FALSE)\n\ngrid.arrange(p1, ncol=1, bottom=\" Scatterplot of the log of claim amount against operational time colored by legal representation in the `persinj` dataset.\")Injuries with legal representation (e.g., those with `legrep` = 1) tend to produce higher claim amounts, except when operational time is extraordinarily large (90 or higher), in which case legal representation does not seem to have a noticeable impact on claim amount."},{"path":"data-exploration-and-visualization.html","id":"chapter-02-data-exploration-bivariate-numeric-vs-categorical","chapter":"2 Data Exploration and Visualization","heading":"Case 2: Numeric vs. Categorical","text":"understand interplay numeric categorical variable, best investigate distribution former indexed (“split”) possible level latter.effect, looking conditional distribution numeric variable given different levels categorical variable.Descriptive Statistics: summarize association numeric variable categorical variable, can partition data different subsets, one subset level categorical variable, compute mean numeric variable . conditional means varying substantially may suggest strong relationship two variables.code , produce table mean log-transformed claim amount (also fine use untransformed claim amount variable) split different levels inj legrep, categorical.clear claim amount average increases injury code 1 injury code 4, decreases injury code 9. two means split legrep agreement observed Figure 2.2.8, shows larger claim amounts associated use legal representation.Graphical Displays: conditional distribution numeric variable given second, categorical variable best visualized \\(\\fbox{split boxplots}\\), series boxplots numeric variable split categorical variable made.following code constructs two split boxplots log claim amount, one split injury code one split legal representation (Figure 2.2.9). categorical variable used split numeric variable simply enters x aesthetic collection boxplots numeric variable level categorical variable shown. two boxplots support findings based summary statistics , turn powerfully diagrams.following code, split log claim amount injury code (x aesthetic), followed legal representation (fill aesthetic) within injury code, view three-way relationship (Figure 2.2.10).Now effect legal representation even pronounced: Regardless injury code, larger claim sizes tend injuries legal representation, effect prominent injuries code 5 code 9.\nFIGURE 2.1: Boxplots log claim amount split injury legal representation persinj data.\nAlthough effective split boxplots (opinion), histograms can also adapted visualize distribution numeric variable split categorical variable. can either histograms stacked top one another (using fill aesthetic) highlight contribution categorical level overall distribution numeric variable, dodged histograms bin placed side side comparison (note argument \\(\\fbox{position=\"dodge\"}\\)).following code produces types histograms log claim amount split legal representation (Figure 2.2.11). dodged histogram, necessary specify \\(\\fbox{y=..density..}\\) histogram shows density rather raw counts; counts misleading lot injuries legal representation without.histograms suggest larger claims (e.g., log(amt) greater 9) tend legal representation, expected.\nFIGURE 2.2: Stacked (top) dodged (bottom) histograms log claim amount persinj dataset.\n","code":"\nlibrary(tidyverse)\n\n# Partitioned by `inj` code\npersinj %>% \n  group_by(inj) %>%\n  summarize(\n    mean=mean(log(amt)),\n    median=median(log(amt)),\n    n=n()\n  )\n#> # A tibble: 7 × 4\n#>   inj    mean median     n\n#>   <fct> <dbl>  <dbl> <int>\n#> 1 1      9.37   9.36 15638\n#> 2 2     10.3   10.3   3376\n#> 3 3     10.7   10.9   1133\n#> 4 4     11.0   11.2    189\n#> # ℹ 3 more rows\n\n# Partitioned by `legrep`\npersinj %>% \n  group_by(legrep) %>%\n  summarize(\n    mean=mean(log(amt)),\n    median=median(log(amt)),\n    n=n()\n  )\n#> # A tibble: 2 × 4\n#>   legrep  mean median     n\n#>   <fct>  <dbl>  <dbl> <int>\n#> 1 0       9.18   9.32  8008\n#> 2 1       9.77   9.64 14028\np1 <- ggplot(persinj, aes(x=inj, y=log(amt))) +\n  geom_boxplot()\np1 <- ggplot(persinj, aes(x=legrep, y=log(amt))) +\n  geom_boxplot()\ngrid.arrange(p1, p2, ncol=2, bottom=\"Two split boxplots for the log of claim amount, one split by injury code (left) and one split by legal representation (right), in the `persinj` data.\")"},{"path":"data-exploration-and-visualization.html","id":"case-3-categorical-vs.-categorical","chapter":"2 Data Exploration and Visualization","heading":"2.2.2.1 Case 3: Categorical vs. Categorical","text":"Descriptive Statistics: examining pair categorical variables, often useful construct two-way frequency table showing number observations every combination levels two variables using \\(\\fbox{table()}\\) function.two arguments supplied table() function, first argument correspond rows two way frequency table, second argument correspond columns.following code makes two-way frequency table legal representation cross injury code:Graphical Displays: proportions frequency table useful statistics, visual picture often expresses statistics much powerfully makes easy interpretation, especially two categorical variables lot levels. visualize distribution categorical variable split another categorical variable effectively, split bar charts can use. charts come different versions, one useful certain purpose. following code produces three bar charts injury code split legal representation (see Figure 2.2.12):\nStacked: first bar chart counts within injury code colored legal representation fill aesthetic set legrep. words, bar broken proportionally injuries legal representation (colored teal) without legal representation (colored red).\nDodged: second bar chart counts within injury code separated according legal representation placed side side comparison due option \\(\\fbox{position=\"dodge\"}\\) geom_bar() function.\nFilled: third bar chart, relative proportions (counts) injuries without legal representation within injury code shown due option \\(\\fbox{position=\"fill\"}\\) (confused fill aesthetic). makes easy compare proportions across different injury codes, although lose ability see number injuries code. predictive modeling, factor levels observations generally considered reliable sparse levels.\nGraphical Displays: proportions frequency table useful statistics, visual picture often expresses statistics much powerfully makes easy interpretation, especially two categorical variables lot levels. visualize distribution categorical variable split another categorical variable effectively, split bar charts can use. charts come different versions, one useful certain purpose. following code produces three bar charts injury code split legal representation (see Figure 2.2.12):Stacked: first bar chart counts within injury code colored legal representation fill aesthetic set legrep. words, bar broken proportionally injuries legal representation (colored teal) without legal representation (colored red).Dodged: second bar chart counts within injury code separated according legal representation placed side side comparison due option \\(\\fbox{position=\"dodge\"}\\) geom_bar() function.Filled: third bar chart, relative proportions (counts) injuries without legal representation within injury code shown due option \\(\\fbox{position=\"fill\"}\\) (confused fill aesthetic). makes easy compare proportions across different injury codes, although lose ability see number injuries code. predictive modeling, factor levels observations generally considered reliable sparse levels.Although discussed PA modules, filled bar charts usually useful depicting interplay two categorical variables; used, example, June 17 18, 2020 PA exams Hospital Re-admissions sample project. cursory glance filled bar chart Figure 2.2.12 shows higher proportion injuries legal representation codes 1 4 codes 5, 6, 9.\nFIGURE 2.3: Stacked (top left), dodged (top right), filled (bottom left) bar charts injury code split legal representation persinj dataset.\n","code":"\ntable(persinj$legrep, persinj$inj)\n#>    \n#>         1     2     3     4     5     6     9\n#>   0  5571  1152   374    56    85   121   649\n#>   1 10067  2224   759   133   103   135   607"},{"path":"data-exploration-and-visualization.html","id":"end-of-chapter-practice-problems","chapter":"2 Data Exploration and Visualization","heading":"2.2.3 End-of-Chapter Practice Problems","text":"","code":""},{"path":"data-exploration-and-visualization.html","id":"problem-2.3.1-small-differences-in-code-large-differences-in-output","chapter":"2 Data Exploration and Visualization","heading":"2.2.3.1 Problem 2.3.1 (Small Differences in code, large differences in output!)","text":"Consider personal injury insurance data set following chunks R commands (look similar!):Make guess chunk code . run code R see output.Solution: Although four chunks code look similar, produce drastically different output:CHUNK 1: making bar chart injury code boundary (interior) vertical bars colored according legal representation. can see, colors hardly perceptible.CHUNK 2: similar CHUNK 1, except time interior vertical bars color-coded according legal representation. output top left panel Figure 2.2.12.CHUNK 3: chunk code work (try run R get error!). reason fill argument (fill aesthetic) geom_bar() function mapped variable (legrep ) instead constant (e.g., “blue”). opposite mistake discussed CHUNK 4 Section 2.1 (Figure 2.1.2).CHUNK 4: chunk code generates output CHUNK 2. Instead putting fill aesthetic ggplot() call, placed inside geom_bar() function.","code":"\nlibrary(ggplot2)\n\n# read in data\npersinj <- read.csv(\"data/persinj.csv\")\n\n# convert categorical variables to factors\npersinj$inj <- as.factor(persinj$inj)\npersinj$legrep <- as.factor(persinj$legrep)\n\n# CHUNK 1\nggplot(persinj, aes(x=inj, color=legrep)) + \n  geom_bar()\n\n# CHUNK 2\nggplot(persinj, aes(x=inj, fill=legrep)) + \n  geom_bar()\n\n# CHUNK 3\n#ggplot(persinj, aes(x=inj)) + \n#  geom_bar(fill=legrep)\n\n# CHUNK 4\nggplot(persinj, aes(x=inj)) + \n  geom_bar(aes(fill=legrep))"},{"path":"data-exploration-and-visualization.html","id":"problem-2.3.2-data-exploration-univariate-and-bivariate","chapter":"2 Data Exploration and Visualization","heading":"2.2.3.2 Problem 2.3.2 (Data Exploration: Univariate and Bivariate)","text":"ggplot2 package comes data set named diamonds contains prices attributes approximately 54,000 diamonds. load data set, use following commands:() Determine number observations variables diamonds data set.aid appropriate graphical displays /summary statistics, complete following sub-tasks.(b) Perform univariate exploration price diamonds (price) quality cut (cut). Determine two variables transformed , , transformation made. recommended transformation(s), , delete original variable(s). (c) Explore relationship price diamonds weight diamonds (carat). (d) Explore relationship price diamonds quality cut. (e) Reconcile apparent contradiction get parts (c) (d).Solution:() number rows diamonds data set can obtained nrow() function:get number variables, can first extract column names data set via colnames() function, apply length() function calculate length:efficiently, can apply dim() function diamonds get row column dimensions:(b) price variable (positive technically continuous) numeric variable. Let’s use summary() function learn numeric statistics:variable ranges 326 18,823 mean much higher median, indication pronounced right skewness. confirmed histogram :deal right skewness price, can use log transformation. following commands create log-transformed price delete original price variable (recall learned Section 1.3).resulting histogram shows distribution log-transformed price closer symmetric, although bell-shaped.() histograms, can experiment different values bins parameter.cut variable 5-level categorical veriable, levels \"Fair\", \"Good\", \"Good\", \"Premium\", \"Ideal\".following table shows counts percentage level:number observations increases “Fair” “Ideal”. 40% diamonds “Ideal” quality.Since cut categorical variable, numeric transformations log-transformation applicable prefer leave .(c) Lprice carat numeric variables, scatterplot two variables appropriate exploring relationship. scatterplot shows two variables strongly positively related; heavier diamond, expensive , conforming intuition.Remark. can add alpha argument geom_point() function reduce amount overlapping.can add alpha argument geom_point() function reduce amount overlapping.plot Lprice log carat, resulting relationship close linear.plot Lprice log carat, resulting relationship close linear.(d) cut categorical variable, split boxplot Lprice broken levels cut appropriate exploring relationship. split boxplot, however, suggests counter-intuitive idea diamonds higher quality tend cheaper (though slightly). can case?(e) reconcile contradiction parts (c) (d), one can look relationship betwen carat cut. , carat numeric cut categorical, split boxplot carat split cut serve purpose:split boxplot shows weight diamond tends drop quality cut becomes higher (“Premium” exception). According part (c), carat important predictor Lprice, findings part (d) may result negative relationship carat cut – higher quality diamonds may less pricey weigh less.","code":"\nlibrary(ggplot2)\ndata(diamonds)\nnrow(diamonds)\n#> [1] 53940\nlength(colnames(diamonds))\n#> [1] 10\ndim(diamonds)\n#> [1] 53940    10\nsummary(diamonds$price)    \n#>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#>     326     950    2401    3933    5324   18823\np1 <- ggplot(diamonds, aes(x=price)) + \n  geom_histogram()\n\ngrid.arrange(p1, ncol=1)\ndiamonds$Lprice <- log(diamonds$price)\ndiamonds$price <- NULL\np1 <- ggplot(diamonds, aes(x=Lprice)) + \n  geom_histogram()\ngrid.arrange(p1, ncol=1)#> [1] \"Fair\"      \"Good\"      \"Very Good\" \"Premium\"  \n#> [5] \"Ideal\"#> \n#>      Fair      Good Very Good   Premium     Ideal \n#>      1610      4906     12082     13791     21551\n#> \n#>      Fair      Good Very Good   Premium     Ideal \n#>   0.02985   0.09095   0.22399   0.25567   0.39954"},{"path":"linear-models.html","id":"linear-models","chapter":"3 Linear Models","heading":"3 Linear Models","text":"Topic: Predictive Analytics Problem Defintion (10-20%)Learning Objectives: Candidate able identify business problem, available data relates possible analyses, use information propose appropriate modeling approach.","code":""},{"path":"linear-models.html","id":"a-primer-on-predictive-analytics","chapter":"3 Linear Models","heading":"3.1 A Primer on Predictive Analytics","text":"subsection streamlines material scattered different parts PA modules (notoriously difficult digest) presents coherent introduction predictive analytics.fundamental concepts introduced section universally applicable sense apply essentially types models, illustrated context specific types models (generalized linear models decision trees, particular) later chapters.","code":""},{"path":"linear-models.html","id":"basic-terminology","chapter":"3 Linear Models","heading":"3.1.1 Basic Terminology","text":"Predictive Analytics Nutshell:exactly predictive analytics? answer question, may help understand three main categories predictive modeling problems suggested PA modules:Descriptive: Descriptive analytics focuses happened past aims “describe” explain observed trends identifying relationships different variables data.\nExample: saw increase lapse rate among policyholders certain line business, kind policyholders highest tendency lapse? question can answered descriptive analytics.\nExample: saw increase lapse rate among policyholders certain line business, kind policyholders highest tendency lapse? question can answered descriptive analytics.Predictive: Predictive analytics focuses happen future concerned making accurate “predictions”.\nExample: prospective policyholder certain characteristics, predicted probability lapse? ability make prediction useful identifying future policyholders lower probability lapse contribute profitability insurer.\nExample: prospective policyholder certain characteristics, predicted probability lapse? ability make prediction useful identifying future policyholders lower probability lapse contribute profitability insurer.Prescriptive: Prescriptive analytics uses combination optimization simulation investigate quantify impact different “prescribed” actions different scenarios.\nExample: reduce premium certain amount, affect lapse rate?\nExample: reduce premium certain amount, affect lapse rate?surprisingly, Exam PA predominantly concerned predictive analytics, although three modeling approaches often mutually complementary, contradictory.always output (outcome) interest, can numeric (salary, premium, injury rates) categorical (positive/negative, email/spam), disposal collection input variables may offer potentially useful information predicting understanding output.“input-output” setting characteristic predictive analytics general, job develop model teasing (possibly complex, overlapping) contributions inputs outputs.Classification Variables:Predictive analytics requires data, often large number observations variables. Generally speaking, two ways classify variables predictive analytics context: role study (intended use) nature (characteristics).Role: variable interested predicting called target variable, simply target (.k.response variable, dependent variable, output variable, outcome variable).\nDespite target variable interest, situations change target variable directly, control associated variables can used predict target. variables go different names, predictors, explanatory variables, independent variables, input variables.\nactuarial context, predictors also known risk factors risk drivers. remainder manual Exam PA, mostly use term “predictors” “features” (defined Subsection 3.1.4).\nThroughout study predictive analytics, useful think predictive model following functional relationship target variable \\(Y\\) set predictors \\(X = (X_{1}, ..., X_{p})\\) (collected vector): \\[\n        \\begin{equation}\n        Y_{} = f(X_{}) + \\epsilon_{} (\\#eq:linear-relationship)\n        \\end{equation}\n        \\] :\nsubscript \\(\\) signifies \\(\\)th observation dataset, \\(Y_{}\\) value target variable \\(\\)th observation \\(X_{} = (X_{i1},..., X_{ip})\\) corresponding vector predictor values.\nsymbol \\(f\\) fixed (non-random) unknown real function connecting predictors target variables. Without subscript \\(\\) (\\(f\\) rather \\(f_{}\\)), function applies observations data, hence “systematic”. Largely synonymous model, function carries systematic information predictors offer target variable, allows us differentiate, discriminate, observations target variable basis predictors.\nDifferent types predictive models distinguished structural form function, e.g., linear linear models piece-wise constant decision trees, see later chapters.\nsymbol \\(\\epsilon_{}\\) zero-mean random error term carrying information specific \\(\\)th observation, hence “idiosyncratic” presence subscript \\(\\). can regarded catch-systematic component model misses, e.g., true relationship \\(X\\) \\(Y\\) probably complex (??), variables associated \\(Y\\) omitted model.\nAlthough (??) looks abstract exam test directly, provide useful framework thinking predictive analytics.\nconvenience, refer \\(f\\) \\(\\epsilon\\) respectively \\(\\fbox{signal function}\\) \\(\\fbox{noise}\\) engineering terms.\ninterested signal, data “contaminated” noise. goal predictive analytics filter noise use variety tools techniques learn much signal possible data. knowledge signal can provide us basis understanding data-generating process underlying population interest, making predictions target variable.\nRole: variable interested predicting called target variable, simply target (.k.response variable, dependent variable, output variable, outcome variable).Despite target variable interest, situations change target variable directly, control associated variables can used predict target. variables go different names, predictors, explanatory variables, independent variables, input variables.actuarial context, predictors also known risk factors risk drivers. remainder manual Exam PA, mostly use term “predictors” “features” (defined Subsection 3.1.4).Throughout study predictive analytics, useful think predictive model following functional relationship target variable \\(Y\\) set predictors \\(X = (X_{1}, ..., X_{p})\\) (collected vector): \\[\n        \\begin{equation}\n        Y_{} = f(X_{}) + \\epsilon_{} (\\#eq:linear-relationship)\n        \\end{equation}\n        \\] :subscript \\(\\) signifies \\(\\)th observation dataset, \\(Y_{}\\) value target variable \\(\\)th observation \\(X_{} = (X_{i1},..., X_{ip})\\) corresponding vector predictor values.subscript \\(\\) signifies \\(\\)th observation dataset, \\(Y_{}\\) value target variable \\(\\)th observation \\(X_{} = (X_{i1},..., X_{ip})\\) corresponding vector predictor values.symbol \\(f\\) fixed (non-random) unknown real function connecting predictors target variables. Without subscript \\(\\) (\\(f\\) rather \\(f_{}\\)), function applies observations data, hence “systematic”. Largely synonymous model, function carries systematic information predictors offer target variable, allows us differentiate, discriminate, observations target variable basis predictors.\nDifferent types predictive models distinguished structural form function, e.g., linear linear models piece-wise constant decision trees, see later chapters.symbol \\(f\\) fixed (non-random) unknown real function connecting predictors target variables. Without subscript \\(\\) (\\(f\\) rather \\(f_{}\\)), function applies observations data, hence “systematic”. Largely synonymous model, function carries systematic information predictors offer target variable, allows us differentiate, discriminate, observations target variable basis predictors.Different types predictive models distinguished structural form function, e.g., linear linear models piece-wise constant decision trees, see later chapters.symbol \\(\\epsilon_{}\\) zero-mean random error term carrying information specific \\(\\)th observation, hence “idiosyncratic” presence subscript \\(\\). can regarded catch-systematic component model misses, e.g., true relationship \\(X\\) \\(Y\\) probably complex (??), variables associated \\(Y\\) omitted model.\nAlthough (??) looks abstract exam test directly, provide useful framework thinking predictive analytics.\nconvenience, refer \\(f\\) \\(\\epsilon\\) respectively \\(\\fbox{signal function}\\) \\(\\fbox{noise}\\) engineering terms.\ninterested signal, data “contaminated” noise. goal predictive analytics filter noise use variety tools techniques learn much signal possible data. knowledge signal can provide us basis understanding data-generating process underlying population interest, making predictions target variable.symbol \\(\\epsilon_{}\\) zero-mean random error term carrying information specific \\(\\)th observation, hence “idiosyncratic” presence subscript \\(\\). can regarded catch-systematic component model misses, e.g., true relationship \\(X\\) \\(Y\\) probably complex (??), variables associated \\(Y\\) omitted model.Although (??) looks abstract exam test directly, provide useful framework thinking predictive analytics.convenience, refer \\(f\\) \\(\\epsilon\\) respectively \\(\\fbox{signal function}\\) \\(\\fbox{noise}\\) engineering terms.interested signal, data “contaminated” noise. goal predictive analytics filter noise use variety tools techniques learn much signal possible data. knowledge signal can provide us basis understanding data-generating process underlying population interest, making predictions target variable.Nature: Variables can also classified numeric variables categorical variables. classification important implications developing effective predictive model aligns character target variable predictors produce realistic output.\nNumeric Variables: Numeric variables take form numbers well-defined order (e.g., 1 must less 2) associated range. can classified :\nDiscrete variables: Restricted certain numeric values range, e.g., non-negative integers.\nContinuous: Can assume value continuum, least theory.\n\nCategorical Variables: Categorical variables take predefined values countable collection “categories”. categories, often numeric order (.e., say category larger smaller), called levels classes variable. examples properties can described categorical variables:\nGender (male, female, non-binary)\nSmoking status (smoking non-smoking)\nRisk group (preferred, standard, rated, un-insurable)\nMarital status (single, married, divorced, widowed)\nimportant special case categorical variable binary variable, takes two possible levels (often yes ), indicating whether event taken place whether characteristic present.\nmany datasets practice, categorical variables represented numbers, note coding levels categorical variable numbers make variable numeric. Consider, instance, variable \\(X\\) represents smoking status encoded : \\[\nX = \\begin{cases} & \\text{0, non-smoker,} \\\\ & \\text{1, smoker,} \\\\ & \\text{2, individual unknown smoking status.}  \\end{cases}\n\\]\nEven though three levels “non-smoker”, “smoker”, “unknown” coded “0”, “1”, “2”, respectively, three numbers merely labels without implicit order compared algebraic fashion. example, “1” less “2” meaningful fashion case.\npredictive modeling, type model use largely determined nature target variable, predictors.\nwords, distinction continuous categorical variables relatively unimportant serve predictors model, need take distinction properly account serve target variable. predictive models (e.g., linear models) work well continuous target variables (e.g., generalized linear models decision trees) apply numeric categorical target variables.Nature: Variables can also classified numeric variables categorical variables. classification important implications developing effective predictive model aligns character target variable predictors produce realistic output.Numeric Variables: Numeric variables take form numbers well-defined order (e.g., 1 must less 2) associated range. can classified :\nDiscrete variables: Restricted certain numeric values range, e.g., non-negative integers.\nContinuous: Can assume value continuum, least theory.\nNumeric Variables: Numeric variables take form numbers well-defined order (e.g., 1 must less 2) associated range. can classified :Discrete variables: Restricted certain numeric values range, e.g., non-negative integers.Discrete variables: Restricted certain numeric values range, e.g., non-negative integers.Continuous: Can assume value continuum, least theory.Continuous: Can assume value continuum, least theory.Categorical Variables: Categorical variables take predefined values countable collection “categories”. categories, often numeric order (.e., say category larger smaller), called levels classes variable. examples properties can described categorical variables:\nGender (male, female, non-binary)\nSmoking status (smoking non-smoking)\nRisk group (preferred, standard, rated, un-insurable)\nMarital status (single, married, divorced, widowed)\nimportant special case categorical variable binary variable, takes two possible levels (often yes ), indicating whether event taken place whether characteristic present.\nmany datasets practice, categorical variables represented numbers, note coding levels categorical variable numbers make variable numeric. Consider, instance, variable \\(X\\) represents smoking status encoded : \\[\nX = \\begin{cases} & \\text{0, non-smoker,} \\\\ & \\text{1, smoker,} \\\\ & \\text{2, individual unknown smoking status.}  \\end{cases}\n\\]\nEven though three levels “non-smoker”, “smoker”, “unknown” coded “0”, “1”, “2”, respectively, three numbers merely labels without implicit order compared algebraic fashion. example, “1” less “2” meaningful fashion case.Categorical Variables: Categorical variables take predefined values countable collection “categories”. categories, often numeric order (.e., say category larger smaller), called levels classes variable. examples properties can described categorical variables:Gender (male, female, non-binary)Gender (male, female, non-binary)Smoking status (smoking non-smoking)Smoking status (smoking non-smoking)Risk group (preferred, standard, rated, un-insurable)Risk group (preferred, standard, rated, un-insurable)Marital status (single, married, divorced, widowed)Marital status (single, married, divorced, widowed)important special case categorical variable binary variable, takes two possible levels (often yes ), indicating whether event taken place whether characteristic present.many datasets practice, categorical variables represented numbers, note coding levels categorical variable numbers make variable numeric. Consider, instance, variable \\(X\\) represents smoking status encoded : \\[\nX = \\begin{cases} & \\text{0, non-smoker,} \\\\ & \\text{1, smoker,} \\\\ & \\text{2, individual unknown smoking status.}  \\end{cases}\n\\]Even though three levels “non-smoker”, “smoker”, “unknown” coded “0”, “1”, “2”, respectively, three numbers merely labels without implicit order compared algebraic fashion. example, “1” less “2” meaningful fashion case.predictive modeling, type model use largely determined nature target variable, predictors.words, distinction continuous categorical variables relatively unimportant serve predictors model, need take distinction properly account serve target variable. predictive models (e.g., linear models) work well continuous target variables (e.g., generalized linear models decision trees) apply numeric categorical target variables.Supervised vs. Unsupervised Problems:Predictive analytics problems can also classified supervised unsupervised problems, depending presence target variable objective analysis.Supervised Learning Problems: refer target variable “supervising” guiding analysis, goal understand relationship target variable predictors, /make accurate predictions target based predictors.\nTypes Supervised Learning Methods:\nGeneralized Linear Models (GLMs)\nDecision Trees\n\nSupervised Learning Problems: refer target variable “supervising” guiding analysis, goal understand relationship target variable predictors, /make accurate predictions target based predictors.Types Supervised Learning Methods:\nGeneralized Linear Models (GLMs)\nDecision Trees\nTypes Supervised Learning Methods:Generalized Linear Models (GLMs)Generalized Linear Models (GLMs)Decision TreesDecision TreesUnsupervised Learning Problems: unsupervised learning methods, target variable supervising analysis (therefore “unsupervised”), interested extracting relationships structures different variables data.\nTypes Unsupervised Learning Methods:\nPrincipal Components Analysis (PCA)\nCluster Analysis\n\nUnsupervised Learning Problems: unsupervised learning methods, target variable supervising analysis (therefore “unsupervised”), interested extracting relationships structures different variables data.Types Unsupervised Learning Methods:\nPrincipal Components Analysis (PCA)\nCluster Analysis\nTypes Unsupervised Learning Methods:Principal Components Analysis (PCA)Principal Components Analysis (PCA)Cluster AnalysisCluster AnalysisNote supervised unsupervised learning methods sometimes used conjunction one another. see Chapter 6, unsupervised learning methods can used purposes data exploration producing potentially useful features predicting target variable accurately.Regression vs. Classification Problems:predictive analytics, customary refer supervised learning problems numeric target variable regression problems (exception logistic regression, target variable binary; see Chapter 4. contrast, target variable categorical nature, dealing classification problems. predictive model predicting categorical target variable involves “classifying” observations certain level aptly called classifier.regression classification problems importance predictive modeling general. two kinds predictive analytics problems unique features covered detail Part II manual.","code":""},{"path":"linear-models.html","id":"the-model-building-process","chapter":"3 Linear Models","heading":"3.1.2 The Model Building Process","text":"Now first taste predictive analytics like, important rather lengthy subsection walk main steps involved construction evaluation predictive model.practice, model building typically requires sequence complex inter-related decisions. whole process iterative often art science. framework necessarily simplified focuses important steps Exam PA, rich enough show takes build good model real life.","code":""},{"path":"linear-models.html","id":"stage-1-problem-definition","chapter":"3 Linear Models","heading":"3.1.2.1 Stage 1: Problem Definition","text":"first step model building exercise clearly formulate business problems predictive analytics applied.","code":""},{"path":"linear-models.html","id":"characteristics-of-predictive-modeling-problems","chapter":"3 Linear Models","heading":"Characteristics of Predictive Modeling Problems","text":"decide apply predictive analytics solve business problem, ensure indeed problem addressed predictive modeling.conceptual exam task testing syllabus learning outcome, “Describe characteristics predictive modeling problems.”, may ask:Explain two (three) reasons business problem addressed predictive modeling*common characteristics predictive modeling problems include:TABLE 2.1: Characteristics Predictive Modeling ProblemsA typical predictive modeling problem , necessarily characteristics.","code":""},{"path":"linear-models.html","id":"problem-definition","chapter":"3 Linear Models","heading":"Problem Definition","text":"deciding use predictive analytics address business issue interest, define problem clearly possible.important get root cause business issue make specific enough solvable.following strategies suggested can help us come meaningful problem definition give project higher chance success:Hypotheses: useful use prior knowledge business problem ask questions develop hypotheses can prove disprove course analytic work. helps us gain clearer understanding business issue guide efforts clearly defined way. questions hypotheses, know focus .Hypotheses: useful use prior knowledge business problem ask questions develop hypotheses can prove disprove course analytic work. helps us gain clearer understanding business issue guide efforts clearly defined way. questions hypotheses, know focus .KPI’s: also need ways assess outcome selecting appropriate key performance indicators (KPI’s), provide quantitative basis measure success project. Naturally, KPI’s align overall business strategy far possible show client key numbers change result predictive analytic work.KPI’s: also need ways assess outcome selecting appropriate key performance indicators (KPI’s), provide quantitative basis measure success project. Naturally, KPI’s align overall business strategy far possible show client key numbers change result predictive analytic work.","code":""},{"path":"linear-models.html","id":"constraints","chapter":"3 Linear Models","heading":"Constraints","text":"soon defined business problem clearly, important evaluate feasibility solving business problem implementing predictive analytic solution.want make sure solutions produce work. considerations constraints keep ind evaluating prioritizing business problems include:availability easily accessible high-quality data.Implementation issues presence necessary infrastructure technology fit complex models efficiently, timeline completing project, cost effort required maintain selected model.resources implement complex models without freezing crashing? model operationally prohibitive execute, makes sense trade prediction performance ease implementation. , actually implement apply model practice, basically useless.","code":""},{"path":"linear-models.html","id":"stage-2-data-collection-and-validation","chapter":"3 Linear Models","heading":"3.1.2.2 Stage 2: Data Collection and Validation","text":"Predictive analytics relies models, turn constructed data. defining business problem properly, spend time effort collecting useful data underlie predictive models going build.data collection stage important intricate probably first thought. may heard phrase “garbage , garbage ”, better known GIGO, means low-quality data doomed produce low-quality output.learn key considerations pitfalls avoid related collection use data.","code":""},{"path":"linear-models.html","id":"data-design","chapter":"3 Linear Models","heading":"Data Design","text":"begin, contemplate collect data used modeling stage.data design stage, things take note maximize usefulness data source, entire set observations enter dataset collect.Relevance: Intuitively, data , information available better, provided data unbiased (e.g., representative environment predictive model operate). ensure data relevant business problem, necessary source data right population time frame.\nPopulation: obvious may seem, important data source aligns true population interested . data source reasonably good proxy representative sample, taking larger dataset help, representing bias .\nTime Frame: selecting appropriate time frame data, advisable choose time period best reflects business environment implementing models. general, recent history predictive near future distant history, data one year ago useful data 10 years ago, things equal.\nExample 3.1.1: Problems Facebook Poll\nNovember 2020, Divide States America (DSA) held 60th presidential election, Donald Dump running Joe Hidden. gauge public sentiments towards election, Great News Network (GNN), /information outlet, set poll Facebook allowing users vote preferred presidential candidate. Facebook users take part poll, open August 2020 September 2020.\nEvaluate quality data collected GNN respect population, time frame, /items.\nSolution: quite problems data collected GNN, :\nPopulation: votes limited Facebook users intended audience supposed eligible citizens DSA. Users social media outlets tend younger generations, may systematic mismatch voters poll general population.\nTime Frame: objective poll understand voters’ preferences time November 2020 election. poll, conducted August 2020 September 2020, premature way might fail gauge public sentiments accurately (lot happen October). may desirable poll extend October, right November election took place.\nQuality: design, data collected susceptible contamination. individual may register several accounts take part poll several times. repetitive votes allowed poll substantially bias polling results.\nExample 3.1.2 Considerations Time Frame (Based Exercise 2.1.1 PA Modules)\nSuppose constructing model predict retirement option customer choose. data 1990 2020 containing customers choices. 2010, company introduced digital platform, increased variety retirement options available, also richness supporting data available customers made decisions.\nDiscuss pros cons including data 1990 relative including data 2010 constructing predictive model.\nSolution:\nPros: Including data 1990 means data. things equal, data generally desirable makes model training robust less vulnerable noise.\nCons: predictive modeling, important select period time best reflects environment implementing model. case, something happened 2010 significantly affected behavior outcome (retirement options) modeling. makes data prior 2010 less likely predictive current future behavior customers. Including (outdated) data may make predictions retirement options current era less accurate.\nRelevance: Intuitively, data , information available better, provided data unbiased (e.g., representative environment predictive model operate). ensure data relevant business problem, necessary source data right population time frame.Population: obvious may seem, important data source aligns true population interested . data source reasonably good proxy representative sample, taking larger dataset help, representing bias .Population: obvious may seem, important data source aligns true population interested . data source reasonably good proxy representative sample, taking larger dataset help, representing bias .Time Frame: selecting appropriate time frame data, advisable choose time period best reflects business environment implementing models. general, recent history predictive near future distant history, data one year ago useful data 10 years ago, things equal.Time Frame: selecting appropriate time frame data, advisable choose time period best reflects business environment implementing models. general, recent history predictive near future distant history, data one year ago useful data 10 years ago, things equal.Example 3.1.1: Problems Facebook PollIn November 2020, Divide States America (DSA) held 60th presidential election, Donald Dump running Joe Hidden. gauge public sentiments towards election, Great News Network (GNN), /information outlet, set poll Facebook allowing users vote preferred presidential candidate. Facebook users take part poll, open August 2020 September 2020.Evaluate quality data collected GNN respect population, time frame, /items.Solution: quite problems data collected GNN, :Population: votes limited Facebook users intended audience supposed eligible citizens DSA. Users social media outlets tend younger generations, may systematic mismatch voters poll general population.Population: votes limited Facebook users intended audience supposed eligible citizens DSA. Users social media outlets tend younger generations, may systematic mismatch voters poll general population.Time Frame: objective poll understand voters’ preferences time November 2020 election. poll, conducted August 2020 September 2020, premature way might fail gauge public sentiments accurately (lot happen October). may desirable poll extend October, right November election took place.Time Frame: objective poll understand voters’ preferences time November 2020 election. poll, conducted August 2020 September 2020, premature way might fail gauge public sentiments accurately (lot happen October). may desirable poll extend October, right November election took place.Quality: design, data collected susceptible contamination. individual may register several accounts take part poll several times. repetitive votes allowed poll substantially bias polling results.Quality: design, data collected susceptible contamination. individual may register several accounts take part poll several times. repetitive votes allowed poll substantially bias polling results.Example 3.1.2 Considerations Time Frame (Based Exercise 2.1.1 PA Modules)Suppose constructing model predict retirement option customer choose. data 1990 2020 containing customers choices. 2010, company introduced digital platform, increased variety retirement options available, also richness supporting data available customers made decisions.Discuss pros cons including data 1990 relative including data 2010 constructing predictive model.Solution:Pros: Including data 1990 means data. things equal, data generally desirable makes model training robust less vulnerable noise.Pros: Including data 1990 means data. things equal, data generally desirable makes model training robust less vulnerable noise.Cons: predictive modeling, important select period time best reflects environment implementing model. case, something happened 2010 significantly affected behavior outcome (retirement options) modeling. makes data prior 2010 less likely predictive current future behavior customers. Including (outdated) data may make predictions retirement options current era less accurate.Cons: predictive modeling, important select period time best reflects environment implementing model. case, something happened 2010 significantly affected behavior outcome (retirement options) modeling. makes data prior 2010 less likely predictive current future behavior customers. Including (outdated) data may make predictions retirement options current era less accurate.Sampling: Sampling process taking subset observations data source generate dataset.\nneed bother sampling? use entire data source? reality, underlying population often big handle, need smaller, much manageable subset form data.\nobservations sampled closely resemble business environment predictive model applied future. fact, good time frame, discussed , can seen form sampling, goal making sure data representative.\nnumber sampling methods commonly used draw appropriate sample:\nRandom Sampling: simplest sampling scheme “randomly” draw observations underlying population without replacement required number observations. record equally likely sampled.\nOne common way achieve random sampling survey questionnaire. Individuals population invited respond survey contribute collected dataset.\nHowever, voluntary surveys known vulnerable respondent bias. interested respond, means respondents survey may make population different population interest.\nexample, university students respond end--semester course evaluations tend feel strongly courses take. students strong opinions may substantially bias results course evaluations supposed general students.\nSurveys also tend suffer low response rate, unless financial incentives people respond.\n\nStratified Sampling: Stratified sampling involves dividing underlying population number non-overlapping “strata” groups non-random fashion, randomly sampling (without replacement) set number observations stratum.\nsampling method notable advantage ensuring every stratum properly represented collected data. Exam PA, strata usually formed respect distribution target variable.\nspecial cases stratified sampling:\nOversampling Undersampling: sampling methods designed specifically unbalanced data, learn detail Subsection 4.1.4.\nSystematic Sampling: systematic sampling, draw observations according set pattern random mechanism controlling observations sampled.\npopulation 100,000 units, may order well-defined way sample every tenth observation get smaller, manageable sample 10,000 units. sampling rule set, sampled observations pre-determined.\n\nSampling: Sampling process taking subset observations data source generate dataset.need bother sampling? use entire data source? reality, underlying population often big handle, need smaller, much manageable subset form data.observations sampled closely resemble business environment predictive model applied future. fact, good time frame, discussed , can seen form sampling, goal making sure data representative.number sampling methods commonly used draw appropriate sample:Random Sampling: simplest sampling scheme “randomly” draw observations underlying population without replacement required number observations. record equally likely sampled.\nOne common way achieve random sampling survey questionnaire. Individuals population invited respond survey contribute collected dataset.\nHowever, voluntary surveys known vulnerable respondent bias. interested respond, means respondents survey may make population different population interest.\nexample, university students respond end--semester course evaluations tend feel strongly courses take. students strong opinions may substantially bias results course evaluations supposed general students.\nSurveys also tend suffer low response rate, unless financial incentives people respond.\nRandom Sampling: simplest sampling scheme “randomly” draw observations underlying population without replacement required number observations. record equally likely sampled.One common way achieve random sampling survey questionnaire. Individuals population invited respond survey contribute collected dataset.\nHowever, voluntary surveys known vulnerable respondent bias. interested respond, means respondents survey may make population different population interest.\nexample, university students respond end--semester course evaluations tend feel strongly courses take. students strong opinions may substantially bias results course evaluations supposed general students.\nSurveys also tend suffer low response rate, unless financial incentives people respond.One common way achieve random sampling survey questionnaire. Individuals population invited respond survey contribute collected dataset.However, voluntary surveys known vulnerable respondent bias. interested respond, means respondents survey may make population different population interest.example, university students respond end--semester course evaluations tend feel strongly courses take. students strong opinions may substantially bias results course evaluations supposed general students.Surveys also tend suffer low response rate, unless financial incentives people respond.Stratified Sampling: Stratified sampling involves dividing underlying population number non-overlapping “strata” groups non-random fashion, randomly sampling (without replacement) set number observations stratum.\nsampling method notable advantage ensuring every stratum properly represented collected data. Exam PA, strata usually formed respect distribution target variable.\nspecial cases stratified sampling:\nOversampling Undersampling: sampling methods designed specifically unbalanced data, learn detail Subsection 4.1.4.\nSystematic Sampling: systematic sampling, draw observations according set pattern random mechanism controlling observations sampled.\npopulation 100,000 units, may order well-defined way sample every tenth observation get smaller, manageable sample 10,000 units. sampling rule set, sampled observations pre-determined.\nStratified Sampling: Stratified sampling involves dividing underlying population number non-overlapping “strata” groups non-random fashion, randomly sampling (without replacement) set number observations stratum.sampling method notable advantage ensuring every stratum properly represented collected data. Exam PA, strata usually formed respect distribution target variable.special cases stratified sampling:Oversampling Undersampling: sampling methods designed specifically unbalanced data, learn detail Subsection 4.1.4.Oversampling Undersampling: sampling methods designed specifically unbalanced data, learn detail Subsection 4.1.4.Systematic Sampling: systematic sampling, draw observations according set pattern random mechanism controlling observations sampled.\npopulation 100,000 units, may order well-defined way sample every tenth observation get smaller, manageable sample 10,000 units. sampling rule set, sampled observations pre-determined.Systematic Sampling: systematic sampling, draw observations according set pattern random mechanism controlling observations sampled.population 100,000 units, may order well-defined way sample every tenth observation get smaller, manageable sample 10,000 units. sampling rule set, sampled observations pre-determined.Granularity: Granularity refers precisely variable dataset measured, equivalently, detailed information contained variable . example, location, ascending order granularity, can recorded different degrees precision:\nCountry (least granular)\nState\nCity\nZip Code\nAddress (granular)\ncountry address, specific information location individual.\ndata design stage, good idea use relatively high level granularity. always possible go higher level granularity lower level granularity track, way around.Granularity: Granularity refers precisely variable dataset measured, equivalently, detailed information contained variable . example, location, ascending order granularity, can recorded different degrees precision:Country (least granular)Country (least granular)StateBy StateBy CityBy CityBy Zip CodeBy Zip CodeBy Address (granular)Address (granular)country address, specific information location individual.data design stage, good idea use relatively high level granularity. always possible go higher level granularity lower level granularity track, way around.","code":""},{"path":"linear-models.html","id":"data-quality-issues","chapter":"3 Linear Models","heading":"Data Quality Issues","text":"collected data data source representative population interest using appropriate sampling method, take important step jumping straight predictive modeling: Data Validation, process ensuring quality appropriateness data available.collected data falls short standards, may go back data collection step search alternative dataset.Reasonableness: dataset useful, data values , minimum, reasonable, can checked exploratory data analysis. key statistics variables make sense context business problem? one variables income one observation negative income, almost certainly recording error fixed launch model construction.Reasonableness: dataset useful, data values , minimum, reasonable, can checked exploratory data analysis. key statistics variables make sense context business problem? one variables income one observation negative income, almost certainly recording error fixed launch model construction.Consistency: also need ensure records data inputted consistently, meaning basis rules applied values, can directly compared one another.\nNumeric Variables: numeric variables, unit used across rows data consistency. one variables weight, nits values measured either kilograms pounds. monetary variable, income, currency used throughout.\nCategorical Variables: make sure factor levels categorical variables defined recorded consistently time coding changes, like either full name (e.g., Iowa) two-character abbreviation (e.g., IA) state policy issue.\ncheck consistency, necessary gain knowledge data entries recorded. end, data dictionary (see next point) accompanies PA exam project useful.\nwant combine multiple datasets form new variables (columns) new observations (rows), consistency checks extend constituent datasets make sure concatenation works properly.\nneed join datasets certain variable (column), especially important ensure format variable consistent across datasets ease matching, e.g., name recorded either [first name] [last name], [first name] person can identified uniquely. possible existence middle name adds complication.Consistency: also need ensure records data inputted consistently, meaning basis rules applied values, can directly compared one another.Numeric Variables: numeric variables, unit used across rows data consistency. one variables weight, nits values measured either kilograms pounds. monetary variable, income, currency used throughout.Numeric Variables: numeric variables, unit used across rows data consistency. one variables weight, nits values measured either kilograms pounds. monetary variable, income, currency used throughout.Categorical Variables: make sure factor levels categorical variables defined recorded consistently time coding changes, like either full name (e.g., Iowa) two-character abbreviation (e.g., IA) state policy issue.Categorical Variables: make sure factor levels categorical variables defined recorded consistently time coding changes, like either full name (e.g., Iowa) two-character abbreviation (e.g., IA) state policy issue.check consistency, necessary gain knowledge data entries recorded. end, data dictionary (see next point) accompanies PA exam project useful.want combine multiple datasets form new variables (columns) new observations (rows), consistency checks extend constituent datasets make sure concatenation works properly.need join datasets certain variable (column), especially important ensure format variable consistent across datasets ease matching, e.g., name recorded either [first name] [last name], [first name] person can identified uniquely. possible existence middle name adds complication.Sufficient Documentation: good dataset also sufficiently documented users can easily gain accurate understanding different aspects data. PA modules suggest effective documentation least include following information:\ndescription dataset overall, including data source (dataset collected).\ndescription variable data, including name, definition, format (range values numeric variables possible levels categorical variables).\nNotes past updates irregularities dataset.\nstatement accountability correctness dataset.\ndescription governance processes used manage dataset.\nExam PA exam project describes dataset “Business Problem” section provides data dictionary, lists variables data together information. sample data dictionary hypothetical group insurance policies:Sufficient Documentation: good dataset also sufficiently documented users can easily gain accurate understanding different aspects data. PA modules suggest effective documentation least include following information:description dataset overall, including data source (dataset collected).description dataset overall, including data source (dataset collected).description variable data, including name, definition, format (range values numeric variables possible levels categorical variables).description variable data, including name, definition, format (range values numeric variables possible levels categorical variables).Notes past updates irregularities dataset.Notes past updates irregularities dataset.statement accountability correctness dataset.statement accountability correctness dataset.description governance processes used manage dataset.description governance processes used manage dataset.Exam PA exam project describes dataset “Business Problem” section provides data dictionary, lists variables data together information. sample data dictionary hypothetical group insurance policies:\nTABLE 3.1: Sample Data Dictionary\n","code":""},{"path":"linear-models.html","id":"stage-3-exploratory-data-analysis","chapter":"3 Linear Models","heading":"3.1.2.3 Stage 3: Exploratory Data Analysis","text":"","code":""},{"path":"linear-models.html","id":"stage-4-model-construction-evaluation-and-selection-important","chapter":"3 Linear Models","heading":"3.1.2.4 Stage 4: Model Construction, Evaluation, and Selection [IMPORTANT!]","text":"","code":""},{"path":"linear-models.html","id":"stage-5-model-validation","chapter":"3 Linear Models","heading":"3.1.2.5 Stage 5: Model Validation","text":"","code":""},{"path":"linear-models.html","id":"stage-6-model-maintenance","chapter":"3 Linear Models","heading":"3.1.2.6 Stage 6: Model Maintenance","text":"","code":""},{"path":"linear-models.html","id":"bias-variance-trade-off","chapter":"3 Linear Models","heading":"3.1.3 Bias-Variance Trade-Off","text":"","code":""},{"path":"linear-models.html","id":"feature-generation-and-selection","chapter":"3 Linear Models","heading":"3.1.4 Feature Generation and Selection","text":"","code":""},{"path":"linear-models.html","id":"linear-models-conceptual-foundation","chapter":"3 Linear Models","heading":"3.2 Linear Models: Conceptual Foundation","text":"","code":""},{"path":"linear-models.html","id":"model-formulation","chapter":"3 Linear Models","heading":"3.2.1 Model Formulation","text":"","code":""},{"path":"linear-models.html","id":"model-evaluation-and-validation","chapter":"3 Linear Models","heading":"3.2.2 Model Evaluation and Validation","text":"","code":""},{"path":"linear-models.html","id":"feature-generation","chapter":"3 Linear Models","heading":"3.2.3 Feature Generation","text":"","code":""},{"path":"linear-models.html","id":"feature-selection","chapter":"3 Linear Models","heading":"3.2.4 Feature Selection","text":"","code":""},{"path":"linear-models.html","id":"regularization","chapter":"3 Linear Models","heading":"3.2.5 Regularization","text":"","code":""},{"path":"linear-models.html","id":"case-study-1-fitting-linear-models-in-r","chapter":"3 Linear Models","heading":"3.3 Case Study 1: Fitting Linear Models in R","text":"","code":""},{"path":"linear-models.html","id":"exploratory-data-analysis","chapter":"3 Linear Models","heading":"3.3.1 Exploratory Data Analysis","text":"","code":""},{"path":"linear-models.html","id":"simple-linear-regression","chapter":"3 Linear Models","heading":"3.3.2 Simple Linear Regression","text":"","code":""},{"path":"linear-models.html","id":"multiple-linear-regression","chapter":"3 Linear Models","heading":"3.3.3 Multiple Linear Regression","text":"","code":""},{"path":"linear-models.html","id":"evaluation-of-linear-models","chapter":"3 Linear Models","heading":"3.3.4 Evaluation of Linear Models","text":"","code":""},{"path":"linear-models.html","id":"case-study-2-feature-selection-and-regularization","chapter":"3 Linear Models","heading":"3.4 Case Study 2: Feature Selection and Regularization","text":"","code":""},{"path":"linear-models.html","id":"preparatory-work","chapter":"3 Linear Models","heading":"3.4.1 Preparatory Work","text":"","code":""},{"path":"linear-models.html","id":"model-construction-and-feature-selection","chapter":"3 Linear Models","heading":"3.4.2 Model Construction and Feature Selection","text":"","code":""},{"path":"linear-models.html","id":"model-validation","chapter":"3 Linear Models","heading":"3.4.3 Model Validation","text":"","code":""},{"path":"linear-models.html","id":"regularization-1","chapter":"3 Linear Models","heading":"3.4.4 Regularization","text":"","code":""},{"path":"linear-models.html","id":"conceptual-review-questions-for-chapter-3","chapter":"3 Linear Models","heading":"Conceptual Review: Questions for Chapter 3","text":"","code":""}]
